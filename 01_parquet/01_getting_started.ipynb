{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbe1c07",
   "metadata": {},
   "source": [
    "# Getting started with Parquet\n",
    "\n",
    "This series introduces Apache Parquet:\n",
    "\n",
    "* What is Parquet?\n",
    "* Developing with Parquet\n",
    "* Writing Parquet\n",
    "* Inspecting Parquet files\n",
    "* Querying Parquet files\n",
    "* Summary\n",
    "\n",
    "After going through the notebook, you should have basic understanding of what Apache Parquet is and how to read and write it from Python. You are encourage to follow the lines of code and to try to understanding what is happening in each line.\n",
    "\n",
    "## What is Parquet?\n",
    "\n",
    "Parquet is\n",
    "\n",
    "* An [Open source file format](https://parquet.apache.org/) for column-oriented storage and bulk transfer of data under Apache governance.\n",
    "* Readable and writable from [all mainstream programming languages and many database systems](https://parquet.apache.org/docs/file-format/implementationstatus/).\n",
    "* Based on an innovative algorithm for seamless \"shredding\" and reassembling arbitrarily complex data structures with  nested structures into flat columns [originally from Google Research](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf).\n",
    "\n",
    "Column-oriented storage is highly efficient for analytics. Consider this IoT example:\n",
    "\n",
    "* A machine sends every second readings from 100 different sensors.\n",
    "* You want to analyze the historical values of just **one** of those sensors.\n",
    "* **Row-based formats** (imagine CSV) store each time-stamped reading as a complete row with all 100 sensor values. To analyze one sensor, the system must read all 100 columns.\n",
    "* **Column-based formats** (like Parquet) group all values for a single sensor into a column. To analyze one sensor, the system only reads that specific column, dramatically reducing I/O.\n",
    "* This also improves compression, as storing similar data together is more efficient.\n",
    "\n",
    "![Row- vs column-oriented](../images/row-vs-column.png)\n",
    "\n",
    "> **This makes Parquet a great match for IoT analytics.**\n",
    "\n",
    "## Developing with Parquet in Python\n",
    "\n",
    "The notebooks focus on usage from Python. For developing with Parquet in Python, multiple options are available. In this notebook, we chose [Daft](https://docs.daft.ai/en/stable/), a high-performance data processing library. We also use [PyArrow](https://arrow.apache.org/docs/python/) for a more detailed look into Parquet files plus [Pandas](https://pandas.pydata.org/) and a [graphing library](https://matplotlib.org/stable/users/index) for visualizing data.\n",
    "\n",
    "There are also some helper functions for working with Parquet content included in this repository. Please ensure to install the libraries into your Python environment. E.g., in Visual Studio Code with Python supoprt enabled, you can select \"Python: Create environment\" and mark \"requirements.txt\" to install the libraries. Or use\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20fe1de",
   "metadata": {},
   "source": [
    "## Writing Parquet files\n",
    "\n",
    "What is the most simple way to write a Parquet file? \n",
    "\n",
    "* Create a DataFrame in memory.\n",
    "* Write the DataFrame to a Parquet file.\n",
    "\n",
    "Let's write a table with just a single row having a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a61f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_df = daft.from_pydict({'value': [42]})\n",
    "\n",
    "files = tiny_df.write_parquet('../data/output/minimal', write_mode='overwrite') # Daft writes a unique file to the given directory.\n",
    "minimal_parquet_file = files.to_pydict()['path'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed06ced",
   "metadata": {},
   "source": [
    "## Inspecting Parquet files\n",
    "\n",
    "What is actually inside a Parquet file? \n",
    "\n",
    "* Data is horizontally partitioned into larger **row groups** (e.g., several MBs).\n",
    "* Within each row group, data is stored column-by-column in **column chunks**.\n",
    "* Each column chunk is divided into **data pages**, which is where data is encoded and compressed.\n",
    "* A **file footer** contains all metadata, including the schema and statistics (like min/max values) for each column chunk. This allows query engines to skip reading unnecessary data.\n",
    "\n",
    "```\n",
    "Parquet file\n",
    "└─ Magic number\n",
    "└─ Row group (horizontal partitioning)\n",
    "   └─ Column chunk (vertical - one per column)\n",
    "      └─ Data page (optional dictionary page followed by actual data pages)\n",
    "         ├─ Page header\n",
    "         └─ Actual compressed/encoded data\n",
    "└─ File footer\n",
    "   └─ Metadata\n",
    "   └─ Footer length\n",
    "   └─ Magic number\n",
    "```\n",
    "\n",
    "![Parquet file layout](../images/FileLayout.gif)\n",
    "\n",
    "Parquet's read performance is enabled by a \"footer-first\" approach. Here's how a query engine efficiently finds data without scanning the whole file:\n",
    "\n",
    "* **Read the Footer:** The engine reads the small footer at the end of the file to get a \"map\" of its contents. \n",
    "* **Use Statistics to Skip Data:** It consults the metadata and statistics (like min/max values) in the footer to determine which row groups and column chunks it can safely ignore. This is called **predicate pushdown**.\n",
    "* **Fetch Only What's Needed:** The engine seeks directly to the required column chunks and reads only that data, dramatically reducing I/O.\n",
    "\n",
    "Let's inspect the file just written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(minimal_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf07a0a",
   "metadata": {},
   "source": [
    "That is a lot of metadata for just one value -- much more than what you get from CSV or even JSON.\n",
    "\n",
    "Note: Not all of the structure is visible on Python API level (e.g., data page headers, dictionary content), but try looking at the file with [parqeye](https://github.com/kaushiksrini/parqeye):\n",
    "\n",
    "```\n",
    "parqeye minimal.parquet\n",
    "```\n",
    "\n",
    "Check \"Row Groups\" and the \"value\" column. What do you notice? \n",
    "\n",
    "Let's repeat this with a larger data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a large file with events\n",
    "events_jsonl_path = Path('../data/input/events.jsonl')\n",
    "events_df = daft.read_json(str(events_jsonl_path))\n",
    "\n",
    "# Print a sample of the data\n",
    "events_df.sample(size = 1).show()\n",
    "\n",
    "# Write to Parquet\n",
    "files = events_df.write_parquet('../data/output/events', write_mode='overwrite')\n",
    "events_parquet_path = Path(files.to_pydict()['path'][0])\n",
    "\n",
    "# Compare with the original JSONL file\n",
    "parquet_mb = events_parquet_path.stat().st_size / (1024**2)\n",
    "reduction = 100 * events_parquet_path.stat().st_size / events_jsonl_path.stat().st_size\n",
    "print(f\"Parquet file created, size {parquet_mb:.2f} MB, {reduction:.2f}% of original JSONL size.\")\n",
    "\n",
    "inspect(events_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c65156",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901b0f3",
   "metadata": {},
   "source": [
    "What can we see?\n",
    "\n",
    "* Daft auto-discovers types. Even though \"time\" and \"creationTime\" are just strings in the input, it finds that these are actually timestamps and converts the type accordingly.\n",
    "* It uses by default snappy compression, resulting very efficient compression and decompression, but a file size that is slightly worse than if you would just gzip the JSONL file.\n",
    "* It writes by default dictionaries before the actual data pages. \n",
    "  * It collects all unique values of the column and writes them into a dictionary table. \n",
    "  * The data page contains instead of the values the index of the value to the dictionary table.\n",
    "  * E.g., if the column has the value \"abc\", \"def\", \"abc\", \"def\", the dictionary will have the entries \"abc\" and \"def\" and the data page itself will have the values 0, 1, 0, 1 (conceptually; full details are [here](https://parquet.apache.org/docs/file-format/data-pages/encodings/)).\n",
    "\n",
    "How would you model and encode the data? Can you think of better options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace410e6",
   "metadata": {},
   "source": [
    "## Querying Parquet files\n",
    "\n",
    "Daft can query Parquet files using expressions or even SQL. Here are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daft.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_events FROM events_df\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c60d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Event type distribution:\")\n",
    "daft.sql(f\"\"\"\n",
    "    SELECT\n",
    "        type,\n",
    "        COUNT(*) as count\n",
    "    FROM events_df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top event producers:\")\n",
    "daft.sql(f\"\"\"\n",
    "    SELECT source as device_id, COUNT(*) as event_count\n",
    "    FROM events_df\n",
    "    GROUP BY device_id\n",
    "    ORDER BY event_count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9561552",
   "metadata": {},
   "source": [
    "## Review questions\n",
    "\n",
    "**Why is columnar storage better for analytics than row-based storage?**\n",
    " - Think about the IoT sensor example - when would row-based storage actually be better?\n",
    "\n",
    "**What makes Parquet's \"footer-first\" approach efficient for large files?**\n",
    " - How does it avoid reading unnecessary data?\n",
    "\n",
    "**What is the purpose of row groups in Parquet?**\n",
    " - Why not just have one giant row group or many tiny ones?\n",
    "\n",
    "**How does predicate pushdown work with Parquet metadata?**\n",
    " - What statistics are needed to skip reading data?\n",
    "\n",
    "**Compare Parquet's compression ratio to JSONL.**\n",
    " - Is Parquet always smaller? What factors affect compression?\n",
    "\n",
    "## Challenges\n",
    "\n",
    "### Inspect different file structures\n",
    "\n",
    "1. Create three versions of the events data with different characteristics, just a few events, all of the events, multiple times the events replicated to create millions of rows. \n",
    "2. Inspect each file and compare:\n",
    "   - Number of row groups and data pages inside the row groups created\n",
    "   - Row group and data page sizes.\n",
    "   - File overhead (metadata size vs. data size)\n",
    "3. What patterns do you notice as file size increases?\n",
    "\n",
    "Try to do this in your programming language of choise, if it is not Python.\n",
    "\n",
    "### Query performance analysis\n",
    "\n",
    "1. Write queries that select:\n",
    "   - All columns for a few rows\n",
    "   - One column for all rows\n",
    "   - Aggregations (COUNT, SUM) on specific columns\n",
    "2. Measure execution time for each query type\n",
    "3. Which query patterns benefit most from columnar storage?\n",
    "\n",
    "### Explore cmdata.jsonl\n",
    "\n",
    "1. Read `cmdata.jsonl` and write it as Parquet\n",
    "2. Inspect the resulting file structure\n",
    "3. Query for devices of a specific type\n",
    "4. Calculate the compression ratio compared to the original JSONL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8euyditt5gd",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this section, we have shown how to easily write and query Parquet files. Parquet files use columnar storage, which is great for IoT analytics. They also contain quite some metadata, which allows a query engine to query the files in-place without necessarily reading the entire file. This is important if you have very large files and you need to transfer the files from an object store, as we will see later.\n",
    "\n",
    "Parquet files can not only be manipulated by Daft and PyArrow, but also from numerous other languages and tools. Try, for example, \n",
    " \n",
    " * Browsing the files using the interactive Parquet viewer [parqeye](https://github.com/kaushiksrini/parqeye).\n",
    " * Reading and write Parquet files using [Go parquet-tools](https://github.com/hangxie/parquet-tools).\n",
    " * Writing Parquet files from the [Java reference implementation](https://github.com/apache/parquet-java) if you are a Java person.\n",
    "\n",
    "In the next section, we dive deeper into the Parquet schemas, encodings and compressions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
