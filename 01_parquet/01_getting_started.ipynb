{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbe1c07",
   "metadata": {},
   "source": [
    "# Getting started with Parquet\n",
    "\n",
    "This series introduces Apache Parquet:\n",
    "\n",
    "* What is Parquet?\n",
    "* Developing with Parquet\n",
    "* Writing Parquet\n",
    "* Inspecting Parquet files\n",
    "* Querying Parquet files\n",
    "* Summary\n",
    "\n",
    "After going through the notebook, you should have basic understanding of what Apache Parquet is and how to read and write it from Python. You are encourage to follow the lines of code and to try to understanding what is happening in each line.\n",
    "\n",
    "## What is Parquet?\n",
    "\n",
    "Parquet is\n",
    "\n",
    "* An [Open source file format](https://parquet.apache.org/) for column-oriented storage and bulk transfer of data under Apache governance.\n",
    "* Readable and writable from [all mainstream programming languages and many database systems](https://parquet.apache.org/docs/file-format/implementationstatus/).\n",
    "* Based on an innovative algorithm for seamless \"shredding\" and reassembling arbitrarily complex data structures with  nested structures into flat columns [originally from Google Research](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf).\n",
    "\n",
    "Column-oriented storage is highly efficient for analytics. Consider this IoT example:\n",
    "\n",
    "* A machine sends every second readings from 100 different sensors.\n",
    "* You want to analyze the historical values of just **one** of those sensors.\n",
    "* **Row-based formats** (imagine CSV) store each time-stamped reading as a complete row with all 100 sensor values. To analyze one sensor, the system must read all 100 columns.\n",
    "* **Column-based formats** (like Parquet) group all values for a single sensor into a column. To analyze one sensor, the system only reads that specific column, dramatically reducing I/O.\n",
    "* This also improves compression, as storing similar data together is more efficient.\n",
    "\n",
    "![Row- vs column-oriented](../images/row-vs-column.png)\n",
    "\n",
    "> **This makes Parquet a great match for IoT analytics.**\n",
    "\n",
    "## Developing with Parquet in Python\n",
    "\n",
    "The notebooks focus on usage from Python. What do you need for developing with Parquet in Python?\n",
    "\n",
    "* PyArrow: A library for reading and writing Parquet files from Python.\n",
    "* DuckDB: An embeddable query engine for querying Parquet files using SQL.\n",
    "* Pandas and a graphing library: For visualizing data.\n",
    "\n",
    "There are also some helper functions for working with Parquet content included in this repository. Please ensure to install the libraries into your Python environment. E.g., in Visual Studio Code with Python supoprt enabled, you can select \"Python: Create environment\" and mark \"requirements.txt\" to install the libraries. Or use\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"PyArrow version: {pa.__version__}\")\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")\n",
    "\n",
    "# Make the display of data frames use more wide tables.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import read_jsonl, inspect, compare_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20fe1de",
   "metadata": {},
   "source": [
    "## Writing Parquet files\n",
    "\n",
    "What is the most simple way to write a Parquet file? \n",
    "\n",
    "* Use PyArrow to create a table in memory.\n",
    "* Write the table to a Parquet file.\n",
    "\n",
    "Let's write a table with just a single row having a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a61f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_table = pa.Table.from_pylist([{'value': 42}])\n",
    "minimal_parquet_file = Path('../data/output/minimal.parquet')\n",
    "pq.write_table(tiny_table, minimal_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed06ced",
   "metadata": {},
   "source": [
    "## Inspecting Parquet files\n",
    "\n",
    "What is actually inside a Parquet file? \n",
    "\n",
    "* Data is horizontally partitioned into larger **row groups** (e.g., several MBs).\n",
    "* Within each row group, data is stored column-by-column in **column chunks**.\n",
    "* Each column chunk is divided into **data pages**, which is where data is encoded and compressed.\n",
    "* A **file footer** contains all metadata, including the schema and statistics (like min/max values) for each column chunk. This allows query engines to skip reading unnecessary data.\n",
    "\n",
    "```\n",
    "Parquet file\n",
    "└─ Magic number\n",
    "└─ Row group (horizontal partitioning)\n",
    "   └─ Column chunk (vertical - one per column)\n",
    "      └─ Data page (optional dictionary page followed by actual data pages)\n",
    "         ├─ Page header\n",
    "         └─ Actual compressed/encoded data\n",
    "└─ File footer\n",
    "   └─ Metadata\n",
    "   └─ Footer length\n",
    "   └─ Magic number\n",
    "```\n",
    "\n",
    "![Parquet file layout](../images/FileLayout.gif)\n",
    "\n",
    "Parquet's read performance is enabled by a \"footer-first\" approach. Here's how a query engine efficiently finds data without scanning the whole file:\n",
    "\n",
    "* **Read the Footer:** The engine reads the small footer at the end of the file to get a \"map\" of its contents. \n",
    "* **Use Statistics to Skip Data:** It consults the metadata and statistics (like min/max values) in the footer to determine which row groups and column chunks it can safely ignore. This is called **predicate pushdown**.\n",
    "* **Fetch Only What's Needed:** The engine seeks directly to the required column chunks and reads only that data, dramatically reducing I/O.\n",
    "\n",
    "Let's inspect the file just written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_parquet = pq.ParquetFile(minimal_parquet_file)\n",
    "inspect(minimal_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf07a0a",
   "metadata": {},
   "source": [
    "That is a lot of metadata for just one value -- much more than what you get from CSV or even JSON.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Pyarrow tries to discover the schema automatically from the source data.\n",
    "* The metadata comes primarily from the Parquet specification, but Pyarrow adds properties. Use `store_schema=False` if you intend to write many small files.\n",
    "* Not all of the structure is visible on Python API level (e.g., data page headers, dictionary content).\n",
    "\n",
    "Try also looking at the file with [parqeye](https://github.com/kaushiksrini/parqeye):\n",
    "\n",
    "```\n",
    "parqeye minimal.parquet\n",
    "```\n",
    "\n",
    "Let's repeat this with a larger data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_jsonl_path = Path('../data/input/events.jsonl')\n",
    "events_data = read_jsonl(events_jsonl_path)\n",
    "\n",
    "from random import choice\n",
    "print(\"Random record from the data:\")\n",
    "display(choice(events_data))\n",
    "\n",
    "print(\"Creating Parquet file with pyarrow schema discovery...\")\n",
    "events_table = pa.Table.from_pylist(events_data)\n",
    "events_parquet_path = Path('../data/output/events.parquet')\n",
    "pq.write_table(events_table, events_parquet_path)\n",
    "\n",
    "parquet_mb = events_parquet_path.stat().st_size / (1024**2)\n",
    "reduction = 100 * events_parquet_path.stat().st_size / events_jsonl_path.stat().st_size\n",
    "print(f\"Parquet file created, size {parquet_mb:.2f} MB, {reduction:.2f}% of original JSONL size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c65156",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_parquet = pq.ParquetFile(events_parquet_path)\n",
    "inspect(events_parquet_path)\n",
    "\n",
    "display(HTML(\"<h2>Sample data</h2>\"))\n",
    "display(events_table.to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901b0f3",
   "metadata": {},
   "source": [
    "What can we see?\n",
    "\n",
    "* There are two row groups for the data. PyArrow automatically splits into row groups of 1024^2 rows size.\n",
    "* It auto-discovers types, but for JSON input, it mostly discovers string.\n",
    "* It uses by default snappy compression, resulting in a file size that is slightly worse than you would just gzip the JSONL file.\n",
    "* It writes by default dictionaries before the actual data pages. \n",
    "  * It collects all unique values of the column and writes them into a dictionary table. \n",
    "  * The data page contains instead of the values the index of the value to the dictionary table.\n",
    "  * E.g., if the column has the value \"abc\", \"def\", \"abc\", \"def\", the dictionary will have the entries \"abc\" and \"def\" and the data page itself will have the values 0, 1, 0, 1 (conceptually; full details are [here](https://parquet.apache.org/docs/file-format/data-pages/encodings/)).\n",
    "\n",
    "How would you model and encode the data? What are better options?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace410e6",
   "metadata": {},
   "source": [
    "## Querying Parquet files\n",
    "\n",
    "DuckDB is an embedded analytical database that can query Parquet files directly. Here are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT COUNT(*) as total_events\n",
    "FROM '{events_parquet_path}'\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c60d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    type,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "FROM '{events_parquet_path}'\n",
    "GROUP BY type\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(\"Event type distribution:\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    source as device_id,\n",
    "    COUNT(*) as event_count\n",
    "FROM '{events_parquet_path}'\n",
    "GROUP BY device_id\n",
    "ORDER BY event_count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = con.execute(query).fetchdf()\n",
    "print(\"Top event producers:\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9561552",
   "metadata": {},
   "source": [
    "What else could you calculate using the events? We will show more examples later.\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this section, we have shown how to easily write and query Parquet files. Parquet files use columnar storage, which is great for IoT analytics. They also contain quite some metadata, which allows a query engine to query the files in-place without necessarily reading the entire file. This is important if you have very large files and you need to transfer the files from an object store, as we will see later.\n",
    "\n",
    "Parquet files can not only be manipulated by Python and DuckDB, but also from numerous other languages and tools. Try, for example, \n",
    " \n",
    " * Browsing the files using the interactive Parquet view [parqeye](https://github.com/kaushiksrini/parqeye).\n",
    " * Reading and write Parquet files using [Go parquet-tools](https://github.com/hangxie/parquet-tools).\n",
    " * Writing Parquet files from the [Java reference implementation](https://github.com/apache/parquet-java) if you are a Java person.\n",
    "\n",
    "In the next section, we dive deeper into the Parquet schemas, encodings and compressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da62b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()\n",
    "print(\"✓ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
