{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4249079f",
   "metadata": {},
   "source": [
    "## Tuning Parquet files\n",
    "\n",
    "For high performance, Parquet requires a good schema and physical design. The Parquet libraries try to infer a working schema and design, but it is sometimes not efficient and sometimes also not working on the data. Let's experiment with Parquet\n",
    "\n",
    "* Data types\n",
    "* Encodings\n",
    "* And compressions\n",
    "\n",
    "and then check how an optimized design performs in querying.\n",
    "\n",
    "### Parquet data types\n",
    "\n",
    "**Physical data types** are the binary data types that Parquet stores data in:\n",
    "\n",
    "- `BOOLEAN`: 1 bit boolean\n",
    "- `INT32`: 32 bit signed ints\n",
    "- `INT64`: 64 bit signed ints\n",
    "- `FLOAT`: IEEE 32-bit floating point values\n",
    "- `DOUBLE`: IEEE 64-bit floating point values\n",
    "- `BYTE_ARRAY`: arbitrarily long byte arrays\n",
    "- `FIXED_LEN_BYTE_ARRAY`: fixed length byte arrays\n",
    "\n",
    "**Logical data types** are additional interpretations on top of the physical data types for outside representation and validation:\n",
    "\n",
    "Numeric Types:\n",
    "\n",
    "- `INT(bitWidth, isSigned)`: Signed/unsigned integers (8, 16, 32, 64 bits)\n",
    "  - `INT8`, `INT16`, `INT32`, `INT64` (signed)\n",
    "  - `UINT8`, `UINT16`, `UINT32`, `UINT64` (unsigned)\n",
    "- `DECIMAL(precision, scale)`: Fixed-point decimal numbers (mapped to INT32, INT64, FIXED_LEN_BYTE_ARRAY)\n",
    "\n",
    "String Types:\n",
    "\n",
    "- `STRING`: UTF-8 encoded character strings\n",
    "- `ENUM`: Enumerated string values\n",
    "- `UUID`: 128-bit universally unique identifiers\n",
    "- `JSON`: JSON-encoded strings\n",
    "\n",
    "Temporal Types:\n",
    "\n",
    "- `DATE`: Calendar date (days since Unix epoch)\n",
    "- `TIME`: Time of day with microsecond or nanosecond precision\n",
    "  - With/without timezone\n",
    "- `TIMESTAMP`: Instant in time with microsecond or nanosecond precision\n",
    "  - With/without timezone (UTC or local)\n",
    "- `INTERVAL`: Time duration\n",
    "\n",
    "Binary Types:\n",
    "\n",
    "- `BSON`: Binary JSON format\n",
    "\n",
    "Nested/Complex Types:\n",
    "\n",
    "- `LIST`: Ordered collection of elements\n",
    "- `MAP`: Key-value pairs\n",
    "- `STRUCT`: Record with named fields (nested columns)\n",
    "\n",
    "We'll review the nested types in the next notebook. Let's first try to give our event data better data types and a better schema first.\n",
    "\n",
    "Note: The notebook expects the files generated by the first notebook to be present for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import dateutil\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import read_jsonl, inspect, compare_sizes\n",
    "\n",
    "events_data_path = Path('../data/input/events.jsonl')\n",
    "events_data = read_jsonl(events_data_path)\n",
    "events_parquet_path = Path('../data/output/events.parquet')\n",
    "events_parquet = pq.ParquetFile(events_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data_typed = []\n",
    "for record in events_data:\n",
    "    events_data_typed.append({\n",
    "        'creationTime': dateutil.parser.isoparse(record['creationTime']),\n",
    "        'id': int(record['id']),\n",
    "        'source': int(record['source']),\n",
    "        'text': record['text'],\n",
    "        'time': dateutil.parser.isoparse(record['time']),\n",
    "        'type': record['type'],\n",
    "    })\n",
    "\n",
    "events_table_typed = pa.Table.from_pylist(\n",
    "    events_data_typed,\n",
    "    schema=pa.schema([\n",
    "        ('creationTime', pa.timestamp(\"ms\")),\n",
    "        ('id', pa.int64()),\n",
    "        ('source', pa.int64()),\n",
    "        ('text', pa.string()),\n",
    "        ('time', pa.timestamp(\"ms\")),\n",
    "        ('type', pa.string())\n",
    "    ])\n",
    ")\n",
    "\n",
    "events_parquet_path_typed = Path('../data/output/events_typed.parquet')\n",
    "pq.write_table(events_table_typed, events_parquet_path_typed)\n",
    "inspect(events_parquet_path_typed)\n",
    "events_parquet_typed = pq.ParquetFile(events_parquet_path_typed)\n",
    "\n",
    "compare_sizes(events_parquet, \"Original\", events_parquet_typed, \"Typed\")\n",
    "print(f\"Original size: {events_parquet_path.stat().st_size} \"\n",
    "      f\"Limited size: {events_parquet_path_typed.stat().st_size} \"\n",
    "      f\"Reduction to: {events_parquet_path_typed.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a07d0f",
   "metadata": {},
   "source": [
    "The dictionaries grew signifcantly. What could be the reason? Hint: How well do you expect text and binary timestamps to compress with, for example, \"gzip\"?\n",
    "\n",
    "Note: If you inspect the generated files with `parquet-tools`, it reports that while a dictionary is created, only the first data page actually uses the dictionary and all other data pages do not. The Java Parquet implementation is a little smarter and immediately skips dictionaries if they are not proving efficient.\n",
    "\n",
    "TBD: The timestamp seems to be now interpreted in the local timezone. The timestamps are output one hour later than with the untyped string.\n",
    "\n",
    "### Controlling Dictionary Encoding\n",
    "\n",
    "Let's use dictionaries only where they make sense, i.e., only where properties have similar values most of the time. What columns would that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema with explicit dictionaries\n",
    "events_table_dict = pa.Table.from_pylist(\n",
    "    events_data_typed,\n",
    "    schema=pa.schema([\n",
    "        ('creationTime', pa.timestamp(\"ms\")),\n",
    "        ('id', pa.int64()),\n",
    "        ('source', pa.dictionary(pa.int8(), pa.int64())),\n",
    "        ('text', pa.string()),\n",
    "        ('time', pa.timestamp(\"ms\")),\n",
    "        ('type', pa.dictionary(pa.int8(), pa.string())),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Write using only the \"source\" and \"type\" column as dictionary-encoded\n",
    "events_parquet_path_dict = Path('../data/output/events_dict.parquet')\n",
    "pq.write_table(events_table_dict, events_parquet_path_dict, use_dictionary=['source', 'type'])\n",
    "\n",
    "events_parquet_dict = pq.ParquetFile(events_parquet_path_dict)\n",
    "compare_sizes(events_parquet, \"Original\", events_parquet_dict, \"Limited\")\n",
    "print(f\"Original size: {events_parquet_path.stat().st_size} \"\n",
    "      f\"Limited size: {events_parquet_path_dict.stat().st_size} \"\n",
    "      f\"Reduction to: {events_parquet_path_dict.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d33e5",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Normal compression does a good job most of the time.\n",
    "* By setting a schema and controlling dictionary usage, we could reduce the file size to 80%.\n",
    "\n",
    "Note: You can view more details on dictionaries and data pages using `parquet-tools`. For example, to see `source` (Column 2), and to view its dictionary (usually in Page 0):\n",
    "\n",
    "```\n",
    "parquet-tools inspect --row-group 0 --column-chunk 2 events.parquet | python3 -mjson.tool \n",
    "parquet-tools inspect --row-group 0 --column-chunk 2 --page 0 events.parquet | python3 -mjson.tool \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a40514",
   "metadata": {},
   "source": [
    "### Sorting order\n",
    "\n",
    "Assume that our predominant use case is to query the data by `source` and `time`. If we sort by these properties, the query engine can narrow down the data pages to be read. \n",
    "\n",
    "Also, Parquet supports run-length encoding and delta encoding. \n",
    "\n",
    "* **Run-length encoding** means that repeating values are stored as a tuple of (number of repetitions, value). Conceptually, if your values are \"device_1\", \"device_1\", \"device_2\", \"device_2\", the run-length encoding is (2, \"device_1\"), (2, \"device_2\").\n",
    "* **Delta encoding** means that the difference between two values is stored. For example, if you have data coming into the system every second for a device, instead of storing each timestamp, only the initial timestamp and a sequence of 1000 (milliseconds) is stored. This way, you get many similar values that can be efficiently run-length encoded.\n",
    "\n",
    "Note: Delta-encoding is not efficient for timestamps on events usually, as they are not regularly emitted. We'll get back to that later. It still helps a bit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data_sorted = sorted(events_data_typed, key=lambda x: (x['source'], x['time']))\n",
    "events_table_sorted = pa.Table.from_pylist(\n",
    "  events_data_sorted,\n",
    "  schema=pa.schema([\n",
    "    ('creationTime', pa.timestamp(\"ms\")),\n",
    "    ('id', pa.int64()),\n",
    "    ('source', pa.dictionary(pa.int8(), pa.int64())),\n",
    "    ('text', pa.string()),\n",
    "    ('time', pa.timestamp(\"ms\")),\n",
    "    ('type', pa.dictionary(pa.int8(), pa.string())),\n",
    "  ])\n",
    ")\n",
    "\n",
    "events_parquet_path_sorted = Path('../data/output/events_sorted.parquet')\n",
    "pq.write_table(events_table_sorted, events_parquet_path_sorted, column_encoding={'time':'DELTA_BINARY_PACKED'}, use_dictionary=['source', 'type'])\n",
    "\n",
    "events_parquet_dict = pq.ParquetFile(events_parquet_path_sorted)\n",
    "compare_sizes(events_parquet, \"Original\", events_parquet_dict, \"Limited\")\n",
    "print(f\"Original size: {events_parquet_path.stat().st_size} \"\n",
    "      f\"Limited size: {events_parquet_path_sorted.stat().st_size} \"\n",
    "      f\"Reduction to: {events_parquet_path_sorted.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bf3a8",
   "metadata": {},
   "source": [
    "What can you observe on the \"source\" and \"time\" column? Verify using `parquet-tools`:\n",
    "\n",
    "```\n",
    "parquet-tools inspect --row-group 0 --column-chunk 2 events_sorted.parquet | python3 -mjson.tool \n",
    "parquet-tools inspect --row-group 0 --column-chunk 2 --page 0 events_sorted.parquet | python3 -mjson.tool \n",
    "```\n",
    "\n",
    "### Data modelling\n",
    "\n",
    "In our example, events contain formatted log messages with a text and the ID of the work piece. \n",
    "\n",
    "```\n",
    "Starting to work on workpiece 2024_9550021\n",
    "Stop to work on workpiece 2024_9550021\n",
    "```\n",
    "\n",
    "This is a common approach, but for analytics, it would be much more efficient to separate the workpiece ID right away instead of first formatting it and then parsing it back. Let's split the log entry and add a column for the workpiece ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workpiece_pattern = re.compile(r'(.*?workpiece\\s+)([\\w_]+)')\n",
    "\n",
    "events_data_split = []\n",
    "for record in events_data_sorted:\n",
    "    new_record = record.copy()\n",
    "    text = new_record['text']\n",
    "    match = workpiece_pattern.match(text)\n",
    "\n",
    "    if match:\n",
    "        new_record['text'] = match.group(1).strip()\n",
    "        new_record['workpiece_id'] = match.group(2)\n",
    "    else:\n",
    "        new_record['workpiece_id'] = None\n",
    "\n",
    "    events_data_split.append(new_record)\n",
    "\n",
    "table_split = pa.Table.from_pylist(\n",
    "    events_data_split,\n",
    "    schema=pa.schema([\n",
    "    ('creationTime', pa.timestamp(\"ms\")),\n",
    "    ('id', pa.int64()),\n",
    "    ('source', pa.dictionary(pa.int8(), pa.int64())),\n",
    "    ('text', pa.dictionary(pa.int8(), pa.string())), # Text is now low-cardinality\n",
    "    ('time', pa.timestamp(\"ms\")),\n",
    "    ('type', pa.dictionary(pa.int8(), pa.string())),\n",
    "    ('workpiece_id', pa.string()) # workpiece_id is high-cardinality\n",
    "]))\n",
    "\n",
    "events_parquet_path_split = Path('../data/output/events_split.parquet')\n",
    "start_time = time()\n",
    "pq.write_table(\n",
    "    table_split,\n",
    "    events_parquet_path_split,\n",
    "    column_encoding={'time': 'DELTA_BINARY_PACKED'},\n",
    "    use_dictionary=['source', 'type', 'text'] # Add the new 'text' to dictionary encoding\n",
    ")\n",
    "end_time = time()\n",
    "snappy_duration = end_time - start_time\n",
    "\n",
    "events_parquet_split = pq.ParquetFile(events_parquet_path_split)\n",
    "print(\"Comparing original file with the new split-text file:\")\n",
    "compare_sizes(events_parquet, \"Original\", events_parquet_split, \"Split\")\n",
    "print(f\"\\nOriginal size: {events_parquet_path.stat().st_size} bytes\")\n",
    "print(f\"Split file size: {events_parquet_path_split.stat().st_size} bytes\")\n",
    "print(f\"Reduction to: {events_parquet_path_split.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}% of original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794fa41",
   "metadata": {},
   "source": [
    "### Compression\n",
    "\n",
    "gzip can be used instead of snappy as compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_parquet_path_gzipped = Path('../data/output/events_gzipped.parquet')\n",
    "start_time = time()\n",
    "pq.write_table(table_split, events_parquet_path_gzipped, compression='gzip', column_encoding={'time':'DELTA_BINARY_PACKED'}, use_dictionary=['source', 'type', 'text'])\n",
    "end_time = time()\n",
    "gzip_duration = end_time - start_time\n",
    "\n",
    "events_parquet_dict = pq.ParquetFile(events_parquet_path_gzipped)\n",
    "compare_sizes(events_parquet, \"Original\", events_parquet_dict, \"Limited\")\n",
    "print(f\"Original size: {events_parquet_path.stat().st_size}\")\n",
    "print(f\"Snappy size: {events_parquet_path_split.stat().st_size} {events_parquet_path_split.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}%\")\n",
    "print(f\"Gzipped size: {events_parquet_path_gzipped.stat().st_size} {events_parquet_path_gzipped.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}% \")\n",
    "print(f\"Snappy write time: {snappy_duration:.2f} seconds\")\n",
    "print(f\"Gzipped write time: {gzip_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf84f1d",
   "metadata": {},
   "source": [
    "Much better compression, but also very heavy performance impact -- also for querying!\n",
    "\n",
    "Note:\n",
    "* If you use gzip on original JSON file, you will reach 23475269 or around double the size of the optimized file.\n",
    "* If you use gzip encoding on the first Parquet file, you will reach around 60% compression.\n",
    "\n",
    "## Querying\n",
    "\n",
    "Let's query the brandnew workpiece ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    DATE_TRUNC('week', time) as week,\n",
    "    COUNT(distinct workpiece_id) as pieces_per_week\n",
    "FROM '{events_parquet_path_split}'\n",
    "GROUP BY week\n",
    "ORDER BY week\n",
    "\"\"\"\n",
    "result = con.execute(query).fetchdf()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(result['week'], result['pieces_per_week'], marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Week', fontsize=12)\n",
    "plt.ylabel('Workpieces per Week', fontsize=12)\n",
    "plt.title('Weekly Workpiece Production', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9561552",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "PyArrow's schema discovery does a \"quick and dirty\" job in discovering the schema in the source data and blindly applies dictionaries everywhere, but does not even consistently use them. The Java implementation is a little bit smarter, but with the very basic type system of JSON as input, there is only so much that you can do automatically. So for analytics on larger scale, some tuning is needed -- we'll get back to that in the third module.\n",
    "\n",
    "Other insights: \n",
    "* Parquet uses only a few physical types in the column storage layer; all other standard types are applied on top.\n",
    "* \"Snappy\" compression seems a good tradeoff between disk and CPU resource usage.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
