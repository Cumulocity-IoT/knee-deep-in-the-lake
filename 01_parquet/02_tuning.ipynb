{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4249079f",
   "metadata": {},
   "source": [
    "## Tuning Parquet files\n",
    "\n",
    "For high performance, Parquet requires a good schema and physical design. Let's look at \n",
    "\n",
    "* Data types\n",
    "* Encodings\n",
    "* And compressions.\n",
    "\n",
    "### Parquet data types\n",
    "\n",
    "Parquet has a complete type system.\n",
    "\n",
    "**Physical data types** are the binary data types that Parquet stores data in:\n",
    "\n",
    "- `BOOLEAN`: 1 bit boolean\n",
    "- `INT32`: 32 bit signed ints\n",
    "- `INT64`: 64 bit signed ints\n",
    "- `FLOAT`: IEEE 32-bit floating point values\n",
    "- `DOUBLE`: IEEE 64-bit floating point values\n",
    "- `BYTE_ARRAY`: arbitrarily long byte arrays\n",
    "- `FIXED_LEN_BYTE_ARRAY`: fixed length byte arrays\n",
    "\n",
    "**Logical data types** are additional interpretations on top of the physical data types for outside representation and validation:\n",
    "\n",
    "Numeric Types:\n",
    "\n",
    "- `INT(bitWidth, isSigned)`: Signed/unsigned integers (8, 16, 32, 64 bits)\n",
    "  - `INT8`, `INT16`, `INT32`, `INT64` (signed)\n",
    "  - `UINT8`, `UINT16`, `UINT32`, `UINT64` (unsigned)\n",
    "- `DECIMAL(precision, scale)`: Fixed-point decimal numbers (mapped to INT32, INT64, FIXED_LEN_BYTE_ARRAY)\n",
    "\n",
    "String Types:\n",
    "\n",
    "- `STRING`: UTF-8 encoded character strings\n",
    "- `ENUM`: Enumerated string values\n",
    "- `UUID`: 128-bit universally unique identifiers\n",
    "- `JSON`: JSON-encoded strings\n",
    "\n",
    "Temporal Types:\n",
    "\n",
    "- `DATE`: Calendar date (days since Unix epoch)\n",
    "- `TIME`: Time of day with microsecond or nanosecond precision\n",
    "  - With/without timezone\n",
    "- `TIMESTAMP`: Instant in time with microsecond or nanosecond precision\n",
    "  - With/without timezone (UTC or local)\n",
    "- `INTERVAL`: Time duration\n",
    "\n",
    "Binary Types:\n",
    "\n",
    "- `BSON`: Binary JSON format\n",
    "\n",
    "Nested/Complex Types:\n",
    "\n",
    "- `LIST`: Ordered collection of elements\n",
    "- `MAP`: Key-value pairs\n",
    "- `STRUCT`: Record with named fields (nested columns)\n",
    "\n",
    "Parquet libraries will typically infer a working data type from the input data, but, for example, JSON has a much more elementary type system and you may need to \"help\" with the interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a40514",
   "metadata": {},
   "source": [
    "### Sorting order\n",
    "\n",
    "Sorting data by frequently queried columns (like `source` and `time`) improves compression and enables predicate pushdown during queries. \n",
    "\n",
    "To improve compression, Parquet applies run-length encoding and can apply delta encoding:\n",
    "\n",
    "* **Run-length encoding** means that repeating values are stored as a tuple of (number of repetitions, value). Conceptually, if your values are \"device_1\", \"device_1\", \"device_2\", \"device_2\", the run-length encoding is (2, \"device_1\"), (2, \"device_2\").\n",
    "* **Delta encoding** means that the difference between two values is stored. For example, if you have data coming into the system every second for a device, instead of storing each timestamp, only the initial timestamp and a sequence of 1000 (milliseconds) is stored. This way, you get many similar values that can be efficiently run-length encoded.\n",
    "\n",
    "Predicate pushdown means that certain parts of files can be skipped during queries. If you sort by device, you can ideally just read the sections of the files with that device. \n",
    "\n",
    "Let's first read again the original events as they are, and then sort them by \"source\" (device) and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "import re\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect, compare_sizes\n",
    "\n",
    "events_data = daft.read_json('../data/input/events.jsonl').exclude('id')\n",
    "files = events_data.write_parquet('../data/output/events')\n",
    "events_parquet_path = Path(files.to_pydict()['path'][0])\n",
    "\n",
    "events_parquet = pq.ParquetFile(events_parquet_path)\n",
    "print(f\"Base file size: {events_parquet_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sorting by device and time...\")\n",
    "events_data_sorted = events_data.sort([daft.col('source'), daft.col('time')])\n",
    "\n",
    "files = events_data_sorted.write_parquet('../data/output/events_sorted')\n",
    "events_parquet_path_sorted = Path(files.to_pydict()['path'][0])\n",
    "\n",
    "events_parquet_sorted = pq.ParquetFile(events_parquet_path_sorted)\n",
    "compare_sizes(events_parquet, \"Original\", events_parquet_sorted, \"Sorted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bf3a8",
   "metadata": {},
   "source": [
    "How can you explain the size differences? Verify your explanation using `parqeye` and `parquet-tools`. For example, to inspect the `source` column in more detail, use\n",
    "\n",
    "```\n",
    "parquet-tools inspect --row-group 0 --column-chunk 2 ….parquet | python3 -mjson.tool \n",
    "parquet-tools inspect --row-group 0 --column-chunk 2 --page 0 ….parquet | python3 -mjson.tool \n",
    "```\n",
    "\n",
    "What do you think, is delta encoding efficient on events? Why or why not?\n",
    "\n",
    "Note: Page 0 usually holds the dictionary data, if a dictionary is present.\n",
    "\n",
    "### Data modelling\n",
    "\n",
    "In our example, events contain formatted log messages with a text and the ID of the work piece. \n",
    "\n",
    "```\n",
    "Starting to work on workpiece 2024_9550021\n",
    "Stop to work on workpiece 2024_9550021\n",
    "```\n",
    "\n",
    "This is a common approach, but for analytics, it would be much more efficient to separate the workpiece ID right away instead of first formatting it and then parsing it back. Let's split the log entry and add a column for the workpiece ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data_split = events_data.with_columns({\n",
    "    \"workpiece_year\": daft.col(\"text\").regexp_extract(r\"workpiece\\s+(\\d+)_\", 1).cast(daft.DataType.int32()),\n",
    "    \"workpiece_num\": daft.col(\"text\").regexp_extract(r\"workpiece\\s+\\d+_(\\d+)\", 1).cast(daft.DataType.int64()),\n",
    "    \"text\": daft.col(\"text\").regexp_replace(r\"work\\s+.*$\", \"work\")\n",
    "}).sort([daft.col('source'), daft.col('time')])\n",
    "\n",
    "start_time = time()\n",
    "files = events_data_split.write_parquet('../data/output/events_split')\n",
    "events_parquet_path_split = Path(files.to_pydict()['path'][0])\n",
    "write_time = time() - start_time\n",
    "\n",
    "events_parquet_split = pq.ParquetFile(events_parquet_path_split)\n",
    "compare_sizes(events_parquet_sorted, \"Sorted\", events_parquet_split, \"Split\")\n",
    "print(f\"Write time (Snappy): {write_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794fa41",
   "metadata": {},
   "source": [
    "### Compression\n",
    "\n",
    "gzip can be used instead of snappy as compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "files = events_data_split.write_parquet('../data/output/events_gzip', compression=\"gzip\")\n",
    "events_parquet_path_gzip = Path(files.to_pydict()['path'][0])\n",
    "gzip_time = time() - start_time\n",
    "\n",
    "print(f\"Snappy Size: {events_parquet_path_split.stat().st_size:,} bytes | Time: {write_time:.2f}s\")\n",
    "print(f\"Gzip Size:   {events_parquet_path_gzip.stat().st_size:,} bytes | Time: {gzip_time:.2f}s\")\n",
    "print(f\"Reduction vs Original: {events_parquet_path_gzip.stat().st_size/events_parquet_path.stat().st_size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf84f1d",
   "metadata": {},
   "source": [
    "Much better compression, but also quite some performance impact -- also for querying!\n",
    "\n",
    "Note: If you use gzip on original JSON file, you will reach 23475269 bytes. So the optimized Parquet version is only 42% of the original file.\n",
    "\n",
    "## Querying\n",
    "\n",
    "Let's query the new data and visualize the weekly workpiece production.\n",
    "\n",
    "Note: Daft does not yet support \"date_trunc\" in SQL, so this example uses Daft's lazy DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    events_data_split\n",
    "    .with_column(\"week\", daft.col(\"time\").date_trunc(\"1 week\"))\n",
    "    .with_column(\"workpiece_id\",\n",
    "        daft.col(\"workpiece_year\").cast(daft.DataType.string()) + \"_\" +\n",
    "        daft.col(\"workpiece_num\").cast(daft.DataType.string()))\n",
    "    .groupby(\"week\")\n",
    "    .agg(daft.col(\"workpiece_id\").count_distinct().alias(\"pieces_per_week\"))\n",
    "    .sort(\"week\")\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(result['week'], result['pieces_per_week'], marker='o', linewidth=2)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Workpieces')\n",
    "plt.title('Weekly Workpiece Production')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9561552",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Parquet has a rich type system and efficient encoding and compression methods. By default, reasonable types and compression methods are inferred from the source data. Howevever, sorting is an essential to both get optimized Parquet files and efficient querying through \"predicate pushdown\".\n",
    "\n",
    "Other insights: \n",
    "* Parquet uses only a few physical types in the column storage layer; all other standard types are applied on top.\n",
    "* \"Snappy\" compression seems a good tradeoff between disk and CPU resource usage.\n",
    "* Parquet works best with larger files in the area of Gigabyes. Besides the correct implementation and encoding of data types, there are also a number of other tuning parameters, like row group and data page size. Parquet [recommends](https://parquet.apache.org/docs/file-format/configurations/) row groups at 1GB size and data page sizes at 8KB. Here are [some benchmarks](https://developer.nvidia.com/blog/encoding-and-compression-guide-for-parquet-string-data-using-rapids/).\n",
    "* Parquet libraries differ in the quality of the Parquet files that they produce. PyArrow, for example, does a rather \"quick and dirty\" job as can be seen in a [previous version of the notebook](https://github.com/Cumulocity-IoT/knee-deep-in-the-lake/blob/bfdf6f0b4d6fe897247e6cb0ff83b26d79738363/01_parquet/02_tuning.ipynb). Daft and Java seem to be smarter out of the box. If you chose a library, verify the output using the Parquet inspection tools.\n",
    "\n",
    "In the next section, we will cover the odds and ends of handling semi-structured data with Parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
