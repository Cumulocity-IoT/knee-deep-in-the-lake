{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae272160",
   "metadata": {},
   "source": [
    "## Structured types\n",
    "\n",
    "In IoT, complex machinery means complex data. How are complex structures handled by Parquet? Let's read an inventory of devices with more interesting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dateutil\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import read_jsonl, inspect, find_type_conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3480a2",
   "metadata": {},
   "source": [
    "### Using automated schema discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83280aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdata_file = Path('../data/input/cmdata.jsonl')\n",
    "cmdata = read_jsonl(cmdata_file)\n",
    "\n",
    "print(\"Displaying first rows of the original data...\")\n",
    "df = pd.DataFrame(cmdata)\n",
    "display(df)\n",
    "\n",
    "print(\"Creating Parquet file with pyarrow schema discovery...\")\n",
    "cmdata_table = pa.Table.from_pylist(cmdata)\n",
    "display(cmdata_table.to_pandas().head())\n",
    "\n",
    "cmdata_parquet_file = Path('../data/output/cmdata.parquet')\n",
    "pq.write_table(cmdata_table, cmdata_parquet_file)\n",
    "\n",
    "inspect(cmdata_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111325b",
   "metadata": {},
   "source": [
    "What do you notice?\n",
    "\n",
    "* How are the arrays for `childDevices` and so on shown in the table and in the actual file? Check also the path and the definition and repetition levels.\n",
    "* What happened to the `c8y_ActiveAlarmsStatus`?\n",
    "\n",
    "### Encoding structured types\n",
    "\n",
    "Arbitrarily deeply nested structured are still represented as a flat list of columns in Parquet. The algorithm for this originates from [Google's Dremel](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf). \n",
    "\n",
    "* **Repetition level** level tells us at what level a value repeated. If it is 0, a new top-level record is started. If it is 1, the repetition is in a top-level list in the record, and so on.\n",
    "* **Definition level** tells us where in the path to a value a null appeared. If it is 0, the top-level property was not present. If it is one, the top-level property was present but the next part of the path was null, and so on.\n",
    "\n",
    "Here is the example from the original paper from Google:\n",
    "\n",
    "![Shredding](../images/dremel-shredding.png)\n",
    "\n",
    "### Defining the schema\n",
    "\n",
    "PyArrow discovers the schema [from the first row only](https://github.com/apache/arrow/blob/534ef71eca582006668f6f4ac83b47dd695d2020/python/pyarrow/table.pxi#L6450). That means if any properties are not in the first row, they will not be transferred. Also, if there are any schema conflicts in the rows, it will stop with an error. \n",
    "\n",
    "Let's define a schema. We will use dictionary encoding for `type` and `owner` and convert timestamps from strings to real timestamps. Also, we explicitely define `c8y_ActiveAlarmsStatus`. We ignore `childAdditions`, `fragments` and `supportedMeasurements` for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db572ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.schema([\n",
    "    pa.field('id', pa.int64()),\n",
    "    pa.field('name', pa.string()),\n",
    "    pa.field('type', pa.dictionary(pa.int8(), pa.string())),\n",
    "    pa.field('owner', pa.dictionary(pa.int8(), pa.string())),\n",
    "    pa.field('creationTime', pa.timestamp('ms')),\n",
    "    pa.field('lastUpdated', pa.timestamp('ms')),\n",
    "    pa.field('childAssets', pa.list_(pa.string())),\n",
    "    pa.field('childDevices', pa.list_(pa.string())),\n",
    "    pa.field('c8y_ActiveAlarmsStatus', pa.struct([\n",
    "        pa.field('critical', pa.int32()),\n",
    "        pa.field('major', pa.int32()),\n",
    "        pa.field('minor', pa.int32()),\n",
    "        pa.field('warning', pa.int32())\n",
    "    ])),\n",
    "])\n",
    "\n",
    "cmdata_typed = []\n",
    "for record in cmdata:\n",
    "    cmdata_typed.append({\n",
    "        'id': int(record.get('id')),\n",
    "        'name': record.get('name'),\n",
    "        'type': record.get('type'),\n",
    "        'owner': record.get('owner'),\n",
    "        'creationTime': dateutil.parser.isoparse(record['creationTime']) if 'creationTime' in record else None,\n",
    "        'lastUpdated': dateutil.parser.isoparse(record['lastUpdated']) if 'lastUpdated' in record else None,\n",
    "        'childAssets': record.get('childAssets'),\n",
    "        'childDevices': record.get('childDevices'),\n",
    "        'c8y_ActiveAlarmsStatus': record.get('c8y_ActiveAlarmsStatus')\n",
    "    })\n",
    "\n",
    "table_typed = pa.Table.from_pylist(cmdata_typed, schema=schema)\n",
    "parquet_file_typed = Path('../data/output/cmdata_typed.parquet')\n",
    "pq.write_table(table_typed, parquet_file_typed, use_dictionary=['type', 'owner'])\n",
    "\n",
    "print(\"Typed Parquet file created successfully!\")\n",
    "inspect(parquet_file_typed)\n",
    "\n",
    "display(table_typed.to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f9483",
   "metadata": {},
   "source": [
    "### Processing complex structured data\n",
    "\n",
    "Let's try a more complex, more deeply nested dataset from `radiator.jsonl`. This file contains detailed process data from a machine, with many nested measurements. Let's try first with PyArrows schema discovery again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64633b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_file = Path('../data/input/radiator.jsonl')\n",
    "radiator_data = read_jsonl(radiator_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495597d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  radiator_table = pa.Table.from_pylist(radiator_data)\n",
    "  radiator_parquet_path = Path('../data/output/radiator_typed.parquet')\n",
    "  pq.write_table(radiator_table, radiator_parquet_path)\n",
    "  print(\"Parquet file created successfully with pyarrow schema discovery!\")\n",
    "except Exception as e:\n",
    "  print(\"Error creating Parquet file using pyarrow schema discovery:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccacce",
   "metadata": {},
   "source": [
    "There seems to be an inconsistency in the types of some fields in the JSONL data. PyArrow schema discovery, or more correctly, the schema discovery of the C++ implementation that PyArrow wraps, cannot handle mixtures of lists or structs with atomic types (among [other things](https://github.com/apache/arrow/blob/5eaf553bfc7aa639fd67bd622b6b808e71fbba39/python/pyarrow/src/arrow/python/inference.cc#L566)).\n",
    "\n",
    "This is daily business in industrial IoT. Any real IoT deployment sees different versions of devices online at the same time, sending different, potentially conflicting data. These devices may come from various suppliers or organisations in your company. \n",
    "\n",
    "In practice, it is rarely possible to centrally control and harmonize all the data that is arriving and you have to deal with some level of inconsistency. However, most data lake technologies are not very forgiving with respect to schema inconsistencies. What to do?\n",
    "\n",
    "The first challenge is to actually find the conflict in the data. Unfortunately, there is no further log or debug information available. Let's analyze the data to find these conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_type_conflicts(radiator_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52214cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_fixed = []\n",
    "for record in radiator_data:\n",
    "    if 'meas_Outcome' in record and 'exceptions' in record['meas_Outcome']:\n",
    "        if not isinstance(record['meas_Outcome']['exceptions'], list):\n",
    "            record['meas_Outcome']['exceptions'] = []\n",
    "    if 'meas_ModelNumber' in record and 'value' in record['meas_ModelNumber'] and isinstance(record['meas_ModelNumber']['value'], str):\n",
    "        try:\n",
    "            record['meas_ModelNumber']['value'] = int(record['meas_ModelNumber']['value'].strip())\n",
    "        except (ValueError, TypeError):\n",
    "            record['meas_ModelNumber']['value'] = None\n",
    "    radiator_fixed.append(record)\n",
    "\n",
    "radiator_table = pa.Table.from_pylist(radiator_fixed)\n",
    "radiator_parquet_path = Path('../data/output/radiator_fixed.parquet')\n",
    "pq.write_table(radiator_table, radiator_parquet_path)\n",
    "print(\"Parquet file created successfully with pyarrow schema discovery!\")\n",
    "\n",
    "inspect(radiator_parquet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6acbd",
   "metadata": {},
   "source": [
    "How can this Parquet file be improved? Try applying the techniques already learned to make this file more officient.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c0dff",
   "metadata": {},
   "source": [
    "## Parquet predicate pushdown\n",
    "\n",
    "The previous section said that query engines can use Parquet metadata to skip reading unnecessary parts of the file. Let's investigate this with a first test.\n",
    "\n",
    "In the queries in the previous section, we saw that the devices produced roughly between 100.000 and 200.000 events each. So let's reduce the size of the row groups from the default of more than a million to just 100000 and check the resulting file. \n",
    "\n",
    "If you open the various row groups and check the statistics for `source` and `time` inside them, what can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6659278",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_sorted = sorted(radiator_fixed, key=lambda x: (x['source']['value'], x['time']))\n",
    "radiator_sorted_table = pa.Table.from_pylist(radiator_sorted)\n",
    "radiator_sorted_parquet_path = Path('../data/output/radiator_sorted.parquet')\n",
    "pq.write_table(radiator_sorted_table, radiator_sorted_parquet_path, row_group_size=10000)\n",
    "inspect(radiator_sorted_parquet_path)\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(\"PRAGMA enable_profiling='json'\")\n",
    "con.execute(\"PRAGMA profiling_mode='detailed'\")\n",
    "\n",
    "query_template = '''\n",
    "SELECT time, meas_Load_1.current_ForceValue.value as force\n",
    "FROM '{}'\n",
    "WHERE source.value = '1822301'\n",
    "'''\n",
    "\n",
    "start = time()\n",
    "result = con.execute(query_template.format(radiator_parquet_path)).fetchdf()\n",
    "full_time = time() - start\n",
    "print(f\"Query time on unoptimized file: {full_time:.2f} seconds\")\n",
    "\n",
    "start = time()\n",
    "result = con.execute(query_template.format(radiator_sorted_parquet_path)).fetchdf()\n",
    "full_time = time() - start\n",
    "print(f\"Query time on optimized file: {full_time:.2f} seconds\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333c44c",
   "metadata": {},
   "source": [
    "Not the total bytes here --> Just 60k\n",
    "\n",
    "Move the sorting and pruning groups part to the second section and add it to the summary there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_parquet_path_split = Path('../data/output/events_split.parquet')\n",
    "table_split = pq.read_table(events_parquet_path_split)\n",
    "\n",
    "events_parquet_path_rowgroups = Path('../data/output/events_rowgroups.parquet')\n",
    "\n",
    "pq.write_table(\n",
    "    table_split,\n",
    "    events_parquet_path_rowgroups,\n",
    "    column_encoding={'time': 'DELTA_BINARY_PACKED'},\n",
    "    use_dictionary=['source', 'type', 'text'],\n",
    "    row_group_size=100000\n",
    ")\n",
    "\n",
    "inspect(events_parquet_path_rowgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125f3b7",
   "metadata": {},
   "source": [
    "From the statistics, you can see that the device `140672` occupies row group #0 and part of row group #1. The device `140673` occupies a part of row group #1, row group #2 and a part of row group #3 and so on.\n",
    "\n",
    "Let's query the device `140673` while DuckDB profiling is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import time\n",
    "\n",
    "dataset = ds.dataset(events_parquet_path_rowgroups, format=\"parquet\")\n",
    "filter_expr = ds.field(\"source\") == 140673\n",
    "\n",
    "start = time.time()\n",
    "table_full = dataset.scanner().to_table()\n",
    "full_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "table_filtered = dataset.scanner(filter=filter_expr).to_table()\n",
    "filtered_time = time.time() - start\n",
    "\n",
    "print(f\"Rows read full scan: {table_full.num_rows}, time: {full_time}\")\n",
    "print(f\"Rows read filtered scan: {table_filtered.num_rows}, time: {filtered_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "#con.execute(\"PRAGMA enable_profiling='json'\")\n",
    "#con.execute(\"PRAGMA profiling_mode='detailed'\")\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    DATE_TRUNC('week', time) as week,\n",
    "    COUNT(distinct workpiece_id) as pieces_per_week\n",
    "FROM '{events_parquet_path_rowgroups}'\n",
    "WHERE source = 140673\n",
    "GROUP BY week\n",
    "ORDER BY week\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "result = con.execute(query).fetchdf()\n",
    "full_time = time.time() - start\n",
    "print(f\"Query time on row-grouped file: {full_time:.2f} seconds\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(result['week'], result['pieces_per_week'], marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Week', fontsize=12)\n",
    "plt.ylabel('Workpieces per Week', fontsize=12)\n",
    "plt.title('Weekly Workpiece Production for Source 140673', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b922d84",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TBD: Add the stats part and write with smaller data pages to demonstrate pruning.\n",
    "https://stackoverflow.com/questions/76696239/predicate-pushdown-in-duckdb-for-a-parquet-file-in-s3\n",
    "Exercises: \n",
    "Manipulate the data_page_size parameter: https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html and check the DuckDB stats\n",
    "Do we need the ID of events? We can't edit in Parquet anyways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
