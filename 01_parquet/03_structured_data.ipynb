{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849a5e6e",
   "metadata": {},
   "source": [
    "# Structured types\n",
    "\n",
    "In IoT, complex machinery means complex data. How are complex structures handled by Parquet? Let's read an inventory of devices with more interesting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb244998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect\n",
    "daft.set_execution_config(parquet_target_row_group_size=128*1024*1024) # Set back to defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1db58",
   "metadata": {},
   "source": [
    "## Using automated schema discovery\n",
    "\n",
    "Daft is excellent at inferring schemas from JSON data, even when the data is nested or slightly inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daft.read_json('../data/input/cmdata.jsonl')\n",
    "df.show(10)\n",
    "display(df.schema())\n",
    "\n",
    "files = df.write_parquet('../data/output/cmdata.parquet', write_mode='overwrite')\n",
    "cmdata_parquet_file = files.to_pydict()['path'][0]\n",
    "inspect(Path(cmdata_parquet_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179e284",
   "metadata": {},
   "source": [
    "What do you notice?\n",
    "\n",
    "* How do the arrays look like in the table and in the actual Parquet file? What is the path, the definition and the repetition level?\n",
    "* How are the nested fields in `c8y_ActiveAlarmStatus` shown in the Parquet file? What is the path, the definition and the repetition level?\n",
    "\n",
    "Even though the first rows contain empty `c8y_ActiveAlarmStatus`, Daft evolves the schema as it goes along to discover the full structure of the alarm statuses. Not all libraries can do that; in PyArrow you would have to specify the schema yourself if it's not adequately represented in the first record.\n",
    "\n",
    "### Encoding structured types\n",
    "\n",
    "Arbitrarily deeply nested structured are still represented as a flat list of columns in Parquet. The algorithm for this originates from [Google's Dremel](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf). \n",
    "\n",
    "* **Repetition level** level tells us at what level a value repeated. If it is 0, a new top-level record is started. If it is 1, the repetition is in a top-level list in the record, and so on.\n",
    "* **Definition level** tells us where in the path to a value a null appeared. If it is 0, the top-level property was not present. If it is one, the top-level property was present but the next part of the path was null, and so on.\n",
    "\n",
    "Here is the example from the original paper from Google:\n",
    "\n",
    "![Shredding](../images/dremel-shredding.png)\n",
    "\n",
    "## Processing complex structured data\n",
    "\n",
    "Let's work with the `radiator.jsonl` dataset. This file is a classic example of \"dirty data\" from an industrial source. \n",
    "\n",
    "Specifically, the `meas_ModelNumber` field has conflicting types:\n",
    "* In most records, it is a struct `{\"name\": \"Audi RS6\", \"value\": 4}` where `value` is an integer.\n",
    "* In some records (around line 172,000), the `value` is sent as a string: `\"value\": \" 3\"` or `\"value\": \"1\"`.\n",
    "\n",
    "How will `meas_ModelNumer.value` turn out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_df = daft.read_json('../data/input/radiator.jsonl').with_column(\"source_id\", daft.col(\"source\")[\"value\"])\n",
    "files = radiator_df.write_parquet('../data/output/radiator', write_mode='overwrite')\n",
    "radiator_parquet_path = files.to_pydict()['path'][0]\n",
    "\n",
    "inspect(Path(radiator_parquet_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e955647",
   "metadata": {},
   "source": [
    "How did Daft handle the type conflict?\n",
    "\n",
    "1. It infers `Int64` for `meas_ModelNumber.value` from the clean records at the beginning.\n",
    "2. When it eventually encounters the string values (`\" 3\"`, `\"1\"`), it effectively **coerces** them into the target type (`Int64`).\n",
    "3. This allows the read to complete successfully without dropping data or requiring a manual \"cleaning pass\" beforehand. \n",
    "\n",
    "Strict schema-on-read systems like PyArrow will crash or fail to parse this file. Daft can handle some level of changes.\n",
    "\n",
    "## Parquet predicate pushdown and Tuning\n",
    "\n",
    "As discussed, sorting data helps query engines skip unnecessary data. Let's create a sorted version of the radiator data. We sort by the ID of the device and the time of measurement.\n",
    "\n",
    "Note: To demonstrate the effect, we write the DataFrame using PyArrow with a small row group size. We also flatten the `source` property -- we will see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_sorted_df = radiator_df.with_column(\"source_id\", daft.col(\"source\")[\"value\"]).sort([daft.col(\"source_id\"), daft.col(\"time\")])\n",
    "\n",
    "daft.set_execution_config(parquet_target_row_group_size=4*1024*1024)\n",
    "files = radiator_sorted_df.write_parquet('../data/output/radiator_sorted', write_mode='overwrite')\n",
    "radiator_sorted_parquet_path = files.to_pydict()['path'][0]\n",
    "\n",
    "inspect(Path(radiator_sorted_parquet_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79c5cc",
   "metadata": {},
   "source": [
    "Open some column chunks and check the statistics of the `source_id` property. \n",
    "\n",
    "Now let's query one measurement of a single device. Device \"1822301\" appears only in the last column chunk of the sorted file, as can be seen by the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Querying unoptimized file...\")\n",
    "df_unopt = daft.read_parquet(radiator_parquet_path)\n",
    "\n",
    "start = time()\n",
    "result = df_unopt.filter(\n",
    "    daft.col(\"source_id\") == \"1822301\"\n",
    ").select(\n",
    "    \"time\",\n",
    "    daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\")\n",
    ").collect()\n",
    "full_time = time() - start\n",
    "print(f\"Query time on unoptimized file: {full_time:.2f} seconds\")\n",
    "\n",
    "print(\"Querying sorted file...\")\n",
    "df_opt = daft.read_parquet(radiator_sorted_parquet_path)\n",
    "\n",
    "start = time()\n",
    "result = df_opt.filter(\n",
    "    daft.col(\"source_id\") == \"1822301\"\n",
    ").select(\n",
    "    \"time\",\n",
    "    daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\")\n",
    ").collect()\n",
    "full_time = time() - start\n",
    "print(f\"Query time on optimized file: {full_time:.2f} seconds\")\n",
    "\n",
    "daft.set_execution_config(parquet_target_row_group_size=128*1024*1024) # Set back to defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ee896",
   "metadata": {},
   "source": [
    "Of course, the files used in our tests are comparatively small and local filesystem access is very small. You should still see a small difference in the execution speed. In my case, the query on sorted data took about a sixth of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ca7fb",
   "metadata": {},
   "source": [
    "## Bonus: Daft schema inferrence\n",
    "\n",
    "As we have seen, real-world machine data is rarely clean. Let's try out some more challenging type conflicts and schema changes with \"micro datasets\" and verify the output. For example,\n",
    "\n",
    "* What is the result if a property is encountered first as numeric `1` and then as string `\"1.0.1\"`?\n",
    "* What happens if there are arrays with mixed types?\n",
    "* What happens when first an array and then a struct is encountered? \n",
    "\n",
    "Feel encourage to find your own \"evil\" scenarios. You will certainly see such a situation at some point in real-life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_evil_scenario(scenario_name, data):\n",
    "    display(Markdown(f\"### {scenario_name}\"))\n",
    "    with tempfile.NamedTemporaryFile(dir=\"../data/output\", mode='w', suffix='.jsonl', delete=False) as tmp:\n",
    "        try:\n",
    "            for record in data:\n",
    "                tmp.write(json.dumps(record) + '\\n')\n",
    "            tmp_path = tmp.name\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating temp file: {e}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        df = daft.read_json(tmp_path)\n",
    "        print(\"Inferred Schema:\")\n",
    "        print(df.schema())\n",
    "        print(\"Data Preview:\")\n",
    "        display(df.collect())\n",
    "    except Exception as e:\n",
    "        print(f\"Daft failed to read: {e}\")\n",
    "    finally:\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "\n",
    "check_evil_scenario(\"Numeric vs string\", [{\"a\": 1}, {\"a\": \"1.0.1\"}] )\n",
    "check_evil_scenario(\"Struct evolution\", [{\"a\": {\"x\": 1}}, {\"a\": {\"y\": 2}}])\n",
    "check_evil_scenario(\"Array vs struct\", [{\"a\": [1]}, {\"a\": {\"b\": 1}}])\n",
    "check_evil_scenario(\"Mixed types in array\", [{\"a\": [1, \"2\", [3]]}])\n",
    "check_evil_scenario(\"Null handling\", [{\"a\": 1}, {\"a\": None}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58fae5",
   "metadata": {},
   "source": [
    "Daft does a quite good job. But when do you need to expect a loss of input data? How can you handle it? Check out Section \"Querying\" for an option to rewrite data with a different structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1fb32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Parquet is able to \"flatten\" out any kind of structured data into a columnar representation by using Dremel's \"shredding\" algorithm. However, Parquet always expects well-defined data structures, which is not always the case in real world input data. Daft does a quite decent job of discovering a viable schema from the input data, but it cannot handle all types of changes automatically. Schemas may also degenerate over longer time. Eventually, you may have to manually intervene.\n",
    "\n",
    "We have also executed a short test to evaluate \"predicate pushdown\" on the complex data, i.e., using metadata to only read the parts of a file that are required for querying. The effects are hard to demonstrate in test environments, but are very relevant for large, real-world data sets on object stores. We'll look into object stores in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
