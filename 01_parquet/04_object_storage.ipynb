{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47a565a",
   "metadata": {},
   "source": [
    "# Object Storage and Parquet\n",
    "\n",
    "Object storage (like AWS S3) is the standard for Data Lakes. Unlike local files, accessing data over the network has high latency. Parquet is designed for this environment. It allows query engines to only download the specific chunks of data they need (Column Pruning and Predicate Pushdown), drastically reducing network traffic.\n",
    "\n",
    "Let's have a look how this works in more detail. We simulate an object storage using the `moto` AWS mocking library and a custom logger to print the requests that run over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import boto3\n",
    "import daft\n",
    "from werkzeug.serving import make_server, WSGIRequestHandler\n",
    "from werkzeug.urls import uri_to_iri\n",
    "from moto.server import DomainDispatcherApplication, create_backend_app\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "os.environ[\"DAFT_DASHBOARD_ENABLED\"] = \"0\"\n",
    "BUCKET = \"data-lake\"\n",
    "\n",
    "\n",
    "def custom_log_request(self, code='-', size='-'):\n",
    "    path = uri_to_iri(self.path)\n",
    "    if BUCKET in path:\n",
    "        range_header = self.headers.get('Range', 'No range')\n",
    "        code = str(code)\n",
    "        print('ðŸ“¡ %04s [%24s] %s %s %s ' % (self.command, range_header, path, code, size))\n",
    "\n",
    "WSGIRequestHandler.log_request = custom_log_request\n",
    "\n",
    "def start_s3_server(port=5000):\n",
    "    app = DomainDispatcherApplication(create_backend_app)\n",
    "    #app = RangeHeaderLogger(app)\n",
    "\n",
    "    server = make_server(\"127.0.0.1\", port, app)\n",
    "    thread = threading.Thread(target=server.serve_forever)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "    print(f\"S3 Server running on port {port}\")\n",
    "    return server\n",
    "\n",
    "server = start_s3_server()\n",
    "s3 = boto3.client(\"s3\", endpoint_url=\"http://127.0.0.1:5000\", aws_access_key_id=\"fake\", aws_secret_access_key=\"fake\", region_name=\"us-east-1\")\n",
    "_ = s3.create_bucket(Bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d72749",
   "metadata": {},
   "source": [
    "Just like in the previous section, we'll sort and write some measurements with fairly small row groups to demonstrate pruning. This time, we write to the mocked object store. The file itself is the same, so you can still inspect the file from the previous section and compare it with this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_df = daft.read_json('../data/input/radiator.jsonl')\n",
    "radiator_sorted_df = radiator_df.sort([daft.col(\"source\")[\"value\"], daft.col(\"time\")])\n",
    "\n",
    "io_config = daft.io.IOConfig(s3=daft.io.S3Config(endpoint_url=\"http://127.0.0.1:5000\", anonymous=True))\n",
    "daft.set_execution_config(parquet_target_row_group_size=4*1024*1024)\n",
    "_ = radiator_sorted_df.write_parquet(f\"s3://{BUCKET}/radiator\", io_config=io_config, write_mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8eb4b6",
   "metadata": {},
   "source": [
    "Next, we'll prepare a query on a particular device that we know is at the end of the sorting order (and hence at the end of the file). Note how during preparing the query, a negative byte range is used. A negative byte range indicates an offset from the end of the object. What is at the end of a Parquet file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = daft.read_parquet(f\"s3://{BUCKET}/radiator\", io_config=io_config)\n",
    "df_s3 = df_s3.filter(daft.col(\"source\")[\"value\"] == \"1822301\").select(daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554aacd",
   "metadata": {},
   "source": [
    "Now, let's actually run the query. Note that all requests read only a part of the file. What part is this? Inspect the previous section's file, e.g., using \n",
    "\n",
    "```\n",
    "parquet-tools inspect --row-group 13 --column-chunk 22 â€¦.parquet| python3 -mjson.tool | less\n",
    "```\n",
    "\n",
    "(Columns chunk 2 is `source.value`, column chunk 22 is `meas_PressureBalance.current_ForceValue.value`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5197e4c",
   "metadata": {},
   "source": [
    "The ranges shown here are actually reaching from the start of column `source.value` to the end of column `meas_PressureBalance.current_ForceValue.value` in each row group. Daft seems to be too daft (sorry for the pun) to understand from the min and max values of `source.value` that only row group 13 needs to be queried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895050f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "def simplify_explain(explain_output):\n",
    "    \"\"\"Simplify explain output by removing verbose schema definitions\"\"\"\n",
    "    lines = explain_output.split('\\n')\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "\n",
    "        # Check if this is an IO config line\n",
    "        if '   IO config = ' in line:\n",
    "            result.append(re.sub(r'(IO config = ).*', r'\\1<...>', line))\n",
    "            i += 1\n",
    "            # Skip all heavily indented continuation lines\n",
    "            while i < len(lines) and (lines[i].startswith('|     ') or lines[i].startswith('    ')):\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Check if this is a File/Output schema line\n",
    "        if re.search(r'\\|   (File schema|Output schema) = ', line):\n",
    "            result.append(re.sub(r'((File schema|Output schema) = ).*', r'\\1<...>', line))\n",
    "            i += 1\n",
    "            # Skip continuation lines (more indented)\n",
    "            while i < len(lines) and (lines[i].startswith('|     ') or lines[i].startswith('    ')):\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Check if this is Schema: {...} (in Physical Plan)\n",
    "        if '   Schema: ' in line:\n",
    "            result.append(re.sub(r'(Schema: ).*', r'\\1<...>', line))\n",
    "            i += 1\n",
    "            # Skip continuation lines\n",
    "            while i < len(lines) and (lines[i].startswith('|     ') or lines[i].startswith('    ')):\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        # Skip these specific lines entirely\n",
    "        if 'Coerce int96' in line or 'Use multithreading' in line:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        result.append(line)\n",
    "        i += 1\n",
    "\n",
    "    return '\\n'.join(result)\n",
    "\n",
    "buffer = StringIO()\n",
    "df_s3.explain(show_all=True, file=buffer)\n",
    "plan = buffer.getvalue()\n",
    "plan = simplify_explain(plan)\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
