{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47a565a",
   "metadata": {},
   "source": [
    "# Object stores and Parquet\n",
    "\n",
    "Object stores (like AWS S3) is the standard for Data Lakes. Unlike local files, accessing data over the network has high latency. Parquet is designed for this environment. It allows query engines to only download the specific chunks of data they need (Column Pruning and Predicate Pushdown), drastically reducing network traffic.\n",
    "\n",
    "## Setup \n",
    "\n",
    "Let's have a look how this works in more detail. We simulate an object storage using the `moto` AWS mocking library and a custom logger to print the requests that run over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import boto3\n",
    "import daft\n",
    "from werkzeug.serving import make_server, WSGIRequestHandler\n",
    "from werkzeug.urls import uri_to_iri\n",
    "from moto.server import DomainDispatcherApplication, create_backend_app\n",
    "\n",
    "os.environ[\"DAFT_DASHBOARD_ENABLED\"] = \"0\"\n",
    "os.environ[\"DAFT_PROGRESS_BAR\"] = \"0\"\n",
    "BUCKET = \"data-lake\"\n",
    "\n",
    "def custom_log_request(self, code='-', size='-'):\n",
    "    path = uri_to_iri(self.path)\n",
    "    if BUCKET in path:\n",
    "        range_header = self.headers.get('Range', 'No range')\n",
    "        code = str(code)\n",
    "        print('ðŸ“¡ %04s [%24s] %s %s %s ' % (self.command, range_header, path, code, size))\n",
    "\n",
    "WSGIRequestHandler.log_request = custom_log_request\n",
    "\n",
    "def start_s3_server(port=5000):\n",
    "    app = DomainDispatcherApplication(create_backend_app)\n",
    "    #app = RangeHeaderLogger(app)\n",
    "\n",
    "    server = make_server(\"127.0.0.1\", port, app)\n",
    "    thread = threading.Thread(target=server.serve_forever)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "    print(f\"S3 Server running on port {port}\")\n",
    "    return server\n",
    "\n",
    "server = start_s3_server()\n",
    "s3 = boto3.client(\"s3\", endpoint_url=\"http://127.0.0.1:5000\", aws_access_key_id=\"fake\", aws_secret_access_key=\"fake\", region_name=\"us-east-1\")\n",
    "_ = s3.create_bucket(Bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d72749",
   "metadata": {},
   "source": [
    "## Write to the object store\n",
    "\n",
    "Just like in the previous section, we'll sort and write some measurements with fairly small row groups to demonstrate pruning. This time, we write to the mocked object store. The file itself is the same, so you can still inspect the file from the previous section and compare it with this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_df = daft.read_json('../data/input/radiator.jsonl')\n",
    "radiator_sorted_df = radiator_df.sort([daft.col(\"source\")[\"value\"], daft.col(\"time\")])\n",
    "\n",
    "io_config = daft.io.IOConfig(s3=daft.io.S3Config(endpoint_url=\"http://127.0.0.1:5000\", anonymous=True))\n",
    "daft.set_execution_config(parquet_target_row_group_size=4*1024*1024)\n",
    "_ = radiator_sorted_df.write_parquet(f\"s3://{BUCKET}/radiator\", io_config=io_config, write_mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8eb4b6",
   "metadata": {},
   "source": [
    "## Query the object store\n",
    "\n",
    "Next, we'll prepare a query on a particular device that we know is at the end of the sorting order (and hence at the end of the file). Note how during preparing the query, a negative byte range is used. A negative byte range indicates an offset from the end of the object. What is at the end of a Parquet file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = daft.read_parquet(f\"s3://{BUCKET}/radiator\", io_config=io_config)\n",
    "df_s3 = df_s3.filter(daft.col(\"source\")[\"value\"] == \"1822301\").select(daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554aacd",
   "metadata": {},
   "source": [
    "Now, let's actually run the query. Note that all requests read only a part of the file. What part is this? Inspect the previous section's file, e.g., using \n",
    "\n",
    "```\n",
    "parquet-tools inspect --row-group 13 --column-chunk 22 â€¦.parquet| python3 -mjson.tool | less\n",
    "```\n",
    "\n",
    "(Columns chunk 2 is `source.value`, column chunk 22 is `meas_PressureBalance.current_ForceValue.value`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5197e4c",
   "metadata": {},
   "source": [
    "The ranges shown here are actually reaching from the start of column `source.value` to the end of column `meas_PressureBalance.current_ForceValue.value` in each row group. Daft seems to be too daft (sorry for the pun) to understand from the min and max values of `source.value` that only row group 13 needs to be queried. \n",
    "\n",
    "## Query the object store, second try\n",
    "\n",
    "However, if we flatten the `source` struct to a top-level column containing just the value, then sort by that value, the pruning actually works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_flat_df = radiator_df.with_column(\"source_id\", daft.col(\"source\")[\"value\"])\n",
    "radiator_flat_sorted_df = radiator_flat_df.sort([daft.col(\"source_id\"), daft.col(\"time\")])\n",
    "_ = radiator_flat_sorted_df.write_parquet(f\"s3://{BUCKET}/radiator_flat\", io_config=io_config, write_mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = daft.read_parquet(f\"s3://{BUCKET}/radiator_flat\", io_config=io_config)\n",
    "df_flat = df_flat.filter(daft.col(\"source_id\") == \"1822301\").select(daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374506a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74ea34",
   "metadata": {},
   "source": [
    "## Review questions\n",
    "\n",
    "**Why is network latency more important for object stores than local files?**\n",
    " - How does Parquet's design address this challenge?\n",
    "\n",
    "**What does a negative byte range request mean?**\n",
    " - Why is this useful for reading Parquet files?\n",
    "\n",
    "**Explain the sequence of HTTP requests when querying a Parquet file on S3.**\n",
    " - What gets fetched first? What can be skipped?\n",
    "\n",
    "**How much data transfer can be saved with good predicate pushdown?**\n",
    " - Consider a 1GB file where you need 1% of rows from one partition.\n",
    "\n",
    "**What are the challenges of using Parquet on object stores vs. local files?**\n",
    " - Think about consistency, listing operations, and metadata access.\n",
    "\n",
    "## Challenges \n",
    "\n",
    "### Measure network efficiency\n",
    "\n",
    "1. Run the same query on sorted vs. unsorted radiator data\n",
    "2. Compare the total bytes transferred by examining byte ranges in requests\n",
    "3. Calculate the percentage of the file that was actually read\n",
    "4. What's the theoretical minimum bytes needed for this query?\n",
    "\n",
    "### Multi-column filtering\n",
    "\n",
    "1. Create queries that filter on multiple columns:\n",
    "   - Filter by `source_id` AND `time` range\n",
    "   - Filter by nested measurement fields\n",
    "2. Observe which column chunks are fetched\n",
    "3. Explain the access pattern - does Daft fetch columns separately or together?\n",
    "\n",
    "### Partition simulation\n",
    "\n",
    "1. Create a dataset partitioned by `source_id` using the `partition_cols` argument of `write_parquet`.\n",
    "2. Compare S3 requests when querying:\n",
    "   - One specific device (one partition)\n",
    "   - All devices (all partitions)\n",
    "3. What metadata operations are needed? How many files are read?\n",
    "\n",
    "### Real object store comparison \n",
    "\n",
    "If you have AWS or Azure access:\n",
    "1. Upload the sorted radiator data to real S3 or ADLS object stores.\n",
    "2. Run the same queries as in the notebook\n",
    "3. Compare latency with the mocked version\n",
    "4. What's different? Consider:\n",
    "   - Network latency\n",
    "   - Service time\n",
    "   - Throttling or rate limits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zudli8fohfe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Parquet is built for querying large files on object stores. However, the querying technology is still fairly new and not all query engines support the entire range of Parquet features yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
