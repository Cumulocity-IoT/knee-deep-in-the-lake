{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47a565a",
   "metadata": {},
   "source": [
    "# Object stores and Parquet\n",
    "\n",
    "Object stores (like AWS S3) are the standard for Data Lakes. Unlike local files, accessing data over the network has high latency. Parquet is designed for this environment. It allows query engines to only download the specific chunks of data they need (Column Pruning and Predicate Pushdown), drastically reducing network traffic.\n",
    "\n",
    "## Setup \n",
    "\n",
    "Let's have a look how this works in more detail. We simulate an object storage using the `moto` AWS mocking library and a custom logger to print the requests that run over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import daft\n",
    "from s3simulator import S3Simulator\n",
    "\n",
    "os.environ[\"DAFT_DASHBOARD_ENABLED\"] = \"0\"\n",
    "os.environ[\"DAFT_PROGRESS_BAR\"] = \"0\"\n",
    "BUCKET = \"data-lake\"\n",
    "\n",
    "s3_sim = S3Simulator(bucket_name=BUCKET, port=5000)\n",
    "s3_sim.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d72749",
   "metadata": {},
   "source": [
    "## Write to the object store\n",
    "\n",
    "Just like in the previous section, we'll sort and write some measurements with fairly small row groups to demonstrate pruning. This time, we write to the mocked object store. The file itself is the same, so you can still inspect the file from the previous section and compare it with this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fa33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_df = daft.read_json('../data/input/radiator.jsonl')\n",
    "radiator_sorted_df = radiator_df.sort([daft.col(\"source\")[\"value\"], daft.col(\"time\")])\n",
    "\n",
    "io_config = daft.io.IOConfig(s3=daft.io.S3Config(endpoint_url=\"http://127.0.0.1:5000\", anonymous=True))\n",
    "daft.set_execution_config(parquet_target_row_group_size=4*1024*1024)\n",
    "_ = radiator_sorted_df.write_parquet(f\"s3://{BUCKET}/radiator\", io_config=io_config, write_mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8eb4b6",
   "metadata": {},
   "source": [
    "## Query the object store\n",
    "\n",
    "Next, we'll prepare a query on a particular device that we know is at the end of the sorting order (and hence at the end of the file). Note how during preparing the query, a negative byte range is used. A negative byte range indicates an offset from the end of the object. What is at the end of a Parquet file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = daft.read_parquet(f\"s3://{BUCKET}/radiator\", io_config=io_config)\n",
    "df_s3 = df_s3.filter(daft.col(\"source\")[\"value\"] == \"1822301\").select(daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554aacd",
   "metadata": {},
   "source": [
    "Now, let's actually run the query. Note that all requests read only a part of the file. What part is this? Inspect the previous section's file, e.g., using \n",
    "\n",
    "```\n",
    "parquet-tools inspect --row-group 13 --column-chunk 22 â€¦.parquet| python3 -mjson.tool | less\n",
    "```\n",
    "\n",
    "(Columns chunk 2 is `source.value`, column chunk 22 is `meas_PressureBalance.current_ForceValue.value`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5197e4c",
   "metadata": {},
   "source": [
    "The ranges shown here are actually reaching from the start of column `source.value` to the end of column `meas_PressureBalance.current_ForceValue.value` in each row group. Daft seems to be too daft (sorry for the pun) to understand from the min and max values of `source.value` that only row group 13 needs to be queried. \n",
    "\n",
    "## Query the object store, second try\n",
    "\n",
    "However, if we flatten the `source` struct to a top-level column containing just the value, then sort by that value, the pruning actually works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aaeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiator_flat_df = radiator_df.with_column(\"source_id\", daft.col(\"source\")[\"value\"])\n",
    "radiator_flat_sorted_df = radiator_flat_df.sort([daft.col(\"source_id\"), daft.col(\"time\")])\n",
    "_ = radiator_flat_sorted_df.write_parquet(f\"s3://{BUCKET}/radiator_flat\", io_config=io_config, write_mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = daft.read_parquet(f\"s3://{BUCKET}/radiator_flat\", io_config=io_config)\n",
    "df_flat = df_flat.filter(daft.col(\"source_id\") == \"1822301\").select(daft.col(\"meas_PressureBalance\")[\"current_ForceValue\"][\"value\"].alias(\"force\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374506a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_sim.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74ea34",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Parquet is built for querying large files on object stores. However, even though Parquet itself is mature, many query engines on top of Parquet are recent and subject to active development. They may not support the entire range of Parquet features yet. \n",
    "\n",
    "This section concludes the Parquet part. The next part covers the Iceberg table standard built on top of Parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
