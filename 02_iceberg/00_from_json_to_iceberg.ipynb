{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# From JSON to time travel in a second\n",
    "\n",
    "This notebook demonstrates the power of Apache Iceberg in just a few lines of code:\n",
    "\n",
    "* Read JSON data, create an Iceberg table and commit the data to the table\n",
    "* Query the table instantly with SQL\n",
    "* Append more data and query again\n",
    "* Travel back in time to the first commit\n",
    "\n",
    "All of this happens in seconds, with full ACID transactions and automatic versioning with the help of a so-called catalog. To get started, we'll use a simple SQLite-based catalog for demo purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from IPython.display import display\n",
    "\n",
    "warehouse_path = Path('../data/warehouse_quick_demo').absolute()\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)  # Fresh start\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    'demo',\n",
    "    **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'}\n",
    ")\n",
    "catalog.create_namespace('demo')\n",
    "print(\"✅ Catalog ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1997c97",
   "metadata": {},
   "source": [
    "Now we import some data, create an Iceberg table and insert the data into the table, then query it with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first 100K events from JSON\n",
    "df_events = daft.read_json('../data/input/events.jsonl')\n",
    "df_batch1 = df_events.limit(100000)\n",
    "\n",
    "# Convert to Arrow, create Iceberg table and append the data\n",
    "arrow_table = df_batch1.to_arrow()\n",
    "iceberg_table = catalog.create_table('demo.events', schema=pa.schema(arrow_table.schema))\n",
    "iceberg_table.append(arrow_table)\n",
    "\n",
    "# Query with Daft\n",
    "df = daft.read_iceberg(iceberg_table)\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b8543",
   "metadata": {},
   "source": [
    "Let's add some more data and query the table again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append next 100K events\n",
    "df_batch2 = df_events.offset(100000).limit(100000)\n",
    "arrow_table = df_batch2.to_arrow()\n",
    "iceberg_table.append(arrow_table)\n",
    "\n",
    "# Query again - now includes both batches\n",
    "df = daft.read_iceberg(iceberg_table)\n",
    "print(\"\\nTotal events after append:\")\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0504ee",
   "metadata": {},
   "source": [
    "Each commit to an Iceberg table creates a new version, and you can go back to older versions (\"time travel\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time travel: go back to first snapshot\n",
    "history = iceberg_table.history()\n",
    "first_snapshot_id = history[0].snapshot_id\n",
    "\n",
    "# Read data as it was in the first snapshot\n",
    "df_past = daft.read_iceberg(iceberg_table, snapshot_id=first_snapshot_id)\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df_past\").show()\n",
    "\n",
    "print(\"\\nYou just:\")\n",
    "print(\"  • Created a versioned table from JSON\")\n",
    "print(\"  • Queried it with SQL\")\n",
    "print(\"  • Appended new data\")\n",
    "print(\"  • Traveled back in time\")\n",
    "print(\"\\nAll with ACID transactions, no data rewrites, in seconds!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
