{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Getting Started with Apache Iceberg\n",
    "\n",
    "This notebook provides a comprehensive introduction to Apache Iceberg:\n",
    "\n",
    "* What is Iceberg?\n",
    "* Setting up Iceberg\n",
    "* Creating and managing tables\n",
    "* Appending data\n",
    "* Querying tables\n",
    "* Basic operations\n",
    "\n",
    "After going through this notebook, you should understand what Iceberg is and how to perform basic table operations.\n",
    "\n",
    "## What is Apache Iceberg?\n",
    "\n",
    "Iceberg is a table format for huge analytic datasets. Think of it as \"Git for data\":\n",
    "\n",
    "* **Parquet** is a **file** format (like individual source code files)\n",
    "* **Iceberg** is a **table** format (like a Git repository that versions those files)\n",
    "\n",
    "The Iceberg table format is [standardized under the umbrella of the Apache Software Foundation](https://iceberg.apache.org/terms/). \n",
    "\n",
    "\n",
    "### Key problems Iceberg solves\n",
    "\n",
    "Working with raw Parquet files in a data lake has challenges:\n",
    "\n",
    "1. **No ACID transactions**: What if your write fails halfway through or you query the file halfway through? You get inconsistent data.\n",
    "2. **No schema evolution**: Adding a column means rewriting all files.\n",
    "3. **Slow queries**: Query engines must list and scan all files to find relevant data.\n",
    "4. **No time travel**: Can't query data as it was yesterday.\n",
    "5. **Unsafe concurrent writes**: Two writers can corrupt each other's changes.\n",
    "\n",
    "### How Iceberg fixes this\n",
    "\n",
    "Iceberg adds a **metadata layer** on top of Parquet files:\n",
    "\n",
    "```\n",
    "Traditional Data Lake       Iceberg Data Lake\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Parquet Files  â”‚        â”‚ Iceberg Catalog â”‚  â† Points to current metadata\n",
    "â”‚  (scattered,    â”‚        â”‚                 â”‚\n",
    "â”‚   untracked)    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "                            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                            â”‚ Metadata JSON  â”‚  â† Schema, snapshots, history\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                            â”‚ Manifest Files â”‚  â† Lists of data files + stats\n",
    "                            â”‚   (AVRO)       â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                            â”‚ Parquet Files  â”‚  â† Actual data (immutable)\n",
    "                            â”‚   (tracked,    â”‚\n",
    "                            â”‚    versioned)  â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This separation of metadata and data provides:\n",
    "\n",
    "* **ACID transactions**: All-or-nothing commits via atomic metadata updates\n",
    "* **Schema evolution**: Change schema in metadata without touching data files\n",
    "* **Fast queries**: Read manifests to find relevant files (no directory listing)\n",
    "* **Time travel**: Each snapshot points to a specific set of files\n",
    "* **Safe concurrency**: Optimistic locking detects conflicts\n",
    "\n",
    "### Iceberg vs. other approaches\n",
    "\n",
    "| Approach | Schema Changes | Time Travel | ACID | Concurrent Writes |\n",
    "|----------|---------------|-------------|------|------------------|\n",
    "| **Raw Parquet** | Rewrite all data | Manual snapshots | âŒ | âŒ |\n",
    "| **Parquet + Hive** | Rewrite all data | Manual snapshots | âŒ | âš ï¸ (locking issues) |\n",
    "| **Iceberg** | Metadata only | Built-in | âœ… | âœ… (optimistic) |\n",
    "| **Delta Lake** | Metadata only | Built-in | âœ… | âœ… (optimistic) |\n",
    "\n",
    "*Note: Delta Lake is similar to Iceberg but is more tightly coupled to Spark. Iceberg works with any engine (Daft, Spark, Trino, Flink, etc.)*\n",
    "\n",
    "### When to use Iceberg\n",
    "\n",
    "Use Iceberg when you need:\n",
    "\n",
    "* **Large-scale analytics**: Billions of rows, millions of files\n",
    "* **Schema flexibility**: Schema changes without downtime\n",
    "* **Data governance**: Auditing, versioning, compliance\n",
    "* **Concurrent access**: Multiple readers and writers\n",
    "* **Query performance**: Fast queries on partitioned data\n",
    "\n",
    "Don't use Iceberg for:\n",
    "\n",
    "* **Small datasets**: < 1 GB - the metadata overhead isn't worth it\n",
    "* **Single-file scenarios**: Just use Parquet directly\n",
    "* **Real-time updates**: Simmilar to Parquet, Iceberg is for batch/micro-batch, not streaming updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setting up Iceberg\n",
    "\n",
    "Iceberg requires a **catalog** to manage table metadata. The catalog stores:\n",
    "\n",
    "* Table locations\n",
    "* Current metadata file pointers\n",
    "* Namespace (database) information\n",
    "\n",
    "We'll use **SqlCatalog** with SQLite for learning purposes. SqlCatalog with SQLite is **not** suitable for production concurrent writes, but it's perfect for learning and development.\n",
    "\n",
    "For production use, Iceberg provides a standard REST Catalog API with a production implementation such as \n",
    "\n",
    "* **[Apache Polaris](https://polaris.apache.org/)**: Java-based implementation, cooperatively developed by Snowflake and Dremio.\n",
    "* **[Lakekeeper](https://docs.lakekeeper.io/)**: An independent, lightweight Rust-based implementation.\n",
    "\n",
    "Let's set up again the catalog and create a namespace in it to hold our tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow as pa\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect_iceberg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_path = Path('../data/warehouse_getting_started').absolute()\n",
    "shutil.rmtree(warehouse_path, ignore_errors=True)\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "catalog = SqlCatalog(\n",
    "    'getting_started',\n",
    "    **{\n",
    "        'uri': f'sqlite:///{catalog_db}',\n",
    "        'warehouse': f'file://{warehouse_path}'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a namespace (like a database schema)\n",
    "catalog.create_namespace('iot')\n",
    "\n",
    "print(f\"âœ… Catalog initialized at {warehouse_path}\")\n",
    "print(f\"   Catalog DB: {catalog_db}\")\n",
    "print(f\"   Namespace 'iot' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creating_tables",
   "metadata": {},
   "source": [
    "## Creating tables\n",
    "\n",
    "There are two main approaches to create Iceberg tables:\n",
    "\n",
    "1. **Define schema explicitly**: Create table with a pre-defined schema, in this case through PyArrow\n",
    "2. **Infer from data**: Let Iceberg tools discover the schema from your data\n",
    "\n",
    "Choosing the right approach depends on your environment. For example, in the IoT space, it is often incredibly hard to centrally define a schema that all the devices from different vendors in your IoT installation and align it with software deployments to all devices. For this reason, Cumulocity infers all schemas from the source data to not cause unintentional data loss.\n",
    "\n",
    "### Approach 1: Explicit schema\n",
    "\n",
    "Use this when you know your schema upfront and want strict validation (i.e., rejecting data that is not adhering to the schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_explicit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema explicitly\n",
    "schema = pa.schema([\n",
    "    pa.field('device_id', pa.string(), nullable=False),\n",
    "    pa.field('timestamp', pa.timestamp('ms'), nullable=False),\n",
    "    pa.field('temperature', pa.float64()),\n",
    "    pa.field('humidity', pa.float64()),\n",
    "])\n",
    "\n",
    "# Create table with explicit schema\n",
    "sensors_table = catalog.create_table(\n",
    "    'iot.sensors',\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "print(\"âœ… Created 'iot.sensors' table with explicit schema\")\n",
    "print(f\"   Location: {sensors_table.location()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approach2",
   "metadata": {},
   "source": [
    "### Approach 2: Infer schema from data\n",
    "\n",
    "Use this when you're working with JSON or other semi-structured data and want to ingest all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_inferred",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = daft.read_json('../data/input/events.jsonl')\n",
    "df_sample = df_events.limit(50000)\n",
    "arrow_table = df_sample.to_arrow()\n",
    "\n",
    "print(f\"ğŸ“Š Loaded {len(arrow_table):,} events from JSON\")\n",
    "print(f\"   Auto-discovered schema: {arrow_table.schema}\")\n",
    "\n",
    "# Create Iceberg table from inferred schema\n",
    "events_table = catalog.create_table(\n",
    "    'iot.events',\n",
    "    schema=pa.schema(arrow_table.schema)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Created 'iot.events' table\")\n",
    "print(f\"   Location: {events_table.location()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appending",
   "metadata": {},
   "source": [
    "## Appending data\n",
    "\n",
    "Appending data to an Iceberg table creates a new **snapshot**. Each snapshot is an immutable view of the table at a point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "append_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Append the events data we loaded earlier\n",
    "events_table.append(arrow_table)\n",
    "\n",
    "print(f\"âœ… Appended {len(arrow_table):,} records\")\n",
    "\n",
    "# Check the table history\n",
    "history = events_table.history()\n",
    "print(f\"\\nğŸ“œ Table now has {len(history)} snapshot(s)\")\n",
    "for i, snapshot in enumerate(history, 1):\n",
    "    time = datetime.fromtimestamp(snapshot.timestamp_ms / 1000)\n",
    "    print(f\"   Snapshot {i}: ID {snapshot.snapshot_id}, Time: {time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_metadata",
   "metadata": {},
   "source": [
    "### What happened behind the scenes?\n",
    "\n",
    "When you appended data:\n",
    "\n",
    "1. **Data file created**: A new Parquet file was written (to `data/`)\n",
    "2. **Manifest created**: An AVRO manifest file lists this new data file (to `metadata/`)\n",
    "3. **Metadata updated**: A new metadata JSON file was created (to `metadata/`)\n",
    "4. **Catalog updated**: The catalog now points to the new metadata file\n",
    "\n",
    "Since the catalog update moves the pointer as part of a catalog database transaction, the entire update is **atomic**. From the outside, either it everything happened or nothing. (It may leave some leftover files in the latter case, though.)\n",
    "\n",
    "### What got created?\n",
    "\n",
    "When you create a table, Iceberg creates this directory structure:\n",
    "\n",
    "```\n",
    "warehouse_getting_started/\n",
    "â””â”€â”€ iot/                  â† Namespace\n",
    "    â””â”€â”€ events/           â† Table\n",
    "        â”œâ”€â”€ data/         â† Parquet files go here (the data that we appended above)\n",
    "        â””â”€â”€ metadata/     â† Metadata JSON and manifest AVRO files\n",
    "    â””â”€â”€ sensors/          â† Table\n",
    "        â””â”€â”€ metadata/     â† Just the metadata JSON, no data here yet\n",
    "â””â”€â”€ catalog.db            â† The SQlite database\n",
    "```\n",
    "\n",
    "Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde29d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the warehouse structure\n",
    "import os\n",
    "for root, dirs, files in os.walk(warehouse_path):\n",
    "    level = root.replace(str(warehouse_path), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files only\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 5:\n",
    "        print(f'{subindent}... and {len(files) - 5} more files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "querying",
   "metadata": {},
   "source": [
    "The `events` table has now two versions, one starting with `00000` and one starting with `00001`. The first one only contains the table description, the second adds a snapshot with the snapshot manifest (`snap-*`) listing the manifests (`*-m0.avro`) with files and statistics. We will look more closely into this in the next section.\n",
    "\n",
    "## Querying tables\n",
    "\n",
    "We again use Daft to read and query Iceberg tables. Daft also understands Iceberg's metadata. Using the metadata, it can do optimizations like column pruning and predicate pushdown that we have discussed in the Parquet chapter also on millions of Parquet files without actually reading each of them.\n",
    "\n",
    "Let's try again our queries, this time on the Iceberg table instead of the raw Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daft.read_iceberg(events_table)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of events by type\n",
    "daft.sql(\"\"\"\n",
    "    SELECT type, COUNT(*) as count\n",
    "    FROM df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show devices with the most events, this time using the Dataframe API instead of SQL\n",
    "(\n",
    "    df\n",
    "    .groupby('source')\n",
    "    .agg(daft.col('source').count().alias('event_count'))\n",
    "    .sort('event_count', desc=True)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_ops",
   "metadata": {},
   "source": [
    "## Basic operations\n",
    "\n",
    "Let's explore common table operations:\n",
    "\n",
    "* **Append**: Add more data\n",
    "* **Delete**: Remove records matching a condition\n",
    "* **Overwrite**: Replace table contents\n",
    "\n",
    "Each operation creates a new snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "append_more",
   "metadata": {},
   "source": [
    "### Appending more data\n",
    "\n",
    "Let's load an additional batch of events and add it to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "append_batch2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch2 = df_events.offset(50000).limit(50000)\n",
    "batch2_arrow = df_batch2.to_arrow()\n",
    "events_table.append(batch2_arrow)\n",
    "\n",
    "df = daft.read_iceberg(events_table)\n",
    "daft.sql(\"\"\"\n",
    "    SELECT type, COUNT(*) as count\n",
    "    FROM df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_ops",
   "metadata": {},
   "source": [
    "### Deleting records\n",
    "\n",
    "Iceberg supports **row-level deletes** using predicates. This creates **delete files** that mark rows as deleted without rewriting the entire data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_records",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_table.delete(\"type = 'OperationMode'\")\n",
    "\n",
    "df = daft.read_iceberg(events_table)\n",
    "daft.sql(\"\"\"\n",
    "    SELECT type, COUNT(*) as count\n",
    "    FROM df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overwrite_ops",
   "metadata": {},
   "source": [
    "### Overwriting data\n",
    "\n",
    "Overwrite replaces the entire table contents with new data. This is useful for reprocessing or fixing data issues. We'll just use a very small sample to show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overwrite_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table = catalog.create_table(\n",
    "    'iot.sample',\n",
    "    schema=pa.schema([pa.field('id', pa.int64()), pa.field('value', pa.string())])\n",
    ")\n",
    "\n",
    "data1 = pa.table({'id': [1, 2, 3], 'value': ['a', 'b', 'c']})\n",
    "sample_table.append(data1)\n",
    "daft.read_iceberg(sample_table).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pa.table({'id': [10, 20], 'value': ['x', 'y']})\n",
    "sample_table.overwrite(data2)\n",
    "daft.read_iceberg(sample_table).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_table",
   "metadata": {},
   "source": [
    "## Inspecting table metadata\n",
    "\n",
    "Let's use our helper function to visualize the complete table structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_events",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_iceberg_table(events_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review_questions",
   "metadata": {},
   "source": [
    "## Review questions\n",
    "\n",
    "**What problem does Iceberg solve that Parquet alone doesn't?**\n",
    " - Think about transactions, versioning, and metadata management.\n",
    "\n",
    "**Why is metadata separation important for large datasets?**\n",
    " - Hint: What happens when you need to list millions of files in S3?\n",
    "\n",
    "**How is Iceberg different from just versioning Parquet files in folders?**\n",
    " - Consider: `data/2024-01-01/`, `data/2024-01-02/` vs. Iceberg snapshots\n",
    "\n",
    "**What happens to data files when you delete records?**\n",
    " - Are the Parquet files rewritten? What gets created instead?\n",
    "\n",
    "**Why does each operation create a new snapshot?**\n",
    " - What would happen without snapshots?\n",
    "\n",
    "**Can you query a table while someone else is writing to it?**\n",
    " - Think about snapshot isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Create a table from cmdata.jsonl\n",
    "\n",
    "1. Load `../data/input/cmdata.jsonl`\n",
    "2. Create an Iceberg table called `iot.devices`\n",
    "3. Query the table to find how many devices have alarms\n",
    "\n",
    "### Explore metadata files\n",
    "\n",
    "1. Install `avro-tools`\n",
    "   * Use `brew install avro-tools` on a Mac\n",
    "   * Or get the [Jar from Maven Central](https://mvnrepository.com/artifact/org.apache.avro/avro-tools) and run it\n",
    "2. Find the metadata JSON file for your events table and open it in a text editor\n",
    "3. Find the `current-snapshot-id` value. Can you locate the corresponding snapshot in the `snapshots` array?\n",
    "4. Check the snapshot and manifest Avro files using Avro Tools (`avro-tools tojson --pretty <file>`)\n",
    "5. How are all files linked?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "* **What Iceberg is**: A table format that adds ACID transactions, schema evolution, and time travel to data lakes\n",
    "* **Why it matters**: Solves key problems with raw Parquet files (no transactions, slow queries, no versioning)\n",
    "* **Setting up**: Using SqlCatalog with SQLite for development\n",
    "* **Creating tables**: Both explicit schema and inferred from data\n",
    "* **Appending data**: Each append creates a new immutable snapshot\n",
    "* **Querying**: Using Daft with SQL and DataFrame API\n",
    "* **Basic operations**: Append, delete, overwrite - all with ACID guarantees\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "1. **Metadata is the magic** (again!): Iceberg's metadata layer enables all its features\n",
    "2. **Snapshots are immutable**: Once created, a snapshot never changes\n",
    "3. **Operations are atomic**: All-or-nothing commits via catalog updates\n",
    "4. **Query engines read metadata**: Daft reads manifests to find relevant files\n",
    "5. **Data files are never edited**: Deletes create delete files, not rewrites\n",
    "\n",
    "In the next section, we'll look deeper into metadata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
