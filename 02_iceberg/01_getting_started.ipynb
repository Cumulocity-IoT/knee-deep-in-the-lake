{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Getting Started with Apache Iceberg\n",
    "\n",
    "This notebook provides a comprehensive introduction to Apache Iceberg:\n",
    "\n",
    "* What is Iceberg?\n",
    "* Setting up Iceberg\n",
    "* Creating and managing tables\n",
    "* Appending data\n",
    "* Querying tables\n",
    "* Basic operations\n",
    "\n",
    "After going through this notebook, you should understand what Iceberg is and how to perform basic table operations.\n",
    "\n",
    "## What is Apache Iceberg?\n",
    "\n",
    "Iceberg is a **table format** for huge analytic datasets. Think of it as \"Git for data\":\n",
    "\n",
    "* **Parquet** is a file format (like individual source code files)\n",
    "* **Iceberg** is a table format (like a Git repository that versions those files)\n",
    "\n",
    "### Key Problems Iceberg Solves\n",
    "\n",
    "Working with raw Parquet files in a data lake has challenges:\n",
    "\n",
    "1. **No ACID transactions**: What if your write fails halfway through? You get inconsistent data.\n",
    "2. **No schema evolution**: Adding a column means rewriting all files.\n",
    "3. **Slow queries**: Query engines must list and scan all files to find relevant data.\n",
    "4. **No time travel**: Can't query data as it was yesterday.\n",
    "5. **Unsafe concurrent writes**: Two writers can corrupt each other's changes.\n",
    "\n",
    "### How Iceberg Fixes This\n",
    "\n",
    "Iceberg adds a **metadata layer** on top of Parquet files:\n",
    "\n",
    "```\n",
    "Traditional Data Lake          Iceberg Data Lake\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Parquet Files  â”‚           â”‚ Iceberg Catalog â”‚  â† Points to current metadata\n",
    "â”‚  (scattered,    â”‚           â”‚                 â”‚\n",
    "â”‚   untracked)    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "                               â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                               â”‚ Metadata JSON  â”‚  â† Schema, snapshots, history\n",
    "                               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                               â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                               â”‚ Manifest Files â”‚  â† Lists of data files + stats\n",
    "                               â”‚   (AVRO)       â”‚\n",
    "                               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                               â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                               â”‚ Parquet Files  â”‚  â† Actual data (immutable)\n",
    "                               â”‚   (tracked,    â”‚\n",
    "                               â”‚    versioned)  â”‚\n",
    "                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This separation of metadata and data provides:\n",
    "\n",
    "* **ACID transactions**: All-or-nothing commits via atomic metadata updates\n",
    "* **Schema evolution**: Change schema in metadata without touching data files\n",
    "* **Fast queries**: Read manifests to find relevant files (no directory listing)\n",
    "* **Time travel**: Each snapshot points to a specific set of files\n",
    "* **Safe concurrency**: Optimistic locking detects conflicts\n",
    "\n",
    "### Iceberg vs. Other Approaches\n",
    "\n",
    "| Approach | Schema Changes | Time Travel | ACID | Concurrent Writes |\n",
    "|----------|---------------|-------------|------|------------------|\n",
    "| **Raw Parquet** | Rewrite all data | Manual snapshots | âŒ | âŒ |\n",
    "| **Parquet + Hive** | Rewrite all data | Manual snapshots | âŒ | âš ï¸ (locking issues) |\n",
    "| **Iceberg** | Metadata only | Built-in | âœ… | âœ… (optimistic) |\n",
    "| **Delta Lake** | Metadata only | Built-in | âœ… | âœ… (optimistic) |\n",
    "\n",
    "*Note: Delta Lake is similar to Iceberg but is more tightly coupled to Spark. Iceberg works with any engine (Daft, Spark, Trino, Flink, etc.)*\n",
    "\n",
    "### When to Use Iceberg\n",
    "\n",
    "Use Iceberg when you need:\n",
    "\n",
    "* **Large-scale analytics**: Billions of rows, millions of files\n",
    "* **Schema flexibility**: Schema changes without downtime\n",
    "* **Data governance**: Auditing, versioning, compliance\n",
    "* **Concurrent access**: Multiple readers and writers\n",
    "* **Query performance**: Fast queries on partitioned data\n",
    "\n",
    "Don't use Iceberg for:\n",
    "\n",
    "* **Small datasets**: < 1 GB - the metadata overhead isn't worth it\n",
    "* **Single-file scenarios**: Just use Parquet directly\n",
    "* **Real-time updates**: Iceberg is for batch/micro-batch, not streaming updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setting Up Iceberg\n",
    "\n",
    "Iceberg requires a **catalog** to manage table metadata. The catalog stores:\n",
    "\n",
    "* Table locations\n",
    "* Current metadata file pointers\n",
    "* Namespace (database) information\n",
    "\n",
    "We'll use **SqlCatalog** with SQLite for learning purposes. In production, you'd use:\n",
    "\n",
    "* **AWS Glue Catalog** (for AWS)\n",
    "* **Nessie** (Git-like catalog with branches)\n",
    "* **REST Catalog** (generic HTTP-based catalog)\n",
    "* **Hive Metastore** (for legacy compatibility)\n",
    "\n",
    "Note: SqlCatalog with SQLite is **not** suitable for production concurrent writes, but it's perfect for learning and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow as pa\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect_iceberg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "warehouse_path = Path('../data/warehouse_getting_started').absolute()\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "\n",
    "# Clean up previous runs for a fresh start\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "# Initialize SQL-based Iceberg catalog\n",
    "catalog = SqlCatalog(\n",
    "    'getting_started',\n",
    "    **{\n",
    "        'uri': f'sqlite:///{catalog_db}',\n",
    "        'warehouse': f'file://{warehouse_path}'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a namespace (like a database schema)\n",
    "catalog.create_namespace('iot')\n",
    "\n",
    "print(f\"âœ… Catalog initialized at {warehouse_path}\")\n",
    "print(f\"   Catalog DB: {catalog_db}\")\n",
    "print(f\"   Namespace 'iot' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creating_tables",
   "metadata": {},
   "source": [
    "## Creating Tables\n",
    "\n",
    "There are two main approaches to create Iceberg tables:\n",
    "\n",
    "1. **Define schema explicitly**: Create table with a PyArrow schema\n",
    "2. **Infer from data**: Let Iceberg discover the schema from your data\n",
    "\n",
    "### Approach 1: Explicit Schema\n",
    "\n",
    "Use this when you know your schema upfront and want strict validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_explicit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema explicitly\n",
    "schema = pa.schema([\n",
    "    pa.field('device_id', pa.string(), nullable=False),\n",
    "    pa.field('timestamp', pa.timestamp('ms'), nullable=False),\n",
    "    pa.field('temperature', pa.float64()),\n",
    "    pa.field('humidity', pa.float64()),\n",
    "])\n",
    "\n",
    "# Create table with explicit schema\n",
    "sensors_table = catalog.create_table(\n",
    "    'iot.sensors',\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "print(\"âœ… Created 'iot.sensors' table with explicit schema\")\n",
    "print(f\"   Location: {sensors_table.location()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approach2",
   "metadata": {},
   "source": [
    "### Approach 2: Infer Schema from Data\n",
    "\n",
    "Use this when you're working with JSON or other semi-structured data and want quick ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_inferred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample of events data\n",
    "jsonl_file = Path('../data/input/events.jsonl')\n",
    "\n",
    "with jsonl_file.open('r') as f:\n",
    "    # Read first 50K events\n",
    "    events_data = [json.loads(line) for i, line in enumerate(f) if i < 50000]\n",
    "\n",
    "# Convert to Arrow table (schema is auto-discovered)\n",
    "arrow_table = pa.Table.from_pylist(events_data)\n",
    "\n",
    "print(f\"ğŸ“Š Loaded {len(events_data):,} events from JSON\")\n",
    "print(f\"   Auto-discovered schema: {arrow_table.schema}\")\n",
    "\n",
    "# Create Iceberg table from inferred schema\n",
    "events_table = catalog.create_table(\n",
    "    'iot.events',\n",
    "    schema=pa.schema(arrow_table.schema)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Created 'iot.events' table\")\n",
    "print(f\"   Location: {events_table.location()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_structure",
   "metadata": {},
   "source": [
    "### What Got Created?\n",
    "\n",
    "When you create a table, Iceberg creates this directory structure:\n",
    "\n",
    "```\n",
    "warehouse/\n",
    "â””â”€â”€ iot/\n",
    "    â””â”€â”€ events/\n",
    "        â”œâ”€â”€ data/         â† Parquet files go here (empty for now)\n",
    "        â””â”€â”€ metadata/     â† Metadata JSON files\n",
    "```\n",
    "\n",
    "Let's inspect the warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_dirs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the warehouse structure\n",
    "import os\n",
    "for root, dirs, files in os.walk(warehouse_path):\n",
    "    level = root.replace(str(warehouse_path), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files only\n",
    "        print(f'{subindent}{file}')\n",
    "    if len(files) > 5:\n",
    "        print(f'{subindent}... and {len(files) - 5} more files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appending",
   "metadata": {},
   "source": [
    "## Appending Data\n",
    "\n",
    "Appending data to an Iceberg table creates a new **snapshot**. Each snapshot is an immutable view of the table at a point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "append_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the events data we loaded earlier\n",
    "events_table.append(arrow_table)\n",
    "\n",
    "print(f\"âœ… Appended {len(events_data):,} records\")\n",
    "print(f\"   This created Snapshot 1\")\n",
    "\n",
    "# Check the table history\n",
    "history = events_table.history()\n",
    "print(f\"\\nğŸ“œ Table now has {len(history)} snapshot(s)\")\n",
    "for i, snapshot in enumerate(history, 1):\n",
    "    print(f\"   Snapshot {i}: ID {snapshot.snapshot_id}, Timestamp: {snapshot.timestamp_ms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_metadata",
   "metadata": {},
   "source": [
    "### What Happened Behind the Scenes?\n",
    "\n",
    "When you appended data:\n",
    "\n",
    "1. **Data file created**: A new Parquet file was written to `data/`\n",
    "2. **Manifest created**: An AVRO manifest file lists this new data file\n",
    "3. **Metadata updated**: A new metadata JSON file was created\n",
    "4. **Catalog updated**: The catalog now points to the new metadata file\n",
    "\n",
    "This is an **atomic operation** - either all of this happens, or none of it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files were created\n",
    "events_dir = Path(events_table.location().replace('file://', ''))\n",
    "data_files = list(events_dir.glob('data/*.parquet'))\n",
    "metadata_files = list(events_dir.glob('metadata/*.json'))\n",
    "manifest_files = list(events_dir.glob('metadata/*.avro'))\n",
    "\n",
    "print(f\"ğŸ“‚ Files created for {len(events_data):,} records:\")\n",
    "print(f\"   Parquet data files: {len(data_files)}\")\n",
    "print(f\"   Metadata JSON files: {len(metadata_files)}\")\n",
    "print(f\"   Manifest AVRO files: {len(manifest_files)}\")\n",
    "\n",
    "if data_files:\n",
    "    data_size = sum(f.stat().st_size for f in data_files)\n",
    "    print(f\"\\n   Total data size: {data_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "querying",
   "metadata": {},
   "source": [
    "## Querying Tables\n",
    "\n",
    "We use **Daft** to read and query Iceberg tables. Daft understands Iceberg's metadata and can perform optimizations like:\n",
    "\n",
    "* **Column pruning**: Only read the columns you need\n",
    "* **Predicate pushdown**: Skip files that don't match your filters\n",
    "* **Partition pruning**: Skip entire partitions (we'll cover this later)\n",
    "\n",
    "Daft provides two query interfaces:\n",
    "\n",
    "1. **SQL**: Familiar SQL syntax\n",
    "2. **DataFrame API**: Similar to Pandas/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Iceberg table with Daft\n",
    "df = daft.read_iceberg(events_table)\n",
    "\n",
    "# Show schema\n",
    "print(\"Schema:\")\n",
    "print(df.schema())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query: Count total events\n",
    "print(\"Total events:\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT COUNT(*) as total\n",
    "    FROM df\n",
    "\"\"\").show()\n",
    "\n",
    "# SQL query: Top event types\n",
    "print(\"\\nTop 10 event types:\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT type, COUNT(*) as count\n",
    "    FROM df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame API: Same queries as above\n",
    "print(\"Using DataFrame API:\")\n",
    "\n",
    "# Top event sources (devices)\n",
    "(\n",
    "    df\n",
    "    .groupby('source')\n",
    "    .agg(daft.col('source').count().alias('event_count'))\n",
    "    .sort('event_count', desc=True)\n",
    "    .limit(10)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_ops",
   "metadata": {},
   "source": [
    "## Basic Operations\n",
    "\n",
    "Let's explore common table operations:\n",
    "\n",
    "* **Append**: Add more data\n",
    "* **Overwrite**: Replace table contents\n",
    "* **Delete**: Remove records matching a condition\n",
    "\n",
    "Each operation creates a new snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "append_more",
   "metadata": {},
   "source": [
    "### Appending More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "append_batch2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load next batch of events (next 50K)\n",
    "with jsonl_file.open('r') as f:\n",
    "    batch2_data = [json.loads(line) for i, line in enumerate(f) if 50000 <= i < 100000]\n",
    "\n",
    "batch2_arrow = pa.Table.from_pylist(batch2_data)\n",
    "events_table.append(batch2_arrow)\n",
    "\n",
    "print(f\"âœ… Appended {len(batch2_data):,} more records (Snapshot 2)\")\n",
    "\n",
    "# Verify the count increased\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"\\nNew total:\")\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delete_ops",
   "metadata": {},
   "source": [
    "### Deleting Records\n",
    "\n",
    "Iceberg supports **row-level deletes** using predicates. This creates **delete files** that mark rows as deleted without rewriting the entire data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_records",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all events of a specific type\n",
    "events_table.delete(\"type = 'c8y_LocationUpdate'\")\n",
    "\n",
    "print(\"âœ… Deleted 'c8y_LocationUpdate' events (Snapshot 3)\")\n",
    "\n",
    "# Verify they're gone\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"\\nRemaining events by type:\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT type, COUNT(*) as count\n",
    "    FROM df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overwrite_ops",
   "metadata": {},
   "source": [
    "### Overwriting Data\n",
    "\n",
    "Overwrite replaces the entire table contents with new data. This is useful for reprocessing or fixing data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overwrite_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample table to demonstrate overwrite\n",
    "sample_table = catalog.create_table(\n",
    "    'iot.sample',\n",
    "    schema=pa.schema([pa.field('id', pa.int64()), pa.field('value', pa.string())])\n",
    ")\n",
    "\n",
    "# Write initial data\n",
    "data1 = pa.table({'id': [1, 2, 3], 'value': ['a', 'b', 'c']})\n",
    "sample_table.append(data1)\n",
    "print(\"Initial data:\")\n",
    "daft.read_iceberg(sample_table).show()\n",
    "\n",
    "# Overwrite with new data\n",
    "data2 = pa.table({'id': [10, 20], 'value': ['x', 'y']})\n",
    "sample_table.overwrite(data2)\n",
    "print(\"\\nAfter overwrite:\")\n",
    "daft.read_iceberg(sample_table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_table",
   "metadata": {},
   "source": [
    "## Inspecting Table Metadata\n",
    "\n",
    "Let's use our helper function to visualize the complete table structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_events",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the events table\n",
    "inspect_iceberg_table(events_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review_questions",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **What problem does Iceberg solve that Parquet alone doesn't?**\n",
    "   - Think about transactions, versioning, and metadata management.\n",
    "\n",
    "2. **Why is metadata separation important for large datasets?**\n",
    "   - Hint: What happens when you need to list millions of files in S3?\n",
    "\n",
    "3. **How is Iceberg different from just versioning Parquet files in folders?**\n",
    "   - Consider: `data/2024-01-01/`, `data/2024-01-02/` vs. Iceberg snapshots\n",
    "\n",
    "4. **What happens to data files when you delete records?**\n",
    "   - Are the Parquet files rewritten? What gets created instead?\n",
    "\n",
    "5. **Why does each operation create a new snapshot?**\n",
    "   - What would happen without snapshots?\n",
    "\n",
    "6. **Can you query a table while someone else is writing to it?**\n",
    "   - Think about snapshot isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Hands-on Challenge\n",
    "\n",
    "Now it's your turn! Try these exercises:\n",
    "\n",
    "### Challenge 1: Create a Table from cmdata.jsonl\n",
    "\n",
    "1. Load `../data/input/cmdata.jsonl`\n",
    "2. Create an Iceberg table called `iot.devices`\n",
    "3. Query the table to find how many devices have alarms\n",
    "\n",
    "### Challenge 2: Append and Verify\n",
    "\n",
    "1. Append the first 1000 records from events.jsonl to a new table\n",
    "2. Append the next 1000 records\n",
    "3. Verify the total count is 2000\n",
    "4. Inspect the table history - how many snapshots?\n",
    "\n",
    "### Challenge 3: Explore Metadata Files\n",
    "\n",
    "1. Find the metadata JSON file for your events table\n",
    "2. Open it in a text editor or use `inspect_metadata_json()` (covered in next notebook)\n",
    "3. Find the `current-snapshot-id` value\n",
    "4. Can you locate the corresponding snapshot in the `snapshots` array?\n",
    "\n",
    "Use the cells below for your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "* **What Iceberg is**: A table format that adds ACID transactions, schema evolution, and time travel to data lakes\n",
    "* **Why it matters**: Solves key problems with raw Parquet files (no transactions, slow queries, no versioning)\n",
    "* **Setting up**: Using SqlCatalog with SQLite for development\n",
    "* **Creating tables**: Both explicit schema and inferred from data\n",
    "* **Appending data**: Each append creates a new immutable snapshot\n",
    "* **Querying**: Using Daft with SQL and DataFrame API\n",
    "* **Basic operations**: Append, delete, overwrite - all with ACID guarantees\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Metadata is the magic**: Iceberg's metadata layer enables all its features\n",
    "2. **Snapshots are immutable**: Once created, a snapshot never changes\n",
    "3. **Operations are atomic**: All-or-nothing commits via catalog updates\n",
    "4. **Query engines read metadata**: Daft reads manifests to find relevant files\n",
    "5. **Data files are never edited**: Deletes create delete files, not rewrites\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebooks, we'll dive deeper into:\n",
    "\n",
    "* **Metadata structures**: What's inside the JSON and AVRO files?\n",
    "* **Time travel**: Query historical data, rollback changes\n",
    "* **Schema evolution**: Add/remove columns without rewrites\n",
    "* **Concurrency**: How optimistic locking works\n",
    "* **Partitioning**: Scale to millions of files\n",
    "* **Object stores**: Iceberg on S3 with predicate pushdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
