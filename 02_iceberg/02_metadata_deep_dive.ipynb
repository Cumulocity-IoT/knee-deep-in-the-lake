{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Inside the Iceberg: Metadata structures\n",
    "\n",
    "In the previous notebook, we saw Iceberg from the outside - creating tables, appending data, querying. Now let's look inside to understand exactly how Iceberg works.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "```\n",
    "The catalog database: What's actually inside the SQLite file?\n",
    "  â””â”€ Metadata file (JSON): Schema, snapshots, and table history\n",
    "       â””â”€ Manifest List (AVRO): What's inside a snapshot?\n",
    "            â””â”€ Manifest (AVRO): Lists of data files and statistics\n",
    "                â””â”€ Data files (Parquet): The actual data\n",
    "```\n",
    "\n",
    "and finally look at the complete picture once more. By the end, you'll understand how Iceberg achieves atomic commits, time travel, and fast queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import daft\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect_metadata_json, inspect_manifest, inspect_manifest_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_data",
   "metadata": {},
   "source": [
    "## Setup: Create a table with history\n",
    "\n",
    "First, let's create a table with multiple snapshots so we have interesting metadata to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_path = Path('../data/warehouse_metadata').absolute()\n",
    "shutil.rmtree(warehouse_path, ignore_errors=True)\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    'metadata_demo',\n",
    "    **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'}\n",
    ")\n",
    "catalog.create_namespace('demo')\n",
    "print(\"âœ… Catalog initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events data and create table with multiple operations\n",
    "df_events = daft.read_json('../data/input/events.jsonl')\n",
    "\n",
    "# Snapshot 1: Initial data\n",
    "print(\"Snapshot 1: Creating initial load...\")\n",
    "df_batch1 = df_events.limit(30000)\n",
    "arrow_table = df_batch1.to_arrow()\n",
    "events_table = catalog.create_table('demo.events', schema=pa.schema(arrow_table.schema))\n",
    "events_table.append(arrow_table)\n",
    "print(f\"Snapshot 1: Appended {len(arrow_table):,} records\")\n",
    "\n",
    "# Snapshot 2: Append more\n",
    "print(\"Snapshot 2: Appending more data...\")\n",
    "df_batch2 = df_events.offset(30000).limit(30000)\n",
    "arrow_table = df_batch2.to_arrow()\n",
    "events_table.append(arrow_table)\n",
    "print(f\"Snapshot 2: Appended {len(arrow_table):,} more records\")\n",
    "\n",
    "# Snapshot 3: Delete some records\n",
    "print(\"Snapshot 3: Deleting OperationMode events...\")\n",
    "events_table.delete(\"type = 'OperationMode'\")\n",
    "print(\"Snapshot 3: Deleted OperationMode events\")\n",
    "\n",
    "print(f\"\\nâœ… Created table with {len(events_table.history())} snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog_db",
   "metadata": {},
   "source": [
    "## The catalog database\n",
    "\n",
    "```\n",
    "The catalog database: What's actually inside the SQLite file? â† We are here!\n",
    "  â””â”€ Metadata file (JSON): Schema, snapshots, and table history\n",
    "       â””â”€ Manifest List (AVRO): What's inside a snapshot?\n",
    "            â””â”€ Manifest (AVRO): Lists of data files and statistics\n",
    "                â””â”€ Data files (Parquet): The actual data\n",
    "```\n",
    "\n",
    "The catalog database is the **entry point** to all Iceberg tables. It's in our case a simple SQLite database that stores:\n",
    "\n",
    "* **Table locations**: Where each table's metadata lives\n",
    "* **Namespace properties**: Configuration for database schemas\n",
    "* **Atomic pointers**: Current metadata file for each table\n",
    "\n",
    "### Why use a catalog?\n",
    "\n",
    "The catalog enables **atomic commits**. When a writer updates a table, the catalog\n",
    "\n",
    "1. Writes a new metadata JSON file\n",
    "2. Updates the catalog pointer atomically (SQL UPDATE)\n",
    "3. In case of conflicts, returns an error and the client has to retry (optimistic concurrency)\n",
    "\n",
    "The catalog is the **single source of truth** for which metadata file is current.\n",
    "\n",
    "### Inspecting the catalog\n",
    "\n",
    "Let's look inside the SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(catalog_db)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in catalog database:\")\n",
    "for table in tables:\n",
    "    print(f\"  â€¢ {table[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog_schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of iceberg_tables\n",
    "cursor.execute(\"PRAGMA table_info(iceberg_tables)\")\n",
    "columns = cursor.fetchall()\n",
    "print(\"Schema of 'iceberg_tables':\")\n",
    "for col in columns:\n",
    "    print(f\"  {col[1]}: {col[2]}\")\n",
    "\n",
    "cursor.execute(\"PRAGMA table_info(iceberg_namespace_properties)\")\n",
    "columns = cursor.fetchall()\n",
    "print(\"\\nSchema of 'iceberg_namespace_properties':\")\n",
    "for col in columns:\n",
    "    print(f\"  {col[1]}: {col[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM iceberg_tables\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(\"Registered Iceberg tables\")\n",
    "for row in rows:\n",
    "    catalog_name, namespace, table_name, metadata_location, prev_metadata = row\n",
    "    print(f\"{namespace}.{table_name}\")\n",
    "    print(f\"  Current metadata: {Path(metadata_location).name}\")\n",
    "    if prev_metadata:\n",
    "        print(f\"  Previous metadata: {Path(prev_metadata).name}\")\n",
    "    print()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM iceberg_namespace_properties\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(\"Registered namespaces:\")\n",
    "for row in rows:\n",
    "    catalog_name, namespace, key, value = row\n",
    "    print(f\"{catalog_name}.{namespace}\")\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_catalog",
   "metadata": {},
   "source": [
    "### What we see\n",
    "\n",
    "The `iceberg_tables` table has:\n",
    "\n",
    "* **metadata_location**: Points to the **current** metadata JSON file\n",
    "* **previous_metadata_location**: Points to the **previous** metadata JSON file\n",
    "\n",
    "This is how Iceberg achieves **atomic commits**:\n",
    "\n",
    "```sql\n",
    "UPDATE iceberg_tables\n",
    "SET metadata_location = 'new_metadata.json',\n",
    "    previous_metadata_location = 'old_metadata.json'\n",
    "WHERE table_name = 'events'\n",
    "  AND metadata_location = 'old_metadata.json'  -- Optimistic lock!\n",
    "```\n",
    "\n",
    "If two writers try to commit at the same time:\n",
    "- First succeeds (updates the row)\n",
    "- Second fails (WHERE clause doesn't match anymore)\n",
    "- Second must retry with the new metadata\n",
    "\n",
    "This is **optimistic concurrency control**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close_conn",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata_json",
   "metadata": {},
   "source": [
    "## Metadata file (JSON)\n",
    "\n",
    "```\n",
    "The catalog database: What's actually inside the SQLite file?\n",
    "  â””â”€ Metadata file (JSON): Schema, snapshots, and table history â† We are here!\n",
    "       â””â”€ Manifest List (AVRO): What's inside a snapshot?\n",
    "            â””â”€ Manifest (AVRO): Lists of data files and statistics\n",
    "                â””â”€ Data files (Parquet): The actual data\n",
    "```\n",
    "\n",
    "The metadata file represents a version of an Iceberg table. Each commit creates a **new JSON file**. This file contains:\n",
    "\n",
    "* **Schema versions**: All schema versions (for time travel)\n",
    "* **Partition specs**: All partition specs (for partition evolution)\n",
    "* **Snapshots**: All snapshots with their manifest lists\n",
    "* **Snapshot log**: Chronological list of snapshots\n",
    "* **Current snapshot ID**: Pointer to the current snapshot\n",
    "* **Metadata log**: History of metadata files\n",
    "\n",
    "Let's find and inspect a metadata JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find metadata files\n",
    "table_dir = Path(events_table.location().replace('file://', ''))\n",
    "metadata_files = sorted(table_dir.glob('metadata/*.metadata.json'))\n",
    "\n",
    "print(f\"Found {len(metadata_files)} metadata file(s):\")\n",
    "for i, mf in enumerate(metadata_files, 1):\n",
    "    size = mf.stat().st_size\n",
    "    print(f\"  {i}. {mf.name} ({size:,} bytes, {size/1024:.1f} KB)\")\n",
    "\n",
    "# Use the latest metadata file\n",
    "latest_metadata = metadata_files[-1]\n",
    "print(f\"\\nUsing latest: {latest_metadata.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b5a28",
   "metadata": {},
   "source": [
    "There is a lot of stuff in the file. Here is a visualization of the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = inspect_metadata_json(latest_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schemas",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "Iceberg stores **all schema versions** in the metadata. Each schema has a unique ID.\n",
    "\n",
    "When you read a snapshot, Iceberg uses the schema that was current at that snapshot. This enables:\n",
    "* **Time travel with old schemas**\n",
    "* **Schema evolution without rewrites**\n",
    "\n",
    "### Snapshots\n",
    "\n",
    "Each snapshot represents a **commit** to the table. Snapshots contain:\n",
    "\n",
    "* **snapshot-id**: Unique identifier\n",
    "* **timestamp-ms**: When this snapshot was created\n",
    "* **manifest-list**: Path to AVRO file listing manifests\n",
    "* **schema-id**: Which schema version to use\n",
    "* **summary**: Statistics (operation, files added/deleted, records added/deleted)\n",
    "\n",
    "### Logs\n",
    "\n",
    "In addition (not shown in the visualization):\n",
    "\n",
    "* The `snapshot-log` is a chronological list of snapshots with timestamps. This enables:\n",
    "  * **Time travel by timestamp**: \"Show me data as of 2024-12-01\"\n",
    "  * **Audit trail**: When was each commit made?\n",
    "* The `metadata-log` tracks which metadata files existed and when. This is used for:\n",
    "  * **Metadata file expiration**: Clean up old metadata files\n",
    "  * **Debugging**: Understand table history\n",
    "  * **Consistency checks**: Verify metadata chain\n",
    "\n",
    "The logs are kept separate because entries in `snapshots` may be cleaned up separately as we will see.\n",
    "\n",
    "Can you guess from the metadata how the delete operation in the third snapshot was handled by Daft? \n",
    "\n",
    "Feel free to open the JSON file directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7n7efx22vs8",
   "metadata": {},
   "source": [
    "## Manifest lists (AVRO)\n",
    "\n",
    "```\n",
    "The catalog database: What's actually inside the SQLite file?\n",
    "  â””â”€ Metadata file (JSON): Schema, snapshots, and table history\n",
    "       â””â”€ Manifest List (AVRO): What's inside a snapshot? â† We are here!\n",
    "            â””â”€ Manifest (AVRO): Lists of data files and statistics\n",
    "                â””â”€ Data files (Parquet): The actual data\n",
    "```\n",
    "\n",
    "Let's continue our trace. Each snapshot in the metadata JSON points to a **manifest list** file. The manifest list is itself a binary [AVRO](https://avro.apache.org/) file that contains references to individual manifest files.\n",
    "\n",
    "The manifest list acts as an **index of indices** - it tracks which manifest files are part of this snapshot, along with statistics about each manifest (like number of files, number of rows, partition info).\n",
    "\n",
    "It provides several important capabilities:\n",
    "\n",
    "* **Snapshot isolation**: Multiple snapshots can share the same manifest files. The manifest list specifies which manifests belong to this snapshot.\n",
    "* **Aggregated statistics**: Total file counts and row counts per manifest enable quick query planning without reading all manifests.\n",
    "* **Partition pruning**: For partitioned tables, manifest lists contain partition bounds to skip entire manifests based on filters.\n",
    "* **Content tracking**: Separate manifests for data files (content=0) and delete files (content=1) are tracked together.\n",
    "\n",
    "Let's check out what is inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1mv96qhyrfxi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the current snapshot from the metadata\n",
    "current_snapshot_id = metadata['current-snapshot-id']\n",
    "current_snapshot = next(s for s in metadata['snapshots'] if s['snapshot-id'] == current_snapshot_id)\n",
    "\n",
    "# Extract manifest list path\n",
    "manifest_list_path = current_snapshot['manifest-list']\n",
    "manifest_list_file = Path(manifest_list_path.replace('file://', ''))\n",
    "\n",
    "# Visualize and capture the entries for later use\n",
    "manifest_list_entries = inspect_manifest_list(manifest_list_file, latest_metadata.name, current_snapshot_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifests",
   "metadata": {},
   "source": [
    "## Manifest files (AVRO)\n",
    "\n",
    "```\n",
    "The catalog database: What's actually inside the SQLite file?\n",
    "  â””â”€ Metadata file (JSON): Schema, snapshots, and table history\n",
    "       â””â”€ Manifest List (AVRO): What's inside a snapshot?\n",
    "            â””â”€ Manifest (AVRO): Lists of data files and statistics â† We are here!\n",
    "                â””â”€ Data files (Parquet): The actual data\n",
    "```\n",
    "\n",
    "Now let's look at the individual manifest files referenced by the manifest list. Each manifest file contains the actual list of data files (Parquet files) with detailed statistics.\n",
    "* **Data file paths**: Where the Parquet files are\n",
    "* **Partition values**: What partition each file belongs to\n",
    "* **Statistics**: Record counts, min/max values, null counts\n",
    "* **File metadata**: Size, format, compression\n",
    "* **Status**: Whether the file is ADDED, EXISTING, or DELETED\n",
    "\n",
    "This metadata enables **predicate pushdown** - skipping files without reading them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_manifests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract manifest paths from the manifest list and collect data file paths\n",
    "print(f\"The manifest list references {len(manifest_list_entries)} manifest file(s).\")\n",
    "\n",
    "# Collect all data file paths from all manifests\n",
    "all_data_file_paths = []\n",
    "for i, entry in enumerate(manifest_list_entries, 1):\n",
    "    manifest_path = Path(entry['manifest_path'].replace('file://', ''))\n",
    "    data_file_paths = inspect_manifest(manifest_path)\n",
    "    all_data_file_paths.extend(data_file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_files",
   "metadata": {},
   "source": [
    "## Data files (Parquet)\n",
    "\n",
    "```\n",
    "The catalog database: What's actually inside the SQLite file?\n",
    "  â””â”€ Metadata file (JSON): Schema, snapshots, and table history\n",
    "       â””â”€ Manifest List (AVRO): What's inside a snapshot?\n",
    "            â””â”€ Manifest (AVRO): Lists of data files and statistics\n",
    "                â””â”€ Data files (Parquet): The actual data â† We are here!\n",
    "```\n",
    "\n",
    "Finally, the actual data! Data files are standard **Parquet files**. Iceberg doesn't change Parquet - it just tracks them in manifests. It makes sure that you always see valid, complete files and no stale data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_picture",
   "metadata": {},
   "source": [
    "## The complete picture: Tracing a query\n",
    "\n",
    "Now let's trace how a query uses all these metadata structures:\n",
    "\n",
    "```\n",
    "SELECT * FROM events WHERE type = 'OperationMode' AND time > '2026-01-01'\n",
    "```\n",
    "\n",
    "### Step-by-step query execution\n",
    "\n",
    "1. **Catalog lookup** (SQLite)\n",
    "   - Query: `SELECT metadata_location FROM iceberg_tables WHERE table_name = 'events'`\n",
    "   - Result: Path to current metadata JSON file\n",
    "\n",
    "2. **Read metadata JSON**\n",
    "   - Parse: `current-snapshot-id`\n",
    "   - Find snapshot with that ID\n",
    "   - Get: `manifest-list` path\n",
    "\n",
    "3. **Read manifest list** (AVRO)\n",
    "   - Lists all manifest files for this snapshot\n",
    "   - Each manifest has aggregated statistics\n",
    "   - Can skip entire manifests based on partition bounds\n",
    "\n",
    "4. **Read manifests** (AVRO)\n",
    "   - For each manifest (that wasn't skipped), check file-level statistics:\n",
    "     - Does `lower_bounds['type']` â‰¤ 'OperationMode' â‰¤ `upper_bounds['type']`?\n",
    "     - Does `lower_bounds['time']` â‰¤ '2026-01-01' â‰¤ `upper_bounds['time']`?\n",
    "   - If not: **skip this data file entirely**\n",
    "   - If yes: add file to the read list\n",
    "\n",
    "5. **Read data files** (Parquet)\n",
    "   - Read only files that passed predicate pushdown\n",
    "   - Within each file, read only necessary columns\n",
    "   - Apply row-level filters\n",
    "\n",
    "This is why Iceberg is fast. \n",
    "\n",
    "* Everything outside of the catalog database is only written once and can be safely cached. \n",
    "* It reads minimal metadata to skip most of the data!\n",
    "\n",
    "### Visualizing the hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete Iceberg metadata hierarchy:\\n\")\n",
    "print(\"1. ðŸ“š Catalog (SQLite)\")\n",
    "print(f\"   {catalog_db.name}\")\n",
    "print(f\"   â””â”€ Table: demo.events â†’ {latest_metadata.name}\")\n",
    "print()\n",
    "print(\"2. ðŸ“„ Metadata JSON\")\n",
    "print(f\"   {latest_metadata.name}\")\n",
    "print(f\"   â”œâ”€ Schema: {len(metadata['schemas'])} version(s)\")\n",
    "print(f\"   â”œâ”€ Partition specs: {len(metadata['partition-specs'])}\")\n",
    "print(f\"   â””â”€ Snapshots: {len(metadata['snapshots'])}\")\n",
    "print()\n",
    "print(\"3. ðŸ“‹ Manifest List (AVRO)\")\n",
    "print(f\"   {manifest_list_file.name} ({manifest_list_file.stat().st_size:,} bytes)\")\n",
    "print(f\"   â””â”€ References {len(manifest_list_entries)} manifest file(s)\")\n",
    "print()\n",
    "print(\"4. ðŸ“¦ Manifest Files (AVRO)\")\n",
    "for i, entry in enumerate(manifest_list_entries, 1):\n",
    "    manifest_path = Path(entry['manifest_path'].replace('file://', ''))\n",
    "    print(f\"   {manifest_path.name} ({manifest_path.stat().st_size:,} bytes)\")\n",
    "print()\n",
    "print(\"5. ðŸ’¾ Data Files (Parquet)\")\n",
    "for i, df in enumerate(all_data_file_paths, 1):\n",
    "    print(f\"   {df.name} ({df.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "print()\n",
    "\n",
    "# Calculate overhead including manifest list\n",
    "manifest_list_size = manifest_list_file.stat().st_size\n",
    "manifest_files_size = sum(Path(entry['manifest_path'].replace('file://', '')).stat().st_size\n",
    "                          for entry in manifest_list_entries)\n",
    "total_metadata_size = sum(mf.stat().st_size for mf in metadata_files) + manifest_list_size + manifest_files_size\n",
    "print(f\"Total metadata overhead: {total_metadata_size / 1024:.1f} KB\")\n",
    "print(f\"  - Metadata JSON: {sum(mf.stat().st_size for mf in metadata_files) / 1024:.1f} KB\")\n",
    "print(f\"  - Manifest list: {manifest_list_size / 1024:.1f} KB\")\n",
    "print(f\"  - Manifest files: {manifest_files_size / 1024:.1f} KB\")\n",
    "print(f\"Total data size: {sum(df.stat().st_size for df in all_data_file_paths) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review questions\n",
    "\n",
    "**Why store metadata in multiple JSON files instead of one?**\n",
    " - Think about atomicity and append-only operations.\n",
    "\n",
    "**What would happen if you directly edited a data file?**\n",
    " - Would the manifest notice? Would queries see your changes?\n",
    "\n",
    "**How does Iceberg achieve atomic commits?**\n",
    " - What SQL statement is used? What makes it atomic?\n",
    "\n",
    "**Why have both manifest lists and manifest files?**\n",
    " - What additional capabilities does the manifest list provide?\n",
    "\n",
    "**How does predicate pushdown work across the metadata layers?**\n",
    " - At what levels can files/partitions be skipped?\n",
    "\n",
    "**What's the relationship between the schema in Parquet files and the schema in Iceberg metadata?**\n",
    " - Can they differ? What might happen if they do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Be the janitor and clean up \n",
    "\n",
    "1. Open the latest metadata JSON in a text editor and find the `current-snapshot-id`\n",
    "3. Locate that snapshot in the `snapshots` array and extract the `manifest-list` path\n",
    "5. Verify that this file exists in the metadata directory and list it using `avro-tools` (see last section).\n",
    "6. Locate the manifest paths in the manifest list and open each using `avro-tools`.\n",
    "7. Locate the referenced data files.\n",
    "8. Determine what can files and metadata can be deleted if you just want to preserve the latest snapshot.\n",
    "\n",
    "### Analyze metadata overhead\n",
    "\n",
    "1. Load 100 rows of radiator data into an Iceberg table.\n",
    "2. Calculate the total amount of metadata stored in JSON, AVRO and Parquet files. \n",
    "3. Calculate the net data amount stored in the Parquet files (dictionary and data pages).\n",
    "4. Calcuate the ratio of metadata and data size. \n",
    "5. Repeat for 1000, 10000, 100000 rows. What are economic sizes for batches of data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this deep dive, we explored the complete Iceberg metadata hierarchy:\n",
    "\n",
    "* **Catalog database**: The atomic pointer to current metadata\n",
    "  - Enables optimistic concurrency control\n",
    "  - Single UPDATE statement makes commits atomic\n",
    "\n",
    "* **Metadata JSON**: Complete table state\n",
    "  - All schema versions (for time travel)\n",
    "  - All snapshots with manifest lists\n",
    "  - Snapshot log for temporal queries\n",
    "  - Metadata log for file management\n",
    "\n",
    "* **Manifest lists**: Index of manifest files\n",
    "  - AVRO format for efficiency\n",
    "  - Aggregated statistics per manifest\n",
    "  - Enables manifest-level pruning\n",
    "  - Tracks file status across snapshots\n",
    "\n",
    "* **Manifest files**: Index of data files\n",
    "  - AVRO format with detailed statistics\n",
    "  - Per-file record counts and bounds\n",
    "  - Partition information\n",
    "  - Enables fine-grained predicate pushdown\n",
    "\n",
    "* **Data files**: Immutable Parquet\n",
    "  - Never modified after creation\n",
    "  - Referenced by manifests\n",
    "  - Standard Parquet format\n",
    "\n",
    "### Key insights\n",
    "\n",
    "1. **Layered metadata**: Each layer provides increasingly detailed information\n",
    "2. **Metadata is append-only**: New files created, old ones retained\n",
    "3. **Multiple pruning levels**: Skip entire manifests or individual files\n",
    "4. **Statistics enable pruning**: At manifest list, manifest, and file level\n",
    "5. **Everything is versioned**: Time travel works by reading old snapshots\n",
    "6. **Minimal metadata overhead**: ~KB of metadata per GB of data\n",
    "\n",
    "In the next section, we'll look at how the metadata structure supports time travel. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
