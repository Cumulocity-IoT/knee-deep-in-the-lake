{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Inside the Iceberg: Metadata Structures\n",
    "\n",
    "In the previous notebook, we saw Iceberg from the outside - creating tables, appending data, querying. Now let's look inside to understand exactly how Iceberg works.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "* **The Catalog Database**: What's in the SQLite file?\n",
    "* **Metadata JSON Files**: Schema, snapshots, and table history\n",
    "* **Manifest Files (AVRO)**: Lists of data files with statistics\n",
    "* **Data Files (Parquet)**: The actual data\n",
    "* **The Complete Picture**: How a query uses all these pieces\n",
    "\n",
    "By the end, you'll understand how Iceberg achieves atomic commits, time travel, and fast queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import daft\nimport pyarrow as pa\nfrom pathlib import Path\nfrom pyiceberg.catalog.sql import SqlCatalog\nimport sqlite3\nfrom datetime import datetime\n\n%reload_ext autoreload\n%autoreload 2\nfrom helpers import inspect_iceberg_table, inspect_metadata_json, inspect_manifest"
  },
  {
   "cell_type": "markdown",
   "id": "create_data",
   "metadata": {},
   "source": [
    "## Setup: Create a Table with History\n",
    "\n",
    "First, let's create a table with multiple snapshots so we have interesting metadata to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup warehouse\n",
    "warehouse_path = Path('../data/warehouse_metadata').absolute()\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    'metadata_demo',\n",
    "    **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'}\n",
    ")\n",
    "catalog.create_namespace('demo')\n",
    "print(\"‚úÖ Catalog initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": "# Load events data and create table with multiple operations\ndf_events = daft.read_json('../data/input/events.jsonl')\n\n# Snapshot 1: Initial data\nprint(\"Snapshot 1: Creating initial load...\")\ndf_batch1 = df_events.limit(30000)\narrow_table = df_batch1.to_arrow()\nevents_table = catalog.create_table('demo.events', schema=pa.schema(arrow_table.schema))\nevents_table.append(arrow_table)\nprint(f\"Snapshot 1: Appended {len(arrow_table):,} records\")\n\n# Snapshot 2: Append more\nprint(\"Snapshot 2: Appending more data...\")\ndf_batch2 = df_events.offset(30000).limit(30000)\narrow_table = df_batch2.to_arrow()\nevents_table.append(arrow_table)\nprint(f\"Snapshot 2: Appended {len(arrow_table):,} more records\")\n\n# Snapshot 3: Delete some records\nprint(\"Snapshot 3: Deleting LocationUpdate events...\")\nevents_table.delete(\"type = 'c8y_LocationUpdate'\")\nprint(\"Snapshot 3: Deleted LocationUpdate events\")\n\nprint(f\"\\n‚úÖ Created table with {len(events_table.history())} snapshots\")"
  },
  {
   "cell_type": "markdown",
   "id": "catalog_db",
   "metadata": {},
   "source": [
    "## The Catalog Database\n",
    "\n",
    "The catalog database is the **entry point** to all Iceberg tables. It's a simple SQLite database (in our case) that stores:\n",
    "\n",
    "* **Table locations**: Where each table's metadata lives\n",
    "* **Namespace properties**: Configuration for database schemas\n",
    "* **Atomic pointers**: Current metadata file for each table\n",
    "\n",
    "### Why Use a Catalog?\n",
    "\n",
    "The catalog enables **atomic commits**. When a writer updates a table:\n",
    "\n",
    "1. Write new metadata JSON file\n",
    "2. Update catalog pointer atomically (SQL UPDATE)\n",
    "3. If UPDATE fails ‚Üí commit failed, retry\n",
    "\n",
    "The catalog is the **single source of truth** for which metadata file is current.\n",
    "\n",
    "### Inspecting the Catalog\n",
    "\n",
    "Let's look inside the SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the catalog database\n",
    "conn = sqlite3.connect(catalog_db)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Show all tables in the catalog database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in catalog database:\")\n",
    "for table in tables:\n",
    "    print(f\"  ‚Ä¢ {table[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog_schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of iceberg_tables\n",
    "cursor.execute(\"PRAGMA table_info(iceberg_tables)\")\n",
    "columns = cursor.fetchall()\n",
    "print(\"Schema of 'iceberg_tables':\")\n",
    "for col in columns:\n",
    "    print(f\"  {col[1]}: {col[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the iceberg_tables table\n",
    "cursor.execute(\"SELECT * FROM iceberg_tables\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(\"Registered Iceberg tables:\\n\")\n",
    "for row in rows:\n",
    "    catalog_name, namespace, table_name, metadata_location, prev_metadata = row\n",
    "    print(f\"Table: {namespace}.{table_name}\")\n",
    "    print(f\"  Current metadata: {Path(metadata_location).name}\")\n",
    "    if prev_metadata:\n",
    "        print(f\"  Previous metadata: {Path(prev_metadata).name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_catalog",
   "metadata": {},
   "source": [
    "### What We See\n",
    "\n",
    "The `iceberg_tables` table has:\n",
    "\n",
    "* **metadata_location**: Points to the **current** metadata JSON file\n",
    "* **previous_metadata_location**: Points to the **previous** metadata JSON file\n",
    "\n",
    "This is how Iceberg achieves **atomic commits**:\n",
    "\n",
    "```sql\n",
    "UPDATE iceberg_tables\n",
    "SET metadata_location = 'new_metadata.json',\n",
    "    previous_metadata_location = 'old_metadata.json'\n",
    "WHERE table_name = 'events'\n",
    "  AND metadata_location = 'old_metadata.json'  -- Optimistic lock!\n",
    "```\n",
    "\n",
    "If two writers try to commit at the same time:\n",
    "- First succeeds (updates the row)\n",
    "- Second fails (WHERE clause doesn't match anymore)\n",
    "- Second must retry with the new metadata\n",
    "\n",
    "This is **optimistic concurrency control**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close_conn",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata_json",
   "metadata": {},
   "source": [
    "## Metadata JSON Files\n",
    "\n",
    "Each commit creates a **new metadata JSON file**. This file contains:\n",
    "\n",
    "* **Schema versions**: All schema versions (for time travel)\n",
    "* **Partition specs**: All partition specs (for partition evolution)\n",
    "* **Snapshots**: All snapshots with their manifest lists\n",
    "* **Snapshot log**: Chronological list of snapshots\n",
    "* **Current snapshot ID**: Pointer to the current snapshot\n",
    "* **Metadata log**: History of metadata files\n",
    "\n",
    "Let's find and inspect a metadata JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find metadata files\n",
    "table_dir = Path(events_table.location().replace('file://', ''))\n",
    "metadata_files = sorted(table_dir.glob('metadata/*.metadata.json'))\n",
    "\n",
    "print(f\"Found {len(metadata_files)} metadata file(s):\")\n",
    "for i, mf in enumerate(metadata_files, 1):\n",
    "    size = mf.stat().st_size\n",
    "    print(f\"  {i}. {mf.name} ({size:,} bytes, {size/1024:.1f} KB)\")\n",
    "\n",
    "# Use the latest metadata file\n",
    "latest_metadata = metadata_files[-1]\n",
    "print(f\"\\nUsing latest: {latest_metadata.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the metadata JSON\n",
    "with open(latest_metadata, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Show top-level structure\n",
    "print(\"Top-level keys in metadata JSON:\")\n",
    "for key in metadata.keys():\n",
    "    value = metadata[key]\n",
    "    if isinstance(value, list):\n",
    "        print(f\"  ‚Ä¢ {key}: list with {len(value)} item(s)\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"  ‚Ä¢ {key}: dict with {len(value)} key(s)\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metadata_details",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show key metadata values\n",
    "print(\"Key metadata values:\\n\")\n",
    "print(f\"Table UUID: {metadata['table-uuid']}\")\n",
    "print(f\"Format version: {metadata['format-version']}\")\n",
    "print(f\"Location: {metadata['location']}\")\n",
    "print(f\"Last updated: {datetime.fromtimestamp(metadata['last-updated-ms']/1000).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Current snapshot ID: {metadata['current-snapshot-id']}\")\n",
    "print(f\"Last sequence number: {metadata['last-sequence-number']}\")\n",
    "print(f\"\\nNumber of schemas: {len(metadata['schemas'])}\")\n",
    "print(f\"Number of partition specs: {len(metadata['partition-specs'])}\")\n",
    "print(f\"Number of snapshots: {len(metadata['snapshots'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schemas",
   "metadata": {},
   "source": [
    "### Schemas in Metadata\n",
    "\n",
    "Iceberg stores **all schema versions** in the metadata. Each schema has a unique ID.\n",
    "\n",
    "When you read a snapshot, Iceberg uses the schema that was current at that snapshot. This enables:\n",
    "* **Time travel with old schemas**\n",
    "* **Schema evolution without rewrites**\n",
    "\n",
    "Let's inspect the schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_schemas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all schemas\n",
    "for schema in metadata['schemas']:\n",
    "    print(f\"Schema ID {schema['schema-id']}:\")\n",
    "    print(f\"  Fields: {len(schema['fields'])}\")\n",
    "    for field in schema['fields'][:3]:  # Show first 3 fields\n",
    "        print(f\"    ‚Ä¢ {field['name']} (ID {field['id']}): {field['type']}\")\n",
    "    if len(schema['fields']) > 3:\n",
    "        print(f\"    ... and {len(schema['fields']) - 3} more fields\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snapshots",
   "metadata": {},
   "source": [
    "### Snapshots in Metadata\n",
    "\n",
    "Each snapshot represents a **commit** to the table. Snapshots contain:\n",
    "\n",
    "* **snapshot-id**: Unique identifier\n",
    "* **timestamp-ms**: When this snapshot was created\n",
    "* **manifest-list**: Path to AVRO file listing manifests\n",
    "* **schema-id**: Which schema version to use\n",
    "* **summary**: Statistics (operation, files added/deleted, records added/deleted)\n",
    "\n",
    "Let's inspect the snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_snapshots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all snapshots\n",
    "print(f\"Total snapshots: {len(metadata['snapshots'])}\\n\")\n",
    "\n",
    "for i, snapshot in enumerate(metadata['snapshots'], 1):\n",
    "    print(f\"Snapshot {i}: ID {snapshot['snapshot-id']}\")\n",
    "    print(f\"  Timestamp: {datetime.fromtimestamp(snapshot['timestamp-ms']/1000).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  Sequence number: {snapshot.get('sequence-number', 'N/A')}\")\n",
    "    print(f\"  Schema ID: {snapshot['schema-id']}\")\n",
    "    print(f\"  Manifest list: {Path(snapshot['manifest-list']).name}\")\n",
    "\n",
    "    if 'summary' in snapshot:\n",
    "        print(\"  Summary:\")\n",
    "        for key, value in sorted(snapshot['summary'].items()):\n",
    "            print(f\"    {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snapshot_log",
   "metadata": {},
   "source": [
    "### Snapshot Log\n",
    "\n",
    "The `snapshot-log` is a chronological list of snapshots with timestamps. This enables:\n",
    "* **Time travel by timestamp**: \"Show me data as of 2024-12-01\"\n",
    "* **Audit trail**: When was each commit made?\n",
    "\n",
    "The log is separate from `snapshots` because expired snapshots are removed from `snapshots` but their log entries might be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snapshot_log_inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show snapshot log\n",
    "print(\"Snapshot log (chronological):\")\n",
    "for entry in metadata['snapshot-log']:\n",
    "    snap_id = entry['snapshot-id']\n",
    "    timestamp = datetime.fromtimestamp(entry['timestamp-ms']/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    is_current = snap_id == metadata['current-snapshot-id']\n",
    "    marker = \" ‚Üê CURRENT\" if is_current else \"\"\n",
    "    print(f\"  {timestamp}: Snapshot {snap_id}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata_log",
   "metadata": {},
   "source": [
    "### Metadata Log\n",
    "\n",
    "The `metadata-log` tracks which metadata files existed and when. This is used for:\n",
    "* **Metadata file expiration**: Clean up old metadata files\n",
    "* **Debugging**: Understand table history\n",
    "* **Consistency checks**: Verify metadata chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metadata_log_inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show metadata log\n",
    "if 'metadata-log' in metadata:\n",
    "    print(\"Metadata file history:\")\n",
    "    for entry in metadata['metadata-log']:\n",
    "        meta_file = Path(entry['metadata-file']).name\n",
    "        timestamp = datetime.fromtimestamp(entry['timestamp-ms']/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"  {timestamp}: {meta_file}\")\n",
    "else:\n",
    "    print(\"No metadata log (first metadata file)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use_helper",
   "metadata": {},
   "source": [
    "### Using the Helper Function\n",
    "\n",
    "Now let's use our helper to visualize the metadata in a more readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_metadata_json(latest_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifests",
   "metadata": {},
   "source": [
    "## Manifest Files (AVRO)\n",
    "\n",
    "Manifests are the **index** that tells Iceberg which data files exist and where they are. The hierarchy is:\n",
    "\n",
    "```\n",
    "Snapshot\n",
    "  ‚îî‚îÄ Manifest List (AVRO) ‚Üê Points to multiple manifests\n",
    "       ‚îú‚îÄ Manifest 1 (AVRO) ‚Üê Lists data files for partition 1\n",
    "       ‚îú‚îÄ Manifest 2 (AVRO) ‚Üê Lists data files for partition 2\n",
    "       ‚îî‚îÄ Manifest N (AVRO) ‚Üê Lists data files for partition N\n",
    "```\n",
    "\n",
    "Each **manifest file** contains:\n",
    "* **Data file paths**: Where the Parquet files are\n",
    "* **Partition values**: What partition each file belongs to\n",
    "* **Statistics**: Record counts, min/max values, null counts\n",
    "* **File metadata**: Size, format, compression\n",
    "\n",
    "This metadata enables **predicate pushdown** - skipping files without reading them.\n",
    "\n",
    "### Finding Manifest Files\n",
    "\n",
    "Manifest files are named with pattern: `<uuid>-m<N>.avro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_manifests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find manifest files\n",
    "manifest_files = sorted(table_dir.glob('metadata/*-m*.avro'))\n",
    "print(f\"Found {len(manifest_files)} manifest file(s):\")\n",
    "for mf in manifest_files:\n",
    "    size = mf.stat().st_size\n",
    "    print(f\"  ‚Ä¢ {mf.name} ({size:,} bytes)\")\n",
    "\n",
    "# Pick first manifest to inspect\n",
    "if manifest_files:\n",
    "    manifest_to_inspect = manifest_files[0]\n",
    "    print(f\"\\nWill inspect: {manifest_to_inspect.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "read_manifest",
   "metadata": {},
   "source": [
    "### Reading Manifest Files\n",
    "\n",
    "Manifests are AVRO files. Let's read one and see what's inside.\n",
    "\n",
    "If you don't have `fastavro` installed, run: `pip install fastavro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read_manifest_raw",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fastavro\n",
    "\n",
    "    # Read the manifest\n",
    "    with open(manifest_to_inspect, 'rb') as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        records = list(reader)\n",
    "\n",
    "    print(f\"Manifest contains {len(records)} entry(ies)\\n\")\n",
    "\n",
    "    # Show first entry in detail\n",
    "    if records:\n",
    "        entry = records[0]\n",
    "        print(\"First entry structure:\")\n",
    "        print(f\"  Status: {entry.get('status', 'N/A')}  (0=EXISTING, 1=ADDED, 2=DELETED)\")\n",
    "\n",
    "        data_file = entry.get('data_file', {})\n",
    "        print(f\"\\n  Data file:\")\n",
    "        print(f\"    Path: {Path(data_file.get('file_path', 'N/A')).name}\")\n",
    "        print(f\"    Format: {data_file.get('file_format', 'N/A')}\")\n",
    "        print(f\"    Records: {data_file.get('record_count', 0):,}\")\n",
    "        print(f\"    Size: {data_file.get('file_size_in_bytes', 0):,} bytes\")\n",
    "\n",
    "        if data_file.get('value_counts'):\n",
    "            print(f\"\\n    Value counts (first 3 columns):\")\n",
    "            for i, (col, count) in enumerate(data_file['value_counts'].items()):\n",
    "                if i >= 3:\n",
    "                    break\n",
    "                print(f\"      {col}: {count:,}\")\n",
    "\n",
    "        if data_file.get('lower_bounds'):\n",
    "            print(f\"\\n    Lower bounds (first 2):\")\n",
    "            for i, (col, val) in enumerate(data_file['lower_bounds'].items()):\n",
    "                if i >= 2:\n",
    "                    break\n",
    "                print(f\"      {col}: {val!r}\")\n",
    "\n",
    "        if data_file.get('upper_bounds'):\n",
    "            print(f\"\\n    Upper bounds (first 2):\")\n",
    "            for i, (col, val) in enumerate(data_file['upper_bounds'].items()):\n",
    "                if i >= 2:\n",
    "                    break\n",
    "                print(f\"      {col}: {val!r}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  fastavro not installed. Install with: pip install fastavro\")\n",
    "    print(\"   We'll skip the detailed manifest inspection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifest_helper",
   "metadata": {},
   "source": [
    "### Using the Helper Function\n",
    "\n",
    "Let's use our helper to visualize the manifest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_manifest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if manifest_files:\n",
    "    inspect_manifest(manifest_to_inspect)\n",
    "else:\n",
    "    print(\"No manifest files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_files",
   "metadata": {},
   "source": [
    "## Data Files (Parquet)\n",
    "\n",
    "Finally, the actual data! Data files are standard **Parquet files**. Iceberg doesn't change Parquet - it just tracks them in manifests.\n",
    "\n",
    "Key properties:\n",
    "* **Immutable**: Once written, never modified\n",
    "* **Referenced by manifests**: Manifests point to data files\n",
    "* **Multiple files per table**: Each append creates new files\n",
    "* **Deletes don't rewrite**: Delete files mark rows as deleted\n",
    "\n",
    "Let's find and inspect a data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_data_files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find data files\n",
    "data_files = sorted(table_dir.glob('data/*.parquet'))\n",
    "print(f\"Found {len(data_files)} data file(s):\")\n",
    "\n",
    "total_size = 0\n",
    "for df in data_files:\n",
    "    size = df.stat().st_size\n",
    "    total_size += size\n",
    "    print(f\"  ‚Ä¢ {df.name} ({size / 1024 / 1024:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nTotal data size: {total_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_data_file",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first data file with PyArrow\n",
    "if data_files:\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    data_file = data_files[0]\n",
    "    pq_file = pq.ParquetFile(data_file)\n",
    "\n",
    "    print(f\"Inspecting: {data_file.name}\\n\")\n",
    "    print(f\"Total rows: {pq_file.metadata.num_rows:,}\")\n",
    "    print(f\"Total columns: {pq_file.metadata.num_columns}\")\n",
    "    print(f\"Row groups: {pq_file.metadata.num_row_groups}\")\n",
    "    print(f\"Format version: {pq_file.metadata.format_version}\")\n",
    "    print(f\"Created by: {pq_file.metadata.created_by}\")\n",
    "\n",
    "    print(f\"\\nSchema:\")\n",
    "    for i, field in enumerate(pq_file.schema):\n",
    "        print(f\"  {i+1}. {field.name}: {field.physical_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_picture",
   "metadata": {},
   "source": [
    "## The Complete Picture: Tracing a Query\n",
    "\n",
    "Now let's trace how a query uses all these metadata structures:\n",
    "\n",
    "```\n",
    "SELECT * FROM events WHERE type = 'c8y_Event' AND time > '2024-01-01'\n",
    "```\n",
    "\n",
    "### Step-by-Step Query Execution\n",
    "\n",
    "1. **Catalog Lookup** (SQLite)\n",
    "   - Query: `SELECT metadata_location FROM iceberg_tables WHERE table_name = 'events'`\n",
    "   - Result: Path to current metadata JSON file\n",
    "\n",
    "2. **Read Metadata JSON**\n",
    "   - Parse: `current-snapshot-id`\n",
    "   - Find snapshot with that ID\n",
    "   - Get: `manifest-list` path\n",
    "\n",
    "3. **Read Manifest List** (AVRO)\n",
    "   - Lists all manifest files for this snapshot\n",
    "   - Each manifest covers a partition or set of files\n",
    "\n",
    "4. **Read Manifests** (AVRO)\n",
    "   - For each manifest, check statistics:\n",
    "     - Does `lower_bounds['type']` ‚â§ 'c8y_Event' ‚â§ `upper_bounds['type']`?\n",
    "     - Does `lower_bounds['time']` ‚â§ '2024-01-01' ‚â§ `upper_bounds['time']`?\n",
    "   - If not: **skip this manifest entirely**\n",
    "   - If yes: read the list of data files\n",
    "\n",
    "5. **Predicate Pushdown on Files**\n",
    "   - For each data file in relevant manifests:\n",
    "     - Check file-level statistics\n",
    "     - Skip files where predicates can't match\n",
    "\n",
    "6. **Read Data Files** (Parquet)\n",
    "   - Read only files that passed predicate pushdown\n",
    "   - Within each file, read only necessary columns\n",
    "   - Apply row-level filters\n",
    "\n",
    "This is why Iceberg is fast - it reads minimal metadata to skip most of the data!\n",
    "\n",
    "### Visualizing the Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the complete metadata hierarchy\n",
    "print(\"Complete Iceberg Metadata Hierarchy:\\n\")\n",
    "print(\"1. üìö Catalog (SQLite)\")\n",
    "print(f\"   {catalog_db.name}\")\n",
    "print(f\"   ‚îî‚îÄ Table: demo.events ‚Üí {latest_metadata.name}\")\n",
    "print()\n",
    "print(\"2. üìÑ Metadata JSON\")\n",
    "print(f\"   {latest_metadata.name}\")\n",
    "print(f\"   ‚îú‚îÄ Schema: {len(metadata['schemas'])} version(s)\")\n",
    "print(f\"   ‚îú‚îÄ Partition specs: {len(metadata['partition-specs'])}\")\n",
    "print(f\"   ‚îî‚îÄ Snapshots: {len(metadata['snapshots'])}\")\n",
    "print()\n",
    "print(\"3. üì¶ Manifest Files (AVRO)\")\n",
    "for i, mf in enumerate(manifest_files, 1):\n",
    "    print(f\"   {mf.name} ({mf.stat().st_size:,} bytes)\")\n",
    "print()\n",
    "print(\"4. üíæ Data Files (Parquet)\")\n",
    "for i, df in enumerate(data_files, 1):\n",
    "    print(f\"   {df.name} ({df.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "print()\n",
    "print(f\"Total metadata overhead: {sum(mf.stat().st_size for mf in metadata_files + manifest_files) / 1024:.1f} KB\")\n",
    "print(f\"Total data size: {sum(df.stat().st_size for df in data_files) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **Why store metadata in multiple JSON files instead of one?**\n",
    "   - Hint: Think about atomicity and append-only operations.\n",
    "\n",
    "2. **What would happen if you directly edited a data file?**\n",
    "   - Would the manifest notice? Would queries see your changes?\n",
    "\n",
    "3. **How does Iceberg achieve atomic commits with SQLite?**\n",
    "   - What SQL statement is used? What makes it atomic?\n",
    "\n",
    "4. **Why separate manifest lists from manifest files?**\n",
    "   - Why not put all data files in one manifest?\n",
    "\n",
    "5. **How does predicate pushdown work?**\n",
    "   - At what levels can files/partitions be skipped?\n",
    "\n",
    "6. **What's the metadata overhead for this table?**\n",
    "   - Calculate: metadata size / data size\n",
    "   - Is this reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Hands-on Challenge\n",
    "\n",
    "### Challenge 1: Parse Metadata Manually\n",
    "\n",
    "1. Open the latest metadata JSON in a text editor\n",
    "2. Find the `current-snapshot-id`\n",
    "3. Locate that snapshot in the `snapshots` array\n",
    "4. Extract the `manifest-list` path\n",
    "5. Verify this file exists in the metadata directory\n",
    "\n",
    "### Challenge 2: Analyze Manifest Statistics\n",
    "\n",
    "1. Read a manifest file using fastavro\n",
    "2. For each data file entry, extract:\n",
    "   - Record count\n",
    "   - File size\n",
    "   - Lower/upper bounds for 'type' column\n",
    "3. Calculate: total records, average file size\n",
    "\n",
    "### Challenge 3: Simulate Predicate Pushdown\n",
    "\n",
    "1. Write a query filter: `type = 'c8y_Measurement'`\n",
    "2. Read manifests and check statistics\n",
    "3. Count how many files would be skipped\n",
    "4. Calculate: % of data skipped\n",
    "\n",
    "Use the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this deep dive, we explored:\n",
    "\n",
    "* **Catalog Database**: The atomic pointer to current metadata\n",
    "  - Enables optimistic concurrency control\n",
    "  - Single UPDATE statement makes commits atomic\n",
    "\n",
    "* **Metadata JSON**: Complete table state\n",
    "  - All schema versions (for time travel)\n",
    "  - All snapshots with manifest lists\n",
    "  - Snapshot log for temporal queries\n",
    "  - Metadata log for file management\n",
    "\n",
    "* **Manifest Files**: Index of data files\n",
    "  - AVRO format for efficiency\n",
    "  - Per-file statistics for pruning\n",
    "  - Partition information\n",
    "  - Enables predicate pushdown\n",
    "\n",
    "* **Data Files**: Immutable Parquet\n",
    "  - Never modified after creation\n",
    "  - Referenced by manifests\n",
    "  - Standard Parquet format\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Metadata is append-only**: New files created, old ones retained\n",
    "2. **Catalog is the single source of truth**: Points to current metadata\n",
    "3. **Statistics enable pruning**: Skip files/partitions without reading\n",
    "4. **Everything is versioned**: Time travel works by reading old snapshots\n",
    "5. **Minimal metadata overhead**: ~KB of metadata per GB of data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand the internal structures, we'll explore:\n",
    "* **Time travel**: Using snapshots for historical queries\n",
    "* **Schema evolution**: How column changes work in metadata\n",
    "* **Concurrency**: Simulating optimistic locking conflicts\n",
    "* **Partitioning**: Managing millions of files efficiently"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}