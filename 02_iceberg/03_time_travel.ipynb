{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Time Travel and Snapshots\n",
    "\n",
    "One of Iceberg's most powerful features is **time travel** - the ability to query your data as it existed at any point in the past.\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "\n",
    "* **Understanding snapshots**: What they are and how they work\n",
    "* **Time travel queries**: Query data from specific points in time\n",
    "* **Snapshot operations**: Inspect, compare, and manage snapshots\n",
    "* **Rollback**: Undo changes by rolling back to previous snapshots\n",
    "* **Branching and tagging**: Create named references to snapshots\n",
    "* **Snapshot expiration**: Clean up old snapshots to save storage\n",
    "\n",
    "By the end, you'll be able to use Iceberg's time travel features for debugging, auditing, and data recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow as pa\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect_iceberg_table, compare_snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_table",
   "metadata": {},
   "source": [
    "## Setup: Create a Table with Rich History\n",
    "\n",
    "Let's create a table with multiple snapshots to demonstrate time travel features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup warehouse\n",
    "warehouse_path = Path('../data/warehouse_time_travel').absolute()\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    'time_travel_demo',\n",
    "    **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'}\n",
    ")\n",
    "catalog.create_namespace('demo')\n",
    "print(\"✅ Catalog initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events and create table with multiple operations\n",
    "jsonl_file = Path('../data/input/events.jsonl')\n",
    "\n",
    "# Snapshot 1: Initial load (Week 1)\n",
    "print(\"Creating Snapshot 1: Initial load\")\n",
    "with jsonl_file.open('r') as f:\n",
    "    week1 = [json.loads(line) for i, line in enumerate(f) if i < 20000]\n",
    "arrow_table = pa.Table.from_pylist(week1)\n",
    "events_table = catalog.create_table('demo.events', schema=pa.schema(arrow_table.schema))\n",
    "events_table.append(arrow_table)\n",
    "time.sleep(0.1)  # Small delay to ensure different timestamps\n",
    "\n",
    "# Snapshot 2: Week 2 data\n",
    "print(\"Creating Snapshot 2: Week 2 data\")\n",
    "with jsonl_file.open('r') as f:\n",
    "    week2 = [json.loads(line) for i, line in enumerate(f) if 20000 <= i < 40000]\n",
    "arrow_table = pa.Table.from_pylist(week2)\n",
    "events_table.append(arrow_table)\n",
    "time.sleep(0.1)\n",
    "\n",
    "# Snapshot 3: Delete bad data (discovered data quality issue)\n",
    "print(\"Creating Snapshot 3: Delete LocationUpdate events (data quality fix)\")\n",
    "events_table.delete(\"type = 'c8y_LocationUpdate'\")\n",
    "time.sleep(0.1)\n",
    "\n",
    "# Snapshot 4: Week 3 data\n",
    "print(\"Creating Snapshot 4: Week 3 data\")\n",
    "with jsonl_file.open('r') as f:\n",
    "    week3 = [json.loads(line) for i, line in enumerate(f) if 40000 <= i < 60000]\n",
    "arrow_table = pa.Table.from_pylist(week3)\n",
    "events_table.append(arrow_table)\n",
    "\n",
    "print(f\"\\n✅ Created table with {len(events_table.history())} snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding_snapshots",
   "metadata": {},
   "source": [
    "## Understanding Snapshots\n",
    "\n",
    "A **snapshot** is an immutable view of a table at a specific point in time. Think of it like a Git commit:\n",
    "\n",
    "* Each write operation (append, delete, overwrite) creates a new snapshot\n",
    "* Snapshots are never modified - they're append-only\n",
    "* Old snapshots remain accessible (until explicitly expired)\n",
    "* Each snapshot has a unique ID and timestamp\n",
    "\n",
    "### Snapshot Properties\n",
    "\n",
    "Each snapshot contains:\n",
    "* **snapshot_id**: Unique identifier (64-bit integer)\n",
    "* **timestamp_ms**: When this snapshot was created\n",
    "* **parent_snapshot_id**: Previous snapshot (forms a chain)\n",
    "* **sequence_number**: Monotonically increasing number\n",
    "* **manifest_list**: Path to manifest list (index of data files)\n",
    "* **summary**: Operation type and statistics\n",
    "\n",
    "Let's inspect our snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list_snapshots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get snapshot history\n",
    "history = events_table.history()\n",
    "\n",
    "print(f\"Total snapshots: {len(history)}\\n\")\n",
    "print(f\"{'Snap':<5} {'Timestamp':<20} {'Operation':<10} {'Records':<12} {'Files'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, snapshot in enumerate(history, 1):\n",
    "    timestamp = datetime.fromtimestamp(snapshot.timestamp_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    operation = snapshot.summary.operation.value if snapshot.summary else 'N/A'\n",
    "    \n",
    "    # Get statistics from summary\n",
    "    props = snapshot.summary.additional_properties if snapshot.summary else {}\n",
    "    total_records = props.get('total-records', props.get('added-records', 'N/A'))\n",
    "    total_files = props.get('total-data-files', props.get('added-data-files', 'N/A'))\n",
    "    \n",
    "    print(f\"{i:<5} {timestamp:<20} {operation:<10} {str(total_records):<12} {total_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a specific snapshot in detail\n",
    "snap1 = history[0]\n",
    "print(f\"Snapshot 1 Details:\\n\")\n",
    "print(f\"ID: {snap1.snapshot_id}\")\n",
    "print(f\"Timestamp: {datetime.fromtimestamp(snap1.timestamp_ms / 1000)}\")\n",
    "print(f\"Sequence number: {snap1.sequence_number if hasattr(snap1, 'sequence_number') else 'N/A'}\")\n",
    "print(f\"Parent snapshot: {snap1.parent_snapshot_id if hasattr(snap1, 'parent_snapshot_id') else 'None (first snapshot)'}\")\n",
    "\n",
    "if snap1.summary:\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Operation: {snap1.summary.operation.value}\")\n",
    "    if snap1.summary.additional_properties:\n",
    "        for key, value in sorted(snap1.summary.additional_properties.items()):\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time_travel_queries",
   "metadata": {},
   "source": [
    "## Time Travel Queries\n",
    "\n",
    "Iceberg supports two ways to query historical data:\n",
    "\n",
    "1. **By snapshot ID**: Query a specific snapshot\n",
    "2. **By timestamp**: Query data as of a specific time\n",
    "\n",
    "### Query by Snapshot ID\n",
    "\n",
    "This is the most precise method - you specify exactly which snapshot to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_by_snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query current state\n",
    "df_current = daft.read_iceberg(events_table)\n",
    "print(\"Current state:\")\n",
    "daft.sql(\"SELECT COUNT(*) as total_events FROM df_current\").show()\n",
    "\n",
    "# Query Snapshot 1 (first load)\n",
    "snapshot1_id = history[0].snapshot_id\n",
    "df_snap1 = daft.read_iceberg(events_table, snapshot_id=snapshot1_id)\n",
    "print(f\"\\nSnapshot 1 (ID: {snapshot1_id}):\")\n",
    "daft.sql(\"SELECT COUNT(*) as total_events FROM df_snap1\").show()\n",
    "\n",
    "# Query Snapshot 2 (after week 2)\n",
    "snapshot2_id = history[1].snapshot_id\n",
    "df_snap2 = daft.read_iceberg(events_table, snapshot_id=snapshot2_id)\n",
    "print(f\"\\nSnapshot 2 (ID: {snapshot2_id}):\")\n",
    "daft.sql(\"SELECT COUNT(*) as total_events FROM df_snap2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timestamp_query",
   "metadata": {},
   "source": [
    "### Query by Timestamp\n",
    "\n",
    "This finds the snapshot that was current at the specified time. Useful for questions like:\n",
    "* \"Show me the data as of yesterday at 3pm\"\n",
    "* \"What did the monthly report see on March 1st?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_by_timestamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp between snapshot 1 and 2\n",
    "snap1_time = history[0].timestamp_ms\n",
    "snap2_time = history[1].timestamp_ms\n",
    "mid_time = snap1_time + (snap2_time - snap1_time) // 2\n",
    "\n",
    "print(f\"Querying as of: {datetime.fromtimestamp(mid_time / 1000)}\")\n",
    "print(f\"(Between snapshot 1 and 2)\\n\")\n",
    "\n",
    "# Query as of that timestamp\n",
    "df_as_of = daft.read_iceberg(events_table, as_of_timestamp=mid_time)\n",
    "daft.sql(\"SELECT COUNT(*) as total_events FROM df_as_of\").show()\n",
    "\n",
    "print(\"\\nThis returns Snapshot 1 data, because that was current at the specified time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use_cases",
   "metadata": {},
   "source": [
    "### Time Travel Use Cases\n",
    "\n",
    "Time travel is useful for:\n",
    "\n",
    "1. **Debugging**: \"What data did the broken job see?\"\n",
    "2. **Auditing**: \"Show me all changes in the last 24 hours\"\n",
    "3. **Reproducing reports**: \"Re-run the quarterly report with the exact data it used\"\n",
    "4. **Data recovery**: \"Restore deleted records from yesterday\"\n",
    "5. **Testing**: \"Compare results before and after a schema change\"\n",
    "\n",
    "Let's demonstrate a debugging scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debugging_scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: We deleted LocationUpdate events in Snapshot 3\n",
    "# Let's verify they existed before and are gone now\n",
    "\n",
    "# Before deletion (Snapshot 2)\n",
    "snap2_id = history[1].snapshot_id\n",
    "df_before = daft.read_iceberg(events_table, snapshot_id=snap2_id)\n",
    "print(\"Before deletion (Snapshot 2):\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN type = 'c8y_LocationUpdate' THEN 1 ELSE 0 END) as location_updates,\n",
    "        COUNT(*) as total\n",
    "    FROM df_before\n",
    "\"\"\").show()\n",
    "\n",
    "# After deletion (Snapshot 3)\n",
    "snap3_id = history[2].snapshot_id\n",
    "df_after = daft.read_iceberg(events_table, snapshot_id=snap3_id)\n",
    "print(\"\\nAfter deletion (Snapshot 3):\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN type = 'c8y_LocationUpdate' THEN 1 ELSE 0 END) as location_updates,\n",
    "        COUNT(*) as total\n",
    "    FROM df_after\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n✅ Time travel confirmed the deletion happened in Snapshot 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollback",
   "metadata": {},
   "source": [
    "## Rollback: Undoing Changes\n",
    "\n",
    "Rollback reverts the table to a previous snapshot. This is like `git reset` - it makes a previous snapshot current.\n",
    "\n",
    "**Important**: Rollback doesn't delete snapshots or data. It just changes which snapshot is \"current\".\n",
    "\n",
    "### When to Use Rollback\n",
    "\n",
    "* Bad data was loaded\n",
    "* A bug in the ingestion pipeline\n",
    "* Accidental deletion\n",
    "* Testing: rollback after test, then rollback to restore\n",
    "\n",
    "Let's rollback to before the deletion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollback_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current state (after deletion)\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"Current state (Snapshot 4 - after deletion):\")\n",
    "result = daft.sql(\"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN type = 'c8y_LocationUpdate' THEN 1 ELSE 0 END) as location_updates,\n",
    "        COUNT(*) as total\n",
    "    FROM df\n",
    "\"\"\").collect()\n",
    "print(result)\n",
    "\n",
    "# Rollback to Snapshot 2 (before deletion)\n",
    "snap2_id = history[1].snapshot_id\n",
    "print(f\"\\nRolling back to Snapshot 2 (ID: {snap2_id})...\")\n",
    "events_table = events_table.manage_snapshots().rollback_to_snapshot(snap2_id).commit()\n",
    "\n",
    "# Verify\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"\\nAfter rollback:\")\n",
    "result = daft.sql(\"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN type = 'c8y_LocationUpdate' THEN 1 ELSE 0 END) as location_updates,\n",
    "        COUNT(*) as total\n",
    "    FROM df\n",
    "\"\"\").collect()\n",
    "print(result)\n",
    "\n",
    "print(\"\\n✅ Rollback successful! LocationUpdate events are back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollback_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check history after rollback\n",
    "history = events_table.history()\n",
    "current_snap_id = events_table.current_snapshot().snapshot_id\n",
    "\n",
    "print(\"Snapshot history after rollback:\\n\")\n",
    "for i, snap in enumerate(history, 1):\n",
    "    is_current = snap.snapshot_id == current_snap_id\n",
    "    marker = \" ← CURRENT\" if is_current else \"\"\n",
    "    print(f\"Snapshot {i}: {snap.snapshot_id}{marker}\")\n",
    "\n",
    "print(\"\\nNote: All snapshots still exist! We just changed which one is 'current'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "branching_tagging",
   "metadata": {},
   "source": [
    "## Branching and Tagging\n",
    "\n",
    "Iceberg supports **named references** to snapshots:\n",
    "\n",
    "* **Tags**: Immutable pointers to snapshots (like Git tags)\n",
    "  - Use for: releases, quarterly reports, milestones\n",
    "  - Example: `quarterly-report-2024-Q4`\n",
    "\n",
    "* **Branches**: Mutable pointers that can advance (like Git branches)\n",
    "  - Use for: experimental changes, staging environments\n",
    "  - Example: `experiment-new-schema`\n",
    "\n",
    "### Creating Tags\n",
    "\n",
    "Tags are useful for marking important snapshots that you want to keep forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_tags",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tags for important snapshots\n",
    "snap1_id = history[0].snapshot_id\n",
    "snap2_id = history[1].snapshot_id\n",
    "\n",
    "# Tag the initial load\n",
    "events_table = events_table.manage_snapshots().create_tag('initial-load', snap1_id).commit()\n",
    "print(f\"✅ Created tag 'initial-load' → Snapshot {snap1_id}\")\n",
    "\n",
    "# Tag week 2\n",
    "events_table = events_table.manage_snapshots().create_tag('week-2-complete', snap2_id).commit()\n",
    "print(f\"✅ Created tag 'week-2-complete' → Snapshot {snap2_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_by_tag",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by tag name\n",
    "df_initial = daft.read_iceberg(events_table, snapshot_id='initial-load')\n",
    "print(\"Querying 'initial-load' tag:\")\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df_initial\").show()\n",
    "\n",
    "# Query another tag\n",
    "df_week2 = daft.read_iceberg(events_table, snapshot_id='week-2-complete')\n",
    "print(\"\\nQuerying 'week-2-complete' tag:\")\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df_week2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "branches",
   "metadata": {},
   "source": [
    "### Creating Branches\n",
    "\n",
    "Branches allow you to make experimental changes without affecting the main timeline.\n",
    "\n",
    "Note: PyIceberg's current version has limited branch support. In production with Spark/Flink, you can:\n",
    "* Create branches: `CREATE BRANCH experiment FROM SNAPSHOT 123`\n",
    "* Write to branches: `INSERT INTO table.branch_experiment ...`\n",
    "* Merge branches back to main\n",
    "\n",
    "For now, we'll demonstrate the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "branch_concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the branch concept\n",
    "print(\"Branch concept (requires Spark/Flink for full support):\\n\")\n",
    "print(\"1. Create branch 'experiment' from current snapshot\")\n",
    "print(\"   CREATE BRANCH experiment\")\n",
    "print()\n",
    "print(\"2. Write to the branch\")\n",
    "print(\"   INSERT INTO table.branch_experiment SELECT ...\")\n",
    "print()\n",
    "print(\"3. Query the branch\")\n",
    "print(\"   SELECT * FROM table.branch_experiment\")\n",
    "print()\n",
    "print(\"4. Main table is unaffected\")\n",
    "print(\"   SELECT * FROM table  -- sees main branch\")\n",
    "print()\n",
    "print(\"5. Merge or drop the branch\")\n",
    "print(\"   CALL merge_branch('table', 'experiment')\")\n",
    "print(\"   DROP BRANCH experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snapshot_expiration",
   "metadata": {},
   "source": [
    "## Snapshot Expiration\n",
    "\n",
    "Snapshots are retained forever by default. This enables unlimited time travel, but:\n",
    "\n",
    "* **Storage costs**: Old data files accumulate\n",
    "* **Metadata overhead**: Manifest lists grow\n",
    "* **Cleanup complexity**: Hard to know what's still needed\n",
    "\n",
    "**Snapshot expiration** removes old snapshots and their unreferenced data files.\n",
    "\n",
    "### Expiration Strategies\n",
    "\n",
    "1. **Time-based**: Expire snapshots older than N days\n",
    "2. **Count-based**: Keep only the last N snapshots\n",
    "3. **Tagged snapshots**: Never expire tagged snapshots\n",
    "\n",
    "### Safe Expiration\n",
    "\n",
    "Iceberg expiration is safe:\n",
    "* Only removes unreferenced files\n",
    "* Respects the `min_snapshots_to_keep` setting\n",
    "* Honors retention policies\n",
    "\n",
    "Let's demonstrate expiration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "before_expiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current snapshots\n",
    "print(\"Snapshots before expiration:\")\n",
    "history = events_table.history()\n",
    "for i, snap in enumerate(history, 1):\n",
    "    timestamp = datetime.fromtimestamp(snap.timestamp_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"  Snapshot {i}: {snap.snapshot_id} @ {timestamp}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(history)} snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expire_snapshots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expire old snapshots (keep last 2, plus tagged)\n",
    "# Note: This is a destructive operation in production!\n",
    "# For this demo, we'll show the concept without actually expiring\n",
    "\n",
    "print(\"Expiration command (not executed in demo):\")\n",
    "print()\n",
    "print(\"  events_table.expire_snapshots(\")\n",
    "print(\"      older_than=datetime.now() - timedelta(days=7),  # Expire > 7 days old\")\n",
    "print(\"      retain_last=2  # But keep at least 2 snapshots\")\n",
    "print(\"  )\")\n",
    "print()\n",
    "print(\"This would:\")\n",
    "print(\"  • Remove snapshots older than 7 days\")\n",
    "print(\"  • But always keep the last 2 snapshots\")\n",
    "print(\"  • Never remove tagged snapshots ('initial-load', 'week-2-complete')\")\n",
    "print(\"  • Delete data files only referenced by expired snapshots\")\n",
    "print()\n",
    "print(\"⚠️  In production, test expiration carefully!\")\n",
    "print(\"    Use 'dry_run=True' first to see what would be deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## Comparing Snapshots\n",
    "\n",
    "Let's use our helper function to compare two snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Snapshot 1 and Snapshot 2\n",
    "snap1_id = history[0].snapshot_id\n",
    "snap2_id = history[1].snapshot_id\n",
    "\n",
    "compare_snapshots(events_table, snap1_id, snap2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **How is Iceberg time travel different from backup files?**\n",
    "   - Think about query capabilities, storage efficiency, and metadata.\n",
    "\n",
    "2. **What happens to data files when you rollback?**\n",
    "   - Are files deleted? What changes in the metadata?\n",
    "\n",
    "3. **Why might you want both tags and branches?**\n",
    "   - When would you use each?\n",
    "\n",
    "4. **Is rollback destructive?**\n",
    "   - Can you undo a rollback?\n",
    "\n",
    "5. **What's the difference between querying by snapshot ID vs. timestamp?**\n",
    "   - Which is more precise? Which is more user-friendly?\n",
    "\n",
    "6. **How does snapshot expiration decide what to delete?**\n",
    "   - What are the rules for safe deletion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Hands-on Challenge\n",
    "\n",
    "### Challenge 1: Simulate a Bad Load and Rollback\n",
    "\n",
    "1. Append some test data to the events table\n",
    "2. Verify the data is there\n",
    "3. \"Discover\" it's bad data (simulate)\n",
    "4. Rollback to before the bad load\n",
    "5. Verify the data is gone\n",
    "\n",
    "### Challenge 2: Create Monthly Tags\n",
    "\n",
    "1. Based on the timestamps of your snapshots\n",
    "2. Create tags like `month-2024-01`, `month-2024-02`\n",
    "3. Query by tag to verify\n",
    "\n",
    "### Challenge 3: Audit Trail\n",
    "\n",
    "1. Create a report showing all changes to the table\n",
    "2. For each snapshot: timestamp, operation, records added/deleted\n",
    "3. Calculate: net change in records from start to now\n",
    "\n",
    "Use the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored Iceberg's time travel capabilities:\n",
    "\n",
    "* **Snapshots**: Immutable views of the table at specific points in time\n",
    "  - Created by every write operation\n",
    "  - Form a chain with parent pointers\n",
    "  - Contain operation summaries and statistics\n",
    "\n",
    "* **Time Travel**: Query historical data\n",
    "  - By snapshot ID (precise)\n",
    "  - By timestamp (user-friendly)\n",
    "  - Use cases: debugging, auditing, reproducing reports\n",
    "\n",
    "* **Rollback**: Undo changes\n",
    "  - Non-destructive (snapshots remain)\n",
    "  - Changes which snapshot is \"current\"\n",
    "  - Can be undone by rolling back again\n",
    "\n",
    "* **Tags and Branches**: Named references\n",
    "  - Tags: Immutable, for milestones\n",
    "  - Branches: Mutable, for experiments\n",
    "  - Enable human-readable snapshot references\n",
    "\n",
    "* **Snapshot Expiration**: Clean up old data\n",
    "  - Time-based or count-based policies\n",
    "  - Safe: only removes unreferenced files\n",
    "  - Respects tags and retention settings\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Time travel is cheap**: Just metadata changes, no data copying\n",
    "2. **Rollback is safe**: Can always roll forward again\n",
    "3. **Tags preserve history**: Tagged snapshots never expire\n",
    "4. **Expiration is necessary**: Balance retention vs. storage costs\n",
    "5. **Audit trail is automatic**: Every change is recorded\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebooks:\n",
    "* **Schema evolution**: Change schema without rewriting data\n",
    "* **Concurrency**: Optimistic locking and conflict resolution\n",
    "* **Partitioning**: Scale to millions of files\n",
    "* **Object stores**: Iceberg on S3 with efficient metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
