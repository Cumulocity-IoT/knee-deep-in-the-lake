{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Schema Evolution Without Rewrites\n",
    "\n",
    "In traditional databases and data lakes, changing a schema means rewriting all your data. With billions of rows, this is expensive and slow.\n",
    "\n",
    "Iceberg solves this by storing **schema in metadata**, not in data files. You can:\n",
    "\n",
    "* Add columns without rewriting data\n",
    "* Remove columns without data loss (for time travel)\n",
    "* Rename columns safely\n",
    "* Promote types (int32 → int64)\n",
    "* Reorder columns\n",
    "\n",
    "All of this happens **instantly** - just metadata updates.\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "\n",
    "* Adding and removing columns\n",
    "* Renaming columns\n",
    "* Type promotion\n",
    "* Schema versioning\n",
    "* Handling schema drift\n",
    "\n",
    "## Why Schema Evolution Matters\n",
    "\n",
    "Imagine you have a table with 10 billion rows (10 TB). Without schema evolution:\n",
    "* Adding a column: Rewrite all 10 TB (hours/days)\n",
    "* Renaming a column: Rewrite all 10 TB\n",
    "* Cost: Compute + storage + time\n",
    "\n",
    "With Iceberg:\n",
    "* Any schema change: Update metadata JSON (milliseconds)\n",
    "* Cost: Nearly zero\n",
    "* Data files: Untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from datetime import datetime\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import inspect_iceberg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "warehouse_path = Path('../data/warehouse_schema_evolution').absolute()\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "catalog = SqlCatalog('schema_demo', **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'})\n",
    "catalog.create_namespace('demo')\n",
    "\n",
    "# Create table with initial schema\n",
    "df_events = daft.read_json('../data/input/events.jsonl')\n",
    "df_sample = df_events.limit(10000)\n",
    "\n",
    "arrow_table = df_sample.to_arrow()\n",
    "events_table = catalog.create_table('demo.events', schema=pa.schema(arrow_table.schema))\n",
    "events_table.append(arrow_table)\n",
    "\n",
    "print(f\"✅ Created table with {len(arrow_table):,} records\")\n",
    "print(f\"\\nInitial schema:\")\n",
    "for field in events_table.schema().fields:\n",
    "    print(f\"  • {field.name}: {field.field_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adding_columns",
   "metadata": {},
   "source": [
    "## Adding Columns\n",
    "\n",
    "When you add a column:\n",
    "* New metadata version is created\n",
    "* New writes include the column\n",
    "* Old data files: Column reads as NULL\n",
    "* No data rewrite required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "with events_table.update_schema() as update:\n",
    "    update.add_column('processed_at', pa.timestamp('ms'), doc='When this event was processed')\n",
    "\n",
    "print(\"✅ Added column 'processed_at'\")\n",
    "print(f\"\\nNew schema:\")\n",
    "for field in events_table.schema().fields:\n",
    "    print(f\"  • {field.name}: {field.field_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query_with_new_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Old data shows NULL for new column\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"Old data (processed_at is NULL):\")\n",
    "df.select('id', 'type', 'processed_at').show(5)\n",
    "\n",
    "# Add new data with the column\n",
    "df_new = df_events.offset(10000).limit(1000)\n",
    "\n",
    "# Add processed_at timestamp to the data\n",
    "from datetime import datetime\n",
    "new_data = df_new.to_arrow().to_pylist()\n",
    "for record in new_data:\n",
    "    record['processed_at'] = datetime.now().isoformat()\n",
    "\n",
    "new_arrow = pa.Table.from_pylist(new_data)\n",
    "events_table.append(new_arrow)\n",
    "\n",
    "print(f\"\\n✅ Appended {len(new_arrow):,} records with processed_at\")\n",
    "\n",
    "# Query again\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"\\nNew data (processed_at has values):\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT id, type, processed_at\n",
    "    FROM df\n",
    "    WHERE processed_at IS NOT NULL\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removing_columns",
   "metadata": {},
   "source": [
    "## Removing Columns\n",
    "\n",
    "When you remove a column:\n",
    "* Column marked as \"deleted\" in metadata\n",
    "* New queries don't see it\n",
    "* Data files still contain it (for time travel)\n",
    "* Column ID preserved (for historical queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remove_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a column\n",
    "with events_table.update_schema() as update:\n",
    "    update.delete_column('processed_at')\n",
    "\n",
    "print(\"✅ Removed column 'processed_at'\")\n",
    "print(\"\\nCurrent schema:\")\n",
    "for field in events_table.schema().fields:\n",
    "    print(f\"  • {field.name}: {field.field_type}\")\n",
    "\n",
    "# Try to query - column is gone\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"\\nAvailable columns:\")\n",
    "print([f.name for f in df.schema()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time_travel_schemas",
   "metadata": {},
   "source": [
    "### Time Travel with Old Schemas\n",
    "\n",
    "Even though we removed the column, we can still see it in old snapshots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_travel_schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query an old snapshot where the column existed\n",
    "history = events_table.history()\n",
    "old_snapshot = history[1].snapshot_id  # After we added the column\n",
    "\n",
    "df_old = daft.read_iceberg(events_table, snapshot_id=old_snapshot)\n",
    "print(f\"Snapshot {old_snapshot} schema:\")\n",
    "print([f.name for f in df_old.schema()])\n",
    "print(\"\\nThe 'processed_at' column is visible in the old snapshot!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renaming",
   "metadata": {},
   "source": [
    "## Renaming Columns\n",
    "\n",
    "Iceberg uses **column IDs** internally, not names. This makes renaming safe:\n",
    "* Old code using old names: Works (uses column ID)\n",
    "* New code using new names: Works (same column ID)\n",
    "* No data rewrite needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rename_column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column\n",
    "with events_table.update_schema() as update:\n",
    "    update.rename_column('source', 'device_id')\n",
    "\n",
    "print(\"✅ Renamed 'source' → 'device_id'\")\n",
    "print(\"\\nNew schema:\")\n",
    "for field in events_table.schema().fields:\n",
    "    print(f\"  • {field.name} (ID: {field.field_id}): {field.field_type}\")\n",
    "\n",
    "# Query with new name\n",
    "df = daft.read_iceberg(events_table)\n",
    "daft.sql(\"SELECT device_id, type FROM df LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "type_promotion",
   "metadata": {},
   "source": [
    "## Type Promotion\n",
    "\n",
    "Iceberg supports **safe type promotions**:\n",
    "* `int32` → `int64` ✅\n",
    "* `float` → `double` ✅\n",
    "* `decimal(10,2)` → `decimal(18,2)` ✅\n",
    "\n",
    "Unsafe promotions are rejected:\n",
    "* `int64` → `int32` ❌ (data loss)\n",
    "* `string` → `int64` ❌ (incompatible)\n",
    "\n",
    "Let's create a table to demonstrate type promotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "type_promotion_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with int32\n",
    "sensor_schema = pa.schema([\n",
    "    pa.field('id', pa.int32()),\n",
    "    pa.field('value', pa.float32())\n",
    "])\n",
    "\n",
    "sensors_table = catalog.create_table('demo.sensors', schema=sensor_schema)\n",
    "sensor_data = pa.table({'id': [1, 2, 3], 'value': [1.5, 2.5, 3.5]})\n",
    "sensors_table.append(sensor_data)\n",
    "\n",
    "print(\"Initial schema:\")\n",
    "for field in sensors_table.schema().fields:\n",
    "    print(f\"  • {field.name}: {field.field_type}\")\n",
    "\n",
    "# Promote types\n",
    "with sensors_table.update_schema() as update:\n",
    "    update.update_column('id', pa.int64())\n",
    "    update.update_column('value', pa.float64())\n",
    "\n",
    "print(\"\\n✅ Promoted types: int32→int64, float→double\")\n",
    "print(\"\\nNew schema:\")\n",
    "for field in sensors_table.schema().fields:\n",
    "    print(f\"  • {field.name}: {field.field_type}\")\n",
    "\n",
    "# Old data is automatically promoted when read\n",
    "df = daft.read_iceberg(sensors_table)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema_versioning",
   "metadata": {},
   "source": [
    "## Schema Versioning\n",
    "\n",
    "Every schema change creates a new **schema version**. Metadata stores all versions:\n",
    "* Schema ID 0: Initial schema\n",
    "* Schema ID 1: After adding column\n",
    "* Schema ID 2: After renaming\n",
    "* ...\n",
    "\n",
    "Snapshots reference specific schema versions. Let's inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_schemas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata to see all schema versions\n",
    "metadata = events_table.metadata\n",
    "\n",
    "print(f\"Total schema versions: {len(metadata.schemas)}\\n\")\n",
    "\n",
    "for schema in metadata.schemas:\n",
    "    print(f\"Schema ID {schema.schema_id}:\")\n",
    "    print(f\"  Fields: {[f.name for f in schema.fields[:5]]}\")  # Show first 5\n",
    "    if len(schema.fields) > 5:\n",
    "        print(f\"  ... and {len(schema.fields) - 5} more\")\n",
    "    print()\n",
    "\n",
    "# Show which schema each snapshot uses\n",
    "print(\"Snapshots and their schemas:\")\n",
    "for i, snap in enumerate(events_table.history(), 1):\n",
    "    # Note: PyIceberg may not expose schema_id directly on snapshot\n",
    "    print(f\"  Snapshot {i}: {snap.snapshot_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema_drift",
   "metadata": {},
   "source": [
    "## Handling Schema Drift\n",
    "\n",
    "Real-world data isn't always clean. Let's simulate schema drift using the radiator.jsonl dataset, which has type conflicts.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In radiator.jsonl, `meas_ModelNumber.value` is:\n",
    "* Usually an integer: `{\"value\": 4}`\n",
    "* Sometimes a string: `{\"value\": \" 3\"}` or `{\"value\": \"1\"}`\n",
    "\n",
    "How do we handle this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema_drift_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read radiator data with Daft (it handles type coercion)\n",
    "radiator_df = daft.read_json('../data/input/radiator.jsonl')\n",
    "\n",
    "print(\"Daft inferred schema:\")\n",
    "print(radiator_df.schema())\n",
    "\n",
    "# Sample the data\n",
    "print(\"\\nSample:\")\n",
    "radiator_df.select('source', 'time', 'type').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_with_drift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Iceberg - Daft handles type conflicts\n",
    "# We'll take a small sample\n",
    "sample_df = radiator_df.limit(5000)\n",
    "sample_arrow = sample_df.to_arrow()\n",
    "\n",
    "radiator_table = catalog.create_table('demo.radiator', schema=pa.schema(sample_arrow.schema))\n",
    "radiator_table.append(sample_arrow)\n",
    "\n",
    "print(\"✅ Written radiator data to Iceberg\")\n",
    "print(\"\\nIceberg schema:\")\n",
    "for field in radiator_table.schema().fields[:5]:\n",
    "    print(f\"  • {field.name}: {field.field_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ca620",
   "metadata": {},
   "source": [
    "Bit shallow here ... what happens when I query old data in the face of larger schema changes in the meanwhile?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "1. **Why does Iceberg use column IDs instead of names?**\n",
    "   - What problems does this solve?\n",
    "\n",
    "2. **What would break if you renamed a column in Parquet directly?**\n",
    "   - Think about existing queries and applications.\n",
    "\n",
    "3. **When would you need to rewrite data despite schema evolution?**\n",
    "   - Are there any schema changes that require rewrites?\n",
    "\n",
    "4. **How does Iceberg handle reading old data with new schemas?**\n",
    "   - What happens to missing columns?\n",
    "\n",
    "5. **Can you demote a type (int64 → int32)?**\n",
    "   - Why or why not?\n",
    "\n",
    "6. **What happens to removed columns in old snapshots?**\n",
    "   - Are they still queryable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Hands-on Challenge\n",
    "\n",
    "### Challenge 1: Add Computed Columns\n",
    "\n",
    "1. Add a column `hour` to the events table\n",
    "2. Extract it from the `time` column\n",
    "3. Append new data with the hour populated\n",
    "4. Verify old data shows NULL, new data has values\n",
    "\n",
    "### Challenge 2: Schema Evolution History\n",
    "\n",
    "1. List all schema versions for a table\n",
    "2. For each version, show: schema ID, number of fields\n",
    "3. Identify what changed between versions\n",
    "\n",
    "### Challenge 3: Handle Type Conflicts\n",
    "\n",
    "1. Create a test dataset with type conflicts\n",
    "2. Try writing to Iceberg\n",
    "3. Use explicit schema to handle conflicts\n",
    "4. Verify data integrity\n",
    "\n",
    "Use the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Schema evolution in Iceberg is powerful and efficient:\n",
    "\n",
    "* **Adding columns**: Instant, old data shows NULL\n",
    "* **Removing columns**: Instant, preserved for time travel\n",
    "* **Renaming columns**: Safe due to column IDs\n",
    "* **Type promotion**: Automatic, safe upcasts only\n",
    "* **Schema versioning**: All versions preserved\n",
    "* **No data rewrites**: All changes are metadata-only\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Column IDs are the secret**: Names can change, IDs can't\n",
    "2. **Metadata-only changes**: No compute cost for schema evolution\n",
    "3. **Backward compatible**: Old queries still work\n",
    "4. **Time travel aware**: Historical schemas preserved\n",
    "5. **Type safety**: Only safe promotions allowed\n",
    "\n",
    "### Comparison with Traditional Systems\n",
    "\n",
    "| Operation | Traditional DB | Hive | Iceberg |\n",
    "|-----------|---------------|------|----------|\n",
    "| Add column | ALTER TABLE (seconds) | Add to metastore | Metadata update (ms) |\n",
    "| Remove column | ALTER TABLE + rewrite | Remove from metastore | Metadata update (ms) |\n",
    "| Rename column | ALTER TABLE + rewrite | Manual migration | Metadata update (ms) |\n",
    "| Type promotion | Rewrite all data | Rewrite all data | Metadata update (ms) |\n",
    "| Time travel | Not supported | Manual snapshots | Built-in |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebooks:\n",
    "* **Concurrency**: Optimistic locking and conflict resolution\n",
    "* **Partitioning**: Scale to millions of files\n",
    "* **Object stores**: Iceberg on S3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
