{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Transactions and Optimistic Concurrency\n",
    "\n",
    "Iceberg provides ACID transactions with optimistic concurrency control. This means multiple writers can work simultaneously, and Iceberg detects conflicts.\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "* **ACID properties**: How Iceberg guarantees them\n",
    "* **Optimistic concurrency**: No locks, detect conflicts at commit\n",
    "* **Conflict scenarios**: When commits fail\n",
    "* **Conflict resolution**: Retry strategies\n",
    "* **Isolation levels**: Serializable isolation\n",
    "\n",
    "## ACID in Iceberg\n",
    "\n",
    "**A**tomicity: All-or-nothing commits\n",
    "* Metadata updates are atomic (single catalog UPDATE)\n",
    "* Either everything commits or nothing does\n",
    "\n",
    "**C**onsistency: Valid states only\n",
    "* Schema always valid\n",
    "* All referenced files exist\n",
    "\n",
    "**I**solation: Writers don't interfere\n",
    "* Each writer sees a consistent snapshot\n",
    "* Serializable isolation (strongest level)\n",
    "\n",
    "**D**urability: Committed changes persist\n",
    "* Once catalog updated, change is permanent\n",
    "* Metadata and data files are durable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "import pyarrow as pa\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimistic_concurrency",
   "metadata": {},
   "source": [
    "## Optimistic Concurrency Control\n",
    "\n",
    "Traditional databases use **pessimistic locking**:\n",
    "* Writer acquires lock\n",
    "* Others wait\n",
    "* Writer releases lock\n",
    "* Problem: Locks block other writers\n",
    "\n",
    "Iceberg uses **optimistic locking**:\n",
    "* No locks acquired\n",
    "* Each writer reads current metadata\n",
    "* Makes changes\n",
    "* At commit: check if metadata still current\n",
    "* If changed: conflict detected, retry\n",
    "\n",
    "### The Optimistic Lock\n",
    "\n",
    "Remember the catalog UPDATE from the metadata notebook:\n",
    "\n",
    "```sql\n",
    "UPDATE iceberg_tables\n",
    "SET metadata_location = 'new.json',\n",
    "    previous_metadata_location = 'current.json'\n",
    "WHERE table_name = 'events'\n",
    "  AND metadata_location = 'current.json'  -- Optimistic lock!\n",
    "```\n",
    "\n",
    "If another writer committed first:\n",
    "* `metadata_location` is no longer `'current.json'`\n",
    "* WHERE clause doesn't match\n",
    "* UPDATE affects 0 rows\n",
    "* Commit fails → must retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_table",
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nwarehouse_path = Path('../data/warehouse_concurrency').absolute()\nwarehouse_path.mkdir(parents=True, exist_ok=True)\ncatalog_db = warehouse_path / 'catalog.db'\ncatalog_db.unlink(missing_ok=True)\n\ncatalog = SqlCatalog('concurrency_demo', **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'})\ncatalog.create_namespace('demo')\n\n# Create initial table\ndf_events = daft.read_json('../data/input/events.jsonl')\ndf_initial = df_events.limit(5000)\n\narrow_table = df_initial.to_arrow()\nevents_table = catalog.create_table('demo.events', schema=pa.schema(arrow_table.schema))\nevents_table.append(arrow_table)\n\nprint(f\"✅ Created table with {len(arrow_table):,} records\")"
  },
  {
   "cell_type": "markdown",
   "id": "conflict_scenarios",
   "metadata": {},
   "source": [
    "## Simulating Concurrent Writers\n",
    "\n",
    "Let's simulate two writers trying to commit simultaneously.\n",
    "\n",
    "### Scenario 1: Both append to same table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenario1",
   "metadata": {},
   "outputs": [],
   "source": "def writer_task(writer_id, start_idx, count, results):\n    \"\"\"Simulated writer that appends data\"\"\"\n    try:\n        # Each writer creates its own catalog connection\n        local_catalog = SqlCatalog('writer', **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'})\n        local_table = local_catalog.load_table('demo.events')\n        \n        # Load data using Daft\n        df_all = daft.read_json('../data/input/events.jsonl')\n        df_batch = df_all.offset(start_idx).limit(count)\n        \n        # Small delay to ensure both writers start around the same time\n        time.sleep(0.1)\n        \n        # Try to append\n        start = time.time()\n        arrow = df_batch.to_arrow()\n        local_table.append(arrow)\n        elapsed = time.time() - start\n        \n        results[writer_id] = {'success': True, 'time': elapsed, 'records': len(arrow)}\n        print(f\"✅ Writer {writer_id} committed {len(arrow)} records in {elapsed:.2f}s\")\n        \n    except Exception as e:\n        results[writer_id] = {'success': False, 'error': str(e)}\n        print(f\"❌ Writer {writer_id} failed: {e}\")\n\n# Run two writers concurrently\nprint(\"Starting two concurrent writers...\\n\")\nresults = {}\nthreads = [\n    threading.Thread(target=writer_task, args=(1, 5000, 1000, results)),\n    threading.Thread(target=writer_task, args=(2, 6000, 1000, results))\n]\n\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n\nprint(\"\\nResults:\")\nfor writer_id, result in results.items():\n    if result['success']:\n        print(f\"  Writer {writer_id}: ✅ Success ({result['records']} records in {result['time']:.2f}s)\")\n    else:\n        print(f\"  Writer {writer_id}: ❌ Failed - {result['error']}\")\n\n# Verify total count\nevents_table = catalog.load_table('demo.events')\ndf = daft.read_iceberg(events_table)\ntotal = daft.sql(\"SELECT COUNT(*) as total FROM df\").collect().to_pydict()['total'][0]\nprint(f\"\\nTotal records in table: {total:,}\")\nprint(f\"Expected: {5000 + sum(r['records'] for r in results.values() if r['success']):,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "conflict_behavior",
   "metadata": {},
   "source": [
    "### What Happened?\n",
    "\n",
    "Depending on timing, you may see:\n",
    "\n",
    "**Both succeed**: Writers committed at different times\n",
    "* Writer 1 commits → updates metadata\n",
    "* Writer 2 commits → sees new metadata, commits on top\n",
    "\n",
    "**One fails**: True concurrent commit\n",
    "* Both read same metadata\n",
    "* Writer 1 commits first\n",
    "* Writer 2's UPDATE fails (metadata changed)\n",
    "* Writer 2 must retry\n",
    "\n",
    "With SQLite + local filesystem, both usually succeed because operations are fast. In production with S3, conflicts are more common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conflict_types",
   "metadata": {},
   "source": [
    "## Types of Conflicts\n",
    "\n",
    "### Non-Conflicting Operations\n",
    "\n",
    "These can succeed concurrently:\n",
    "* Append to different partitions\n",
    "* Add different columns (schema evolution)\n",
    "* Different snapshot operations\n",
    "\n",
    "### Conflicting Operations\n",
    "\n",
    "These typically conflict:\n",
    "* Both append to same partition\n",
    "* Delete + Append\n",
    "* Schema evolution conflicts (both add same column)\n",
    "* Compaction + Write\n",
    "\n",
    "### Demonstration: Delete + Append Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delete_append_conflict",
   "metadata": {},
   "outputs": [],
   "source": "def delete_task(results):\n    try:\n        local_catalog = SqlCatalog('deleter', **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'})\n        local_table = local_catalog.load_table('demo.events')\n        time.sleep(0.05)\n        local_table.delete(\"type = 'c8y_LocationUpdate'\")\n        results['delete'] = {'success': True}\n        print(\"✅ Delete committed\")\n    except Exception as e:\n        results['delete'] = {'success': False, 'error': str(e)}\n        print(f\"❌ Delete failed: {e}\")\n\ndef append_task(results):\n    try:\n        local_catalog = SqlCatalog('appender', **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'})\n        local_table = local_catalog.load_table('demo.events')\n        \n        # Load data using Daft\n        df_all = daft.read_json('../data/input/events.jsonl')\n        df_batch = df_all.offset(7000).limit(500)\n        \n        time.sleep(0.05)\n        arrow = df_batch.to_arrow()\n        local_table.append(arrow)\n        results['append'] = {'success': True, 'records': len(arrow)}\n        print(f\"✅ Append committed ({len(arrow)} records)\")\n    except Exception as e:\n        results['append'] = {'success': False, 'error': str(e)}\n        print(f\"❌ Append failed: {e}\")\n\nprint(\"Simulating concurrent delete + append...\\n\")\nresults = {}\nthreads = [\n    threading.Thread(target=delete_task, args=(results,)),\n    threading.Thread(target=append_task, args=(results,))\n]\n\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n\nprint(\"\\nResult: One likely succeeded, the other would need to retry in production.\")"
  },
  {
   "cell_type": "markdown",
   "id": "retry_strategy",
   "metadata": {},
   "source": [
    "## Conflict Resolution: Retry Strategy\n",
    "\n",
    "When a commit fails, the application should:\n",
    "\n",
    "1. **Reload table metadata**: Get the new current metadata\n",
    "2. **Reapply changes**: Recalculate what needs to be done\n",
    "3. **Retry commit**: Try again with updated metadata\n",
    "4. **Exponential backoff**: Wait longer between retries\n",
    "\n",
    "Here's a retry wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retry_wrapper",
   "metadata": {},
   "outputs": [],
   "source": "def retry_on_conflict(func, max_retries=3, initial_delay=0.1):\n    \"\"\"\n    Retry a function on conflict with exponential backoff.\n    \n    Args:\n        func: Function to execute\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay in seconds\n    \"\"\"\n    delay = initial_delay\n    \n    for attempt in range(max_retries + 1):\n        try:\n            return func()\n        except Exception as e:\n            if attempt < max_retries:\n                print(f\"Attempt {attempt + 1} failed: {e}\")\n                print(f\"Retrying in {delay:.2f}s...\")\n                time.sleep(delay)\n                delay *= 2  # Exponential backoff\n            else:\n                print(f\"All {max_retries + 1} attempts failed\")\n                raise\n\n# Example usage\ndef append_with_retry():\n    catalog = SqlCatalog('retry_demo', **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'})\n    table = catalog.load_table('demo.events')\n    \n    # Load data using Daft\n    df_all = daft.read_json('../data/input/events.jsonl')\n    df_batch = df_all.offset(8000).limit(100)\n    \n    arrow = df_batch.to_arrow()\n    table.append(arrow)\n    return len(arrow)\n\ntry:\n    records = retry_on_conflict(append_with_retry, max_retries=3)\n    print(f\"\\n✅ Successfully appended {records} records (with retries if needed)\")\nexcept Exception as e:\n    print(f\"\\n❌ Failed after retries: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "isolation",
   "metadata": {},
   "source": [
    "## Serializable Isolation\n",
    "\n",
    "Iceberg provides **serializable isolation** - the strongest isolation level:\n",
    "\n",
    "* Each transaction sees a consistent snapshot\n",
    "* Transactions appear to execute in serial order\n",
    "* No dirty reads, non-repeatable reads, or phantoms\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Writer reads snapshot X\n",
    "2. Makes changes based on X\n",
    "3. At commit: checks if X is still current\n",
    "4. If not: conflict detected\n",
    "\n",
    "This is stronger than most databases' default (read-committed).\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Isolation Level | Dirty Read | Non-Repeatable Read | Phantom Read | Iceberg |\n",
    "|----------------|-----------|---------------------|--------------|----------|\n",
    "| Read Uncommitted | ✅ | ✅ | ✅ | ❌ |\n",
    "| Read Committed | ❌ | ✅ | ✅ | ❌ |\n",
    "| Repeatable Read | ❌ | ❌ | ✅ | ❌ |\n",
    "| Serializable | ❌ | ❌ | ❌ | ✅ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production",
   "metadata": {},
   "source": [
    "## Production Considerations\n",
    "\n",
    "### Catalog Choice Matters\n",
    "\n",
    "* **SQLite (this demo)**: Not suitable for production\n",
    "  - Single file, local only\n",
    "  - Limited concurrency\n",
    "\n",
    "* **AWS Glue**: Good for AWS\n",
    "  - Managed service\n",
    "  - Built-in optimistic locking\n",
    "  - DynamoDB-backed\n",
    "\n",
    "* **Nessie**: Git-like catalog\n",
    "  - Branches and tags\n",
    "  - Multi-table transactions\n",
    "  - Good for complex workflows\n",
    "\n",
    "* **REST Catalog**: Generic HTTP-based\n",
    "  - Works with any backend\n",
    "  - Custom implementation\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always retry on conflict**: Conflicts are expected\n",
    "2. **Use exponential backoff**: Don't hammer the catalog\n",
    "3. **Keep transactions short**: Less chance of conflict\n",
    "4. **Partition your data**: Reduces write conflicts\n",
    "5. **Monitor conflict rate**: High rate indicates design issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "1. **Why is optimistic concurrency better for data lakes than pessimistic locking?**\n",
    "   - Think about S3, distributed systems, and long-running operations.\n",
    "\n",
    "2. **What would happen without conflict detection?**\n",
    "   - How would two writers corrupt each other?\n",
    "\n",
    "3. **How would you design a retry strategy for production?**\n",
    "   - Backoff? Max retries? Logging?\n",
    "\n",
    "4. **Can reads block writes in Iceberg?**\n",
    "   - What about writes blocking reads?\n",
    "\n",
    "5. **Why is serializable isolation expensive in most databases?**\n",
    "   - How does Iceberg make it cheap?\n",
    "\n",
    "6. **What's the difference between a conflict and a failure?**\n",
    "   - When should you retry vs. give up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Hands-on Challenge\n",
    "\n",
    "### Challenge 1: Simulate Schema Conflict\n",
    "\n",
    "1. Two writers try to add different columns\n",
    "2. Both use same initial schema\n",
    "3. One succeeds, one conflicts\n",
    "4. Implement retry logic\n",
    "\n",
    "### Challenge 2: Conflict Rate Monitor\n",
    "\n",
    "1. Run 10 concurrent writers\n",
    "2. Track success vs. failure rate\n",
    "3. Calculate: conflict percentage\n",
    "4. Plot: conflicts over time\n",
    "\n",
    "### Challenge 3: Partition-Based Isolation\n",
    "\n",
    "1. Create a partitioned table\n",
    "2. Writers each write to different partitions\n",
    "3. Verify: no conflicts\n",
    "4. Compare with unpartitioned\n",
    "\n",
    "Use the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Iceberg provides robust concurrency control:\n",
    "\n",
    "* **ACID transactions**: All-or-nothing, consistent, isolated, durable\n",
    "* **Optimistic concurrency**: No locks, detect conflicts at commit\n",
    "* **Serializable isolation**: Strongest consistency guarantee\n",
    "* **Automatic conflict detection**: Via catalog UPDATE\n",
    "* **Retry-friendly**: Conflicts are expected and handleable\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **No locks = better scalability**: Writers don't block each other\n",
    "2. **Conflicts are normal**: Build retry logic\n",
    "3. **Catalog matters**: Choose production-ready catalog\n",
    "4. **Partitioning reduces conflicts**: Independent partitions don't conflict\n",
    "5. **Short transactions win**: Less time = less chance of conflict\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "* **Partitioning**: How to scale to millions of files\n",
    "* **Object stores**: Iceberg on S3 with efficient metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}