{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Partitioning and performance\n",
    "\n",
    "NOTE: THIS NOTEBOOK IS WORK IN PROGRESS.\n",
    "\n",
    "As Iceberg tables grow to millions of files and trillions of rows, efficiently locating relevant data becomes critical. **Partitioning** is Iceberg's primary tool for this.\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "* Why partitioning matters for large tables\n",
    "* Iceberg's hidden partitioning vs. Hive-style partitioning\n",
    "* Partition transforms: identity, bucket, truncate, and time-based\n",
    "* Creating and querying partitioned tables\n",
    "* Partition evolution: changing partition scheme without rewrites\n",
    "* IoT-specific partitioning patterns\n",
    "\n",
    "Partitioning allows Iceberg to skip entire partitions during query execution. For a table with 1 million files across 365 daily partitions, a single-day query needs to scan only ~2,700 files instead of all 1 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import daft\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from pyiceberg.transforms import (\n",
    "    IdentityTransform,\n",
    "    BucketTransform,\n",
    "    TruncateTransform,\n",
    "    DayTransform,\n",
    "    HourTransform,\n",
    "    MonthTransform,\n",
    "    YearTransform,\n",
    ")\n",
    "\n",
    "os.environ[\"DAFT_DASHBOARD_ENABLED\"] = \"0\"\n",
    "os.environ[\"DAFT_PROGRESS_BAR\"] = \"0\"\n",
    "\n",
    "warehouse_path = Path('../data/warehouse_partitioning').absolute()\n",
    "shutil.rmtree(warehouse_path, ignore_errors=True)\n",
    "warehouse_path.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = warehouse_path / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    'partitioning_demo',\n",
    "    **{'uri': f'sqlite:///{catalog_db}', 'warehouse': f'file://{warehouse_path}'}\n",
    ")\n",
    "catalog.create_namespace('demo')\n",
    "\n",
    "# Load events data\n",
    "df_events = daft.read_json('../data/input/events.jsonl')\n",
    "print(f\"Loaded {df_events.count_rows():,} events\")\n",
    "print(\"\\nInferred schema:\")\n",
    "print(df_events.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why_partition",
   "metadata": {},
   "source": [
    "## Why partition?\n",
    "\n",
    "Without partitioning, every query must check every file. With 10,000 Parquet files:\n",
    "\n",
    "1. Read all 10,000 manifest entries\n",
    "2. Check per-file statistics (min/max bounds) for each file\n",
    "3. Read data files that pass the filter\n",
    "\n",
    "With partitioning by event type (say 50 types), a query for one type:\n",
    "\n",
    "1. Read manifest entries - and immediately **skip 49 out of 50 partitions** without checking file-level statistics\n",
    "2. Read only files in the matching partition\n",
    "\n",
    "This can be a 50x reduction in files to scan.\n",
    "\n",
    "### Hive-style partitioning vs. hidden partitioning\n",
    "\n",
    "Traditional **Hive-style partitioning** uses physical directories:\n",
    "\n",
    "```\n",
    "events/\n",
    "  year=2024/month=01/day=01/data.parquet\n",
    "  year=2024/month=01/day=02/data.parquet\n",
    "```\n",
    "\n",
    "Problems:\n",
    "* Users **must** include partition columns in queries: `WHERE year=2024 AND month=1`\n",
    "* Changing the partition scheme requires rewriting all data\n",
    "* Partition columns are duplicate data (stored in both the path and the file)\n",
    "\n",
    "Iceberg uses **hidden partitioning**:\n",
    "\n",
    "* Users write normal predicates: `WHERE time > '2024-01-01'`\n",
    "* Iceberg **automatically maps** the predicate to partition boundaries\n",
    "* Changing the partition scheme doesn't break existing queries\n",
    "* Partition columns are **not** stored in the data - only the transform result\n",
    "\n",
    "The partition structure is \"hidden\" because users don't need to know about it to write correct queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_partitioned_header",
   "metadata": {},
   "source": [
    "## Creating a partitioned table\n",
    "\n",
    "Let's create a table partitioned by event `type` using the `IdentityTransform`. This means each unique value of `type` gets its own partition.\n",
    "\n",
    "After creating the table, we use `update_spec()` to define the partition. This allows us to add partitioning to an existing table schema, and also to **evolve** the partition scheme later without rewriting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_partitioned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table from inferred schema\n",
    "arrow_sample = df_events.limit(1000).to_arrow()\n",
    "events_table = catalog.create_table(\n",
    "    'demo.events_by_type',\n",
    "    schema=pa.schema(arrow_sample.schema)\n",
    ")\n",
    "\n",
    "# Add partition spec: partition by 'type' using identity transform\n",
    "# IdentityTransform means: one partition per unique value of 'type'\n",
    "with events_table.update_spec() as update:\n",
    "    update.add_field(\n",
    "        source_column_name='type',\n",
    "        transform=IdentityTransform(),\n",
    "        partition_field_name='type'\n",
    "    )\n",
    "\n",
    "print(\"✅ Created partitioned table\")\n",
    "print(f\"\\nPartition spec:\")\n",
    "for field in events_table.spec().fields:\n",
    "    print(f\"  Field '{field.name}': {field.transform} on source field ID {field.source_id}\")\n",
    "\n",
    "# Write a larger sample\n",
    "arrow_batch = df_events.limit(50000).to_arrow()\n",
    "events_table.append(arrow_batch)\n",
    "print(f\"\\n✅ Appended {len(arrow_batch):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file_structure_header",
   "metadata": {},
   "source": [
    "### File structure with partitioning\n",
    "\n",
    "Let's look at how the data files are organized. Each partition gets its own directory named after the partition value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file_structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the partition directory structure\n",
    "table_dir = Path(events_table.location().replace('file://', ''))\n",
    "data_dir = table_dir / 'data'\n",
    "\n",
    "partitions = sorted(data_dir.iterdir()) if data_dir.exists() else []\n",
    "\n",
    "print(f\"Data directory: {data_dir.relative_to(warehouse_path)}\")\n",
    "print(f\"Number of partitions: {len(partitions)}\")\n",
    "print()\n",
    "\n",
    "total_size = 0\n",
    "for partition_dir in sorted(partitions):\n",
    "    files = list(partition_dir.glob('*.parquet'))\n",
    "    partition_size = sum(f.stat().st_size for f in files)\n",
    "    total_size += partition_size\n",
    "    print(f\"  {partition_dir.name}/\")\n",
    "    for f in files:\n",
    "        print(f\"    {f.name} ({f.stat().st_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nTotal data size: {total_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partition_pruning_header",
   "metadata": {},
   "source": [
    "## Partition pruning in queries\n",
    "\n",
    "When Daft queries an Iceberg table with a filter on the partition column, it can skip entire partitions. The manifest list already knows which event types are in which manifests, so Daft never needs to read irrelevant partitions.\n",
    "\n",
    "Note that this works even though our query uses the regular `type` column name - Daft knows internally that `type` is a partition column and applies partition pruning automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partition_pruning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: check what event types exist\n",
    "df = daft.read_iceberg(events_table)\n",
    "print(\"Event types in the table:\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT type, COUNT(*) as count\n",
    "    FROM df\n",
    "    GROUP BY type\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partition_filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for a specific type - Daft will only read the matching partition\n",
    "df_filtered = daft.read_iceberg(events_table)\n",
    "df_alarm = df_filtered.filter(daft.col('type') == 'c8y_LocationUpdate')\n",
    "print(\"Querying for type = 'c8y_LocationUpdate'\")\n",
    "print(\"(Only the matching partition directory will be read)\")\n",
    "print()\n",
    "df_alarm.select('id', 'type', 'time', 'source').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manifest_inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the manifest to see how partition information is stored\n",
    "import fastavro\n",
    "from pathlib import Path\n",
    "\n",
    "# Find a manifest file\n",
    "metadata_dir = table_dir / 'metadata'\n",
    "manifest_files = list(metadata_dir.glob('*-m*.avro'))\n",
    "\n",
    "if manifest_files:\n",
    "    manifest = manifest_files[0]\n",
    "    with open(manifest, 'rb') as f:\n",
    "        reader = fastavro.reader(f)\n",
    "        records = list(reader)\n",
    "\n",
    "    print(f\"Manifest: {manifest.name}\")\n",
    "    print(f\"Contains {len(records)} file entries\\n\")\n",
    "\n",
    "    # Show partition info for first few entries\n",
    "    for i, record in enumerate(records[:5], 1):\n",
    "        data_file = record.get('data_file', {})\n",
    "        partition = data_file.get('partition', {})\n",
    "        print(f\"  Entry {i}: partition={partition}, records={data_file.get('record_count', 0):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transforms_header",
   "metadata": {},
   "source": [
    "## Partition transforms\n",
    "\n",
    "Iceberg supports several transforms that control how partition values are computed from column values:\n",
    "\n",
    "| Transform | Input type | Output | Use case |\n",
    "|-----------|-----------|--------|----------|\n",
    "| `identity` | Any | Column value unchanged | Exact value partitioning for low cardinality |\n",
    "| `year` | timestamp/date | Year integer | Annual partitioning |\n",
    "| `month` | timestamp/date | Year+month integer | Monthly partitioning |\n",
    "| `day` | timestamp/date | Date integer | Daily partitioning |\n",
    "| `hour` | timestamp | Year+hour integer | Hourly partitioning |\n",
    "| `bucket(n)` | int, long, string, date, timestamp | Bucket 0..n-1 | High-cardinality columns |\n",
    "| `truncate(w)` | int, long, string | First w chars (string) or rounded (int/long) | Prefix partitioning |\n",
    "\n",
    "### Choosing the right transform\n",
    "\n",
    "* **Identity**: Use when cardinality is low (< 100 distinct values) and values are stable. Good for: event type, status, region.\n",
    "* **Time-based (year/month/day/hour)**: Use for time-series data. Choose granularity based on query patterns and file sizes. For IoT with millions of events/day: `hour`. For slower data: `day` or `month`.\n",
    "* **Bucket**: Use for high-cardinality ID columns (device ID, source ID). Distributes data evenly, avoids creating too many partitions. Good for: device_id, tenant_id, user_id.\n",
    "* **Truncate**: Use for string prefixes or integer ranges. Less common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time_partition_header",
   "metadata": {},
   "source": [
    "### Time-based partitioning\n",
    "\n",
    "For IoT time-series data, partitioning by time is the most common pattern. Iceberg transforms timestamps into year, month, day, or hour partitions.\n",
    "\n",
    "> **Note:** The time-based transforms (`DayTransform`, `HourTransform`, etc.) require the source column to be of a **timestamp** type in the Iceberg schema. If your time column is stored as a string, you need to cast it to a timestamp type first.\n",
    "\n",
    "Let's create a time-partitioned table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import NestedField, StringType, TimestampType\n",
    "\n",
    "# Create a synthetic table with an explicit timestamp column to demonstrate time partitioning\n",
    "# We use PyArrow's timestamp type so Iceberg gets a proper timestamp field\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate synthetic time-series data spanning multiple days\n",
    "np.random.seed(42)\n",
    "n_records = 10000\n",
    "base_time = datetime(2024, 1, 1)\n",
    "timestamps = [base_time + timedelta(hours=i * 0.5) for i in range(n_records)]\n",
    "\n",
    "ts_data = pa.table({\n",
    "    'event_time': pa.array(timestamps, type=pa.timestamp('us')),\n",
    "    'device_id': pa.array([f'device_{i % 100}' for i in range(n_records)]),\n",
    "    'value': pa.array(np.random.randn(n_records).tolist()),\n",
    "    'event_type': pa.array(['alarm' if i % 10 == 0 else 'measurement' for i in range(n_records)])\n",
    "})\n",
    "\n",
    "print(f\"Generated {n_records:,} synthetic IoT records\")\n",
    "print(f\"Time range: {timestamps[0].date()} to {timestamps[-1].date()}\")\n",
    "print(f\"Schema: {ts_data.schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_partition_create",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table and partition by day\n",
    "ts_table = catalog.create_table(\n",
    "    'demo.timeseries',\n",
    "    schema=pa.schema(ts_data.schema)\n",
    ")\n",
    "\n",
    "# Add day-based partition on the timestamp column\n",
    "with ts_table.update_spec() as update:\n",
    "    update.add_field(\n",
    "        source_column_name='event_time',\n",
    "        transform=DayTransform(),\n",
    "        partition_field_name='event_day'\n",
    "    )\n",
    "\n",
    "print(\"✅ Created time-partitioned table\")\n",
    "print(f\"\\nPartition spec: {ts_table.spec()}\")\n",
    "\n",
    "# Write data\n",
    "ts_table.append(ts_data)\n",
    "print(f\"\\n✅ Appended {n_records:,} records\")\n",
    "\n",
    "# Show resulting partitions\n",
    "ts_dir = Path(ts_table.location().replace('file://', ''))\n",
    "ts_data_dir = ts_dir / 'data'\n",
    "\n",
    "print(\"\\nResulting partitions (by day):\")\n",
    "if ts_data_dir.exists():\n",
    "    for partition_dir in sorted(ts_data_dir.iterdir()):\n",
    "        files = list(partition_dir.glob('*.parquet'))\n",
    "        total = sum(f.stat().st_size for f in files)\n",
    "        print(f\"  {partition_dir.name} → {len(files)} file(s), {total:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time_query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with a time filter - only matching day partitions will be read\n",
    "df_ts = daft.read_iceberg(ts_table)\n",
    "\n",
    "print(\"Counting records per day:\")\n",
    "daft.sql(\"\"\"\n",
    "    SELECT\n",
    "        CAST(event_time AS DATE) as day,\n",
    "        COUNT(*) as event_count,\n",
    "        SUM(CASE WHEN event_type = 'alarm' THEN 1 ELSE 0 END) as alarms\n",
    "    FROM df_ts\n",
    "    GROUP BY CAST(event_time AS DATE)\n",
    "    ORDER BY day\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bucket_header",
   "metadata": {},
   "source": [
    "### Bucket partitioning\n",
    "\n",
    "**Bucket** partitioning is designed for high-cardinality columns like device IDs or user IDs. Instead of creating one partition per unique value (which could be millions), it hashes values into a fixed number of buckets.\n",
    "\n",
    "With `BucketTransform(n=16)`, Iceberg applies a hash function and takes the result modulo 16. This guarantees exactly 16 partitions regardless of how many distinct device IDs exist.\n",
    "\n",
    "Benefits:\n",
    "* **Controlled partition count**: Always exactly n partitions\n",
    "* **Even distribution**: Hash functions distribute data uniformly\n",
    "* **Good for joins**: Two tables with the same bucket spec on the same column can be joined without shuffling\n",
    "\n",
    "The trade-off: You can no longer filter by a specific device ID at the partition level (the query engine must check all buckets). But you gain control over partition count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bucket_partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table partitioned by bucket on device_id\n",
    "bucket_table = catalog.create_table(\n",
    "    'demo.timeseries_bucketed',\n",
    "    schema=pa.schema(ts_data.schema)\n",
    ")\n",
    "\n",
    "NUM_BUCKETS = 8\n",
    "\n",
    "with bucket_table.update_spec() as update:\n",
    "    update.add_field(\n",
    "        source_column_name='device_id',\n",
    "        transform=BucketTransform(num_buckets=NUM_BUCKETS),\n",
    "        partition_field_name='device_bucket'\n",
    "    )\n",
    "\n",
    "bucket_table.append(ts_data)\n",
    "print(f\"✅ Wrote {n_records:,} records to {NUM_BUCKETS}-bucket partitioned table\")\n",
    "\n",
    "# Show bucket distribution\n",
    "bucket_dir = Path(bucket_table.location().replace('file://', '')) / 'data'\n",
    "\n",
    "print(\"\\nBucket distribution:\")\n",
    "if bucket_dir.exists():\n",
    "    for partition_dir in sorted(bucket_dir.iterdir()):\n",
    "        files = list(partition_dir.glob('*.parquet'))\n",
    "        total = sum(f.stat().st_size for f in files)\n",
    "        print(f\"  {partition_dir.name}: {total:,} bytes\")\n",
    "\n",
    "print(f\"\\nWith {n_records:,} records across {n_records // 100} unique devices and {NUM_BUCKETS} buckets,\")\n",
    "print(f\"each bucket contains roughly {n_records // NUM_BUCKETS:,} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi_partition_header",
   "metadata": {},
   "source": [
    "### Multi-column partitioning\n",
    "\n",
    "You can combine multiple partition transforms to create a hierarchical partition scheme. For IoT data, a natural combination is:\n",
    "\n",
    "* **Day** on time: separate data by date\n",
    "* **Bucket** on device_id: within each day, distribute devices across buckets\n",
    "\n",
    "This creates a two-level hierarchy. A query like `WHERE event_time = '2024-01-15' AND device_id = 'device_42'` can skip:\n",
    "\n",
    "1. All partitions not in day `2024-01-15`\n",
    "2. Within that day, all buckets that don't contain `device_42`'s hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with combined day + bucket partitioning\n",
    "multi_table = catalog.create_table(\n",
    "    'demo.timeseries_multi',\n",
    "    schema=pa.schema(ts_data.schema)\n",
    ")\n",
    "\n",
    "with multi_table.update_spec() as update:\n",
    "    update.add_field(\n",
    "        source_column_name='event_time',\n",
    "        transform=DayTransform(),\n",
    "        partition_field_name='event_day'\n",
    "    )\n",
    "    update.add_field(\n",
    "        source_column_name='device_id',\n",
    "        transform=BucketTransform(num_buckets=4),\n",
    "        partition_field_name='device_bucket'\n",
    "    )\n",
    "\n",
    "multi_table.append(ts_data)\n",
    "\n",
    "print(\"✅ Created multi-column partitioned table\")\n",
    "print(f\"\\nPartition spec:\")\n",
    "for field in multi_table.spec().fields:\n",
    "    print(f\"  {field.name}: {field.transform}\")\n",
    "\n",
    "# Show the resulting structure\n",
    "multi_dir = Path(multi_table.location().replace('file://', '')) / 'data'\n",
    "print(\"\\nPartition directories (day/bucket):\")\n",
    "if multi_dir.exists():\n",
    "    partitions = sorted(multi_dir.iterdir())\n",
    "    shown = 0\n",
    "    for p in partitions:\n",
    "        sub_partitions = sorted(p.iterdir()) if p.is_dir() else []\n",
    "        for sp in sub_partitions[:2]:  # Show first 2 sub-partitions per day\n",
    "            files = list(sp.glob('*.parquet'))\n",
    "            print(f\"  {p.name}/{sp.name}/: {len(files)} file(s)\")\n",
    "            shown += 1\n",
    "        if len(sub_partitions) > 2:\n",
    "            print(f\"  {p.name}/... and {len(sub_partitions) - 2} more bucket(s)\")\n",
    "        if shown >= 10:\n",
    "            remaining = sum(1 for _ in multi_dir.iterdir()) - shown\n",
    "            if remaining > 0:\n",
    "                print(f\"  ... and more partitions\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partition_evolution_header",
   "metadata": {},
   "source": [
    "## Partition evolution\n",
    "\n",
    "One of Iceberg's most powerful features: you can **change the partition scheme without rewriting data**.\n",
    "\n",
    "Suppose you initially partition by month but queries become too slow because monthly partitions are too large. You want to switch to daily partitioning. In Hive, this would require rewriting terabytes of data. In Iceberg:\n",
    "\n",
    "1. Add the new partition spec\n",
    "2. New writes use the new spec\n",
    "3. Old data remains in old partitions\n",
    "4. Queries work transparently across both old and new partition schemes\n",
    "\n",
    "This is possible because Iceberg stores the partition spec **per data file** in the manifest. The query engine knows which spec each file uses and applies the appropriate pruning.\n",
    "\n",
    "Let's demonstrate: start with monthly partitioning, then evolve to daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partition_evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with monthly partitioning first\n",
    "evolving_table = catalog.create_table(\n",
    "    'demo.timeseries_evolving',\n",
    "    schema=pa.schema(ts_data.schema)\n",
    ")\n",
    "\n",
    "with evolving_table.update_spec() as update:\n",
    "    update.add_field(\n",
    "        source_column_name='event_time',\n",
    "        transform=MonthTransform(),\n",
    "        partition_field_name='event_month'\n",
    "    )\n",
    "\n",
    "# Write first batch with monthly partitioning\n",
    "evolving_table.append(ts_data)\n",
    "\n",
    "print(\"Phase 1: Monthly partitioning\")\n",
    "print(f\"Spec ID: {evolving_table.spec().spec_id}\")\n",
    "evol_dir = Path(evolving_table.location().replace('file://', '')) / 'data'\n",
    "if evol_dir.exists():\n",
    "    for p in sorted(evol_dir.iterdir()):\n",
    "        print(f\"  {p.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partition_evolution2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve to daily partitioning - no data rewrite needed!\n",
    "with evolving_table.update_spec() as update:\n",
    "    update.remove_field('event_month')  # Remove old partition\n",
    "    update.add_field(\n",
    "        source_column_name='event_time',\n",
    "        transform=DayTransform(),\n",
    "        partition_field_name='event_day'\n",
    "    )\n",
    "\n",
    "# Reload to get the updated spec\n",
    "evolving_table = catalog.load_table('demo.timeseries_evolving')\n",
    "\n",
    "print(\"Phase 2: Evolved to daily partitioning\")\n",
    "print(f\"New spec ID: {evolving_table.spec().spec_id}\")\n",
    "\n",
    "# Write more data with the new spec\n",
    "# Generate data for the next month\n",
    "base_time2 = datetime(2024, 3, 1)\n",
    "timestamps2 = [base_time2 + timedelta(hours=i * 0.5) for i in range(5000)]\n",
    "ts_data2 = pa.table({\n",
    "    'event_time': pa.array(timestamps2, type=pa.timestamp('us')),\n",
    "    'device_id': pa.array([f'device_{i % 100}' for i in range(5000)]),\n",
    "    'value': pa.array(np.random.randn(5000).tolist()),\n",
    "    'event_type': pa.array(['alarm' if i % 10 == 0 else 'measurement' for i in range(5000)])\n",
    "})\n",
    "\n",
    "evolving_table.append(ts_data2)\n",
    "\n",
    "print(\"\\nData directory after evolution:\")\n",
    "if evol_dir.exists():\n",
    "    for p in sorted(evol_dir.iterdir()):\n",
    "        if p.is_dir():\n",
    "            sub_dirs = list(p.iterdir()) if p.name.startswith('event_month') else []\n",
    "            if sub_dirs:\n",
    "                print(f\"  {p.name}/ (old monthly partition)\")\n",
    "            else:\n",
    "                print(f\"  {p.name}/ (new daily partition)\")\n",
    "\n",
    "print(\"\\n✅ Old data stays in monthly partitions, new data uses daily partitions\")\n",
    "print(\"   Queries work transparently across both!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evolution_query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries work transparently across old and new partition schemes\n",
    "df_evolving = daft.read_iceberg(evolving_table)\n",
    "\n",
    "print(\"Total records across all partition specs:\")\n",
    "daft.sql(\"SELECT COUNT(*) as total FROM df_evolving\").show()\n",
    "\n",
    "print(\"\\nRecords by date (spans both partition schemes):\")\n",
    "# Cast to date and count records per date\n",
    "df_by_date = daft.sql(\"\"\"\n",
    "    SELECT\n",
    "        CAST(event_time AS DATE) as event_date,\n",
    "        COUNT(*) as count\n",
    "    FROM df_evolving\n",
    "    GROUP BY CAST(event_time AS DATE)\n",
    "    ORDER BY event_date\n",
    "\"\"\")\n",
    "df_by_date.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iot_patterns",
   "metadata": {},
   "source": [
    "## IoT partitioning patterns\n",
    "\n",
    "For IoT data, the right partitioning strategy depends on your query patterns:\n",
    "\n",
    "### Pattern 1: Time-first (most common)\n",
    "```\n",
    "Partition by: day(event_time), bucket(device_id, 16)\n",
    "```\n",
    "Best for:\n",
    "* Most queries filter by time range\n",
    "* Dashboards showing recent data\n",
    "* Retention policies (delete old day partitions)\n",
    "\n",
    "### Pattern 2: Device-first\n",
    "```\n",
    "Partition by: bucket(device_id, 64), day(event_time)\n",
    "```\n",
    "Best for:\n",
    "* Device-specific analytics dominate\n",
    "* Queries like \"all events for device X in the last year\"\n",
    "\n",
    "### Pattern 3: Type-based routing\n",
    "```\n",
    "Partition by: identity(event_type), day(event_time)\n",
    "```\n",
    "Best for:\n",
    "* Few distinct event types\n",
    "* Analytics segregated by type (alarms vs. measurements)\n",
    "* Type-specific retention policies\n",
    "\n",
    "### Key guidelines\n",
    "\n",
    "* **Avoid identity partitioning on high-cardinality columns**: If you have 10 million device IDs, you get 10 million partitions → metadata overhead becomes extreme\n",
    "* **Target file size**: Aim for 128 MB-1 GB per Parquet file. Too many small files hurt performance.\n",
    "* **Partition granularity**: With 1 million events/day, use `day`. With 100 events/day, use `month`.\n",
    "* **Query patterns drive design**: Partition to skip most data for your most frequent queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review questions\n",
    "\n",
    "**What is the key difference between Hive-style and Iceberg hidden partitioning?**\n",
    "   - How does each affect the queries users write?\n",
    "   - What happens when you change the partition scheme?\n",
    "\n",
    "**Why is identity partitioning dangerous for high-cardinality columns?**\n",
    "   - What happens to manifest size with 10 million partitions?\n",
    "   - What would you use instead?\n",
    "\n",
    "**How does partition evolution work without data rewrites?**\n",
    "   - Where is the partition spec stored per file?\n",
    "   - How can old and new partition specs coexist?\n",
    "\n",
    "**When should you use bucket vs. day partitioning on a timestamp column?**\n",
    "   - What queries benefit from day partitioning?\n",
    "   - Can bucket partitioning coexist with day partitioning?\n",
    "\n",
    "**How do you choose the number of buckets for BucketTransform?**\n",
    "   - What factors influence this decision?\n",
    "   - Can you change the number of buckets later?\n",
    "\n",
    "**What is the \"small files problem\" and how does partitioning contribute to it?**\n",
    "   - When does this become a problem?\n",
    "   - What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Partition the events table by type and compare performance\n",
    "\n",
    "1. Create an unpartitioned version of the events table with 100,000 records\n",
    "2. Create a version partitioned by `identity(type)`\n",
    "3. Query both tables for a specific event type\n",
    "4. How many files are read in each case? Check the manifest entries.\n",
    "\n",
    "### Experiment with bucket counts\n",
    "\n",
    "1. Create three tables partitioned by `bucket(device_id, n)` with n=4, 8, 16\n",
    "2. Write the same 10,000 records to all three\n",
    "3. Check how evenly data is distributed across buckets\n",
    "4. Which count gives the most even distribution?\n",
    "\n",
    "### IoT data partitioning analysis\n",
    "\n",
    "Using the events.jsonl data:\n",
    "1. Count distinct values of `type` field\n",
    "2. Decide: is identity or bucket better for `type`?\n",
    "3. If `time` is a timestamp, what granularity (hour/day/month) makes sense?\n",
    "   - Hint: how many events are there per day?\n",
    "4. Design the optimal partition spec for this dataset\n",
    "\n",
    "### Partition evolution experiment\n",
    "\n",
    "1. Create a table with `identity(type)` partitioning\n",
    "2. Write 20,000 events\n",
    "3. Evolve the partition spec to add `bucket(type, 4)` instead\n",
    "4. Write another 10,000 events\n",
    "5. Verify that queries return the correct total across both partition specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Partitioning is one of the most impactful performance features in Iceberg:\n",
    "\n",
    "* **Hidden partitioning**: Users write normal predicates; Iceberg handles partition pruning automatically\n",
    "* **Transforms**: Identity, bucket, truncate, and time-based (year/month/day/hour) handle all common patterns\n",
    "* **Partition pruning**: Entire partitions are skipped during query planning, before reading any data files\n",
    "* **Partition evolution**: Change the partition scheme without rewriting data - old and new specs coexist transparently\n",
    "* **Multi-column partitioning**: Combine transforms for hierarchical partition schemes\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "1. **Design for your queries**: Partition to skip data for your most frequent predicates\n",
    "2. **Avoid high-cardinality identity**: Use bucket for device IDs, user IDs, etc.\n",
    "3. **Time-first for IoT**: Time-based partitioning is almost always beneficial for time-series data\n",
    "4. **Partition evolution is safe**: Don't be afraid to change the partition scheme as requirements evolve\n",
    "5. **Balance granularity**: Too fine → too many small files; too coarse → too little pruning\n",
    "\n",
    "### What's next?\n",
    "\n",
    "Next, we'll look at how Iceberg works on object stores like S3 - the standard deployment target for production data lakes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
