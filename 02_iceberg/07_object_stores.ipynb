{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Iceberg on object stores\n",
    "\n",
    "NOTE: THIS NOTEBOOK IS WORK IN PROGRESS.\n",
    "\n",
    "In the Parquet module, we saw how query engines make byte-range requests to object stores to read only the relevant chunks of a Parquet file. Iceberg adds another layer: before touching any data file, it reads a chain of small metadata files.\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "* Why object stores are the standard for data lakes\n",
    "* How to configure Iceberg with S3\n",
    "* What requests Iceberg makes to S3 to serve a query\n",
    "* The small files problem and compaction\n",
    "* Object store-specific design considerations\n",
    "\n",
    "We'll use the same simulated S3 setup from the Parquet module so you can observe exactly which HTTP requests Iceberg makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "object_stores_intro",
   "metadata": {},
   "source": [
    "## Object stores for data lakes\n",
    "\n",
    "Object stores (AWS S3, Google Cloud Storage, Azure Blob Storage) are the dominant storage layer for modern data lakes. Unlike local filesystems:\n",
    "\n",
    "| Property | Local filesystem | Object store |\n",
    "|----------|-----------------|--------------|\n",
    "| Latency | < 1 ms | 50-200 ms |\n",
    "| Throughput | High | Very high (parallel requests) |\n",
    "| Cost | Expensive | Very cheap |\n",
    "| Durability | Single disk | 99.999999999% |\n",
    "| Consistency | Immediate | Eventual (S3: strong after 2020) |\n",
    "| Scalability | Limited | Virtually unlimited |\n",
    "| Operations | POSIX | GET, PUT, DELETE, LIST |\n",
    "\n",
    "Object stores have no true directories, no random writes, and no append operations - only full object reads and writes. This means:\n",
    "\n",
    "* **No in-place updates**: To update a file, you must write a new one\n",
    "* **No atomic multi-file operations**: You must coordinate this yourself (Iceberg does this via the catalog)\n",
    "* **LIST is expensive**: Listing millions of objects is slow and costly\n",
    "\n",
    "Iceberg is specifically designed for these constraints. Its metadata layer:\n",
    "* Enables **atomic commits** without filesystem atomicity (via catalog)\n",
    "* Eliminates **LIST calls** (manifests track all files explicitly)\n",
    "* Minimizes **requests** (metadata chain tells you exactly which files to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import daft\n",
    "import pyarrow as pa\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the S3Simulator from the Parquet module\n",
    "sys.path.insert(0, '../01_parquet')\n",
    "from s3simulator import S3Simulator\n",
    "\n",
    "os.environ[\"DAFT_DASHBOARD_ENABLED\"] = \"0\"\n",
    "os.environ[\"DAFT_PROGRESS_BAR\"] = \"0\"\n",
    "\n",
    "BUCKET = \"iceberg-lake\"\n",
    "PORT = 5001  # Use different port than Parquet module to avoid conflicts\n",
    "ENDPOINT = f\"http://127.0.0.1:{PORT}\"\n",
    "\n",
    "# Start mock S3 server\n",
    "s3_sim = S3Simulator(bucket_name=BUCKET, port=PORT)\n",
    "s3_sim.start()\n",
    "\n",
    "# Create S3 client for inspection\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=ENDPOINT,\n",
    "    aws_access_key_id='fake',\n",
    "    aws_secret_access_key='fake',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(f\"S3 simulator running at {ENDPOINT}\")\n",
    "print(f\"Bucket: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog_config",
   "metadata": {},
   "source": [
    "## Configuring Iceberg for S3\n",
    "\n",
    "To store an Iceberg table on S3, we need two things:\n",
    "\n",
    "1. **A catalog**: We'll keep using SQLite (stored locally) to manage table metadata pointers. In production this would be a REST Catalog, AWS Glue, or similar.\n",
    "2. **S3 as the warehouse**: All table data and metadata files (JSON, AVRO, Parquet) go to S3.\n",
    "\n",
    "The catalog configuration tells PyIceberg:\n",
    "* Where to find S3 (`s3.endpoint`)\n",
    "* How to authenticate (`s3.access-key-id`, `s3.secret-access-key`)\n",
    "* Where to write data (`warehouse` pointing to `s3://...`)\n",
    "\n",
    "This is the same configuration that would work against real AWS S3 - just replace the endpoint URL and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite catalog stored locally (just the pointer to metadata)\n",
    "catalog_dir = Path('../data/catalog_s3_demo').absolute()\n",
    "catalog_dir.mkdir(parents=True, exist_ok=True)\n",
    "catalog_db = catalog_dir / 'catalog.db'\n",
    "catalog_db.unlink(missing_ok=True)\n",
    "\n",
    "# Create Iceberg catalog with S3 warehouse\n",
    "catalog = SqlCatalog(\n",
    "    's3_demo',\n",
    "    **{\n",
    "        'uri': f'sqlite:///{catalog_db}',\n",
    "        'warehouse': f's3://{BUCKET}/warehouse',\n",
    "        's3.endpoint': ENDPOINT,\n",
    "        's3.access-key-id': 'fake',\n",
    "        's3.secret-access-key': 'fake',\n",
    "        's3.region': 'us-east-1',\n",
    "        's3.path-style-access': 'true',\n",
    "    }\n",
    ")\n",
    "catalog.create_namespace('iot')\n",
    "\n",
    "print(\"âœ… Catalog initialized\")\n",
    "print(f\"   Catalog DB (local): {catalog_db}\")\n",
    "print(f\"   Warehouse (S3): s3://{BUCKET}/warehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "write_to_s3",
   "metadata": {},
   "source": [
    "## Writing an Iceberg table to S3\n",
    "\n",
    "Writing to S3 works exactly like writing to local files - we use the same PyIceberg API. The only difference is that all file I/O goes through S3. Watch the S3 simulator output above to see the PUT requests as each file is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load events data\n",
    "df_events = daft.read_json('../data/input/events.jsonl')\n",
    "df_batch = df_events.limit(30000)\n",
    "arrow_batch = df_batch.to_arrow()\n",
    "\n",
    "# Create table and write first batch\n",
    "print(\"Creating table and writing first batch (watch the PUT requests above)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events_table = catalog.create_table(\n",
    "    'iot.events',\n",
    "    schema=pa.schema(arrow_batch.schema)\n",
    ")\n",
    "events_table.append(arrow_batch)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâœ… Appended {len(arrow_batch):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "list_s3_objects",
   "metadata": {},
   "source": [
    "### What got written to S3?\n",
    "\n",
    "Let's see exactly which objects Iceberg created in S3. You'll recognize the metadata hierarchy from the earlier notebook:\n",
    "\n",
    "```\n",
    "warehouse/\n",
    "  iot/\n",
    "    events/\n",
    "      metadata/   â† JSON and AVRO metadata files\n",
    "      data/       â† Parquet data files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list_objects",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_objects(prefix=''):\n",
    "    \"\"\"List all objects in the S3 bucket with their sizes.\"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    objects = []\n",
    "    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            objects.append(obj)\n",
    "    return objects\n",
    "\n",
    "objects = list_s3_objects('warehouse/')\n",
    "\n",
    "print(f\"Objects in S3 bucket ({len(objects)} total):\")\n",
    "print()\n",
    "\n",
    "total_size = 0\n",
    "metadata_size = 0\n",
    "data_size = 0\n",
    "\n",
    "for obj in sorted(objects, key=lambda x: x['Key']):\n",
    "    key = obj['Key'].replace(f'warehouse/iot/events/', '')\n",
    "    size = obj['Size']\n",
    "    total_size += size\n",
    "\n",
    "    # Categorize by file type\n",
    "    if '.metadata.json' in obj['Key']:\n",
    "        category = 'ðŸ“„ Metadata JSON'\n",
    "        metadata_size += size\n",
    "    elif '.avro' in obj['Key']:\n",
    "        category = 'ðŸ“‹ AVRO (manifest)'\n",
    "        metadata_size += size\n",
    "    elif '.parquet' in obj['Key']:\n",
    "        category = 'ðŸ’¾ Parquet (data)'\n",
    "        data_size += size\n",
    "    else:\n",
    "        category = 'ðŸ“ Other'\n",
    "\n",
    "    print(f\"  {category:25s} {size:>10,} bytes  {key}\")\n",
    "\n",
    "print()\n",
    "print(f\"Metadata files: {metadata_size / 1024:.1f} KB\")\n",
    "print(f\"Data files:     {data_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Overhead ratio: {metadata_size / (data_size + 1) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second_batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a second batch to create another snapshot\n",
    "print(\"Writing second batch (watch more PUT requests)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_batch2 = df_events.offset(30000).limit(20000)\n",
    "arrow_batch2 = df_batch2.to_arrow()\n",
    "events_table.append(arrow_batch2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List again to see what was added\n",
    "objects2 = list_s3_objects('warehouse/')\n",
    "new_objects = [o for o in objects2 if o not in objects]\n",
    "\n",
    "print(f\"\\nâœ… Appended {len(arrow_batch2):,} more records\")\n",
    "print(f\"New objects created: {len(objects2) - len(objects)}\")\n",
    "for obj in sorted(objects2, key=lambda x: x['Key']):\n",
    "    if obj not in objects:\n",
    "        key = obj['Key'].replace(f'warehouse/iot/events/', '')\n",
    "        print(f\"  + {obj['Size']:>10,} bytes  {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reading_from_s3",
   "metadata": {},
   "source": [
    "## Reading from S3\n",
    "\n",
    "Reading an Iceberg table from S3 follows the same metadata chain we traced in the metadata deep-dive notebook:\n",
    "\n",
    "1. **Catalog lookup** (local SQLite) â†’ current metadata JSON path in S3\n",
    "2. **GET metadata JSON** from S3 â†’ current snapshot â†’ manifest list path\n",
    "3. **GET manifest list** from S3 â†’ list of manifest paths\n",
    "4. **GET manifests** from S3 â†’ list of data file paths + statistics\n",
    "5. **GET data files** from S3 â†’ byte-range requests for relevant row groups\n",
    "\n",
    "For Daft to read Iceberg on S3, we need to provide S3 credentials in the IO config, just like we did in the Parquet module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read_from_s3",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_config = daft.io.IOConfig(\n",
    "    s3=daft.io.S3Config(\n",
    "        endpoint_url=ENDPOINT,\n",
    "        key_id='fake',\n",
    "        access_key='fake',\n",
    "        region_name='us-east-1',\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Reading Iceberg table from S3 (watch GET requests above)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df = daft.read_iceberg(events_table, io_config=io_config)\n",
    "result = daft.sql(\"SELECT COUNT(*) as total FROM df\").collect()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal records: {result.to_pydict()['total'][0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtered_query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a filtered query - observe the sequence of requests\n",
    "print(\"Running filtered query (type = 'c8y_LocationUpdate')...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Notice the request sequence:\")\n",
    "print(\"  1. GET metadata JSON (small, catalog tells us where)\")\n",
    "print(\"  2. GET manifest list AVRO (small)\")\n",
    "print(\"  3. GET manifest AVRO files (small, check statistics)\")\n",
    "print(\"  4. GET parquet data files (only files that pass filter)\")\n",
    "print()\n",
    "\n",
    "df_filtered = daft.read_iceberg(events_table, io_config=io_config)\n",
    "df_filtered.filter(daft.col('type') == 'c8y_LocationUpdate').select('id', 'type', 'time').show(5)\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "request_analysis",
   "metadata": {},
   "source": [
    "## Understanding the request pattern\n",
    "\n",
    "Look at the requests logged above. You'll see:\n",
    "\n",
    "* **Metadata requests** (`GET /warehouse/iot/events/metadata/...`): Always small, fast. These are the JSON and AVRO files.\n",
    "* **Data requests** (`GET /warehouse/iot/events/data/...`): These are byte-range requests to Parquet files.\n",
    "\n",
    "The key insight: **Iceberg reads metadata first, then only the relevant data**. For a query that touches 1% of data, Iceberg reads:\n",
    "* 100% of metadata files (small, fast)\n",
    "* ~1% of data files (potentially with byte-range precision)\n",
    "\n",
    "### Why this matters for object stores\n",
    "\n",
    "Every S3 GET request has fixed overhead (~50ms latency). Minimizing the number of requests is critical. Iceberg's approach:\n",
    "\n",
    "1. **No directory listing**: Manifests track all files - no expensive S3 LIST calls\n",
    "2. **Hierarchical metadata**: Skip entire manifest files based on partition info before reading per-file statistics\n",
    "3. **Statistics-based pruning**: Use min/max bounds to skip files without reading them\n",
    "4. **Byte-range reads**: Within files, read only relevant columns and row groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "object_count_growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how the object count grows with operations\n",
    "print(\"Tracking S3 object count across operations:\\n\")\n",
    "\n",
    "def count_s3_objects():\n",
    "    objects = list_s3_objects('warehouse/')\n",
    "    metadata_count = sum(1 for o in objects if '/metadata/' in o['Key'])\n",
    "    data_count = sum(1 for o in objects if '/data/' in o['Key'])\n",
    "    return len(objects), metadata_count, data_count\n",
    "\n",
    "total, meta, data = count_s3_objects()\n",
    "print(f\"After 2 appends: {total} total ({meta} metadata, {data} data)\")\n",
    "\n",
    "# Another append\n",
    "df_batch3 = df_events.offset(50000).limit(10000)\n",
    "events_table.append(df_batch3.to_arrow())\n",
    "total, meta, data = count_s3_objects()\n",
    "print(f\"After 3 appends: {total} total ({meta} metadata, {data} data)\")\n",
    "\n",
    "# A delete operation\n",
    "print(\"\\nRunning a delete operation...\")\n",
    "events_table.delete(\"type = 'OperationMode'\")\n",
    "total, meta, data = count_s3_objects()\n",
    "print(f\"After delete:    {total} total ({meta} metadata, {data} data)\")\n",
    "\n",
    "print(\"\\nEach operation adds new files. Old files remain until cleaned up.\")\n",
    "print(\"This is Iceberg's append-only, immutable file model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small_files",
   "metadata": {},
   "source": [
    "## The small files problem\n",
    "\n",
    "Object stores perform best with large files (hundreds of MB to GB). Small files cause:\n",
    "\n",
    "* **High request overhead**: Each file needs one S3 GET request (~50-200ms). 10,000 small files = 10,000 requests.\n",
    "* **Metadata bloat**: Each file gets an entry in the manifest. Large manifests slow down query planning.\n",
    "* **Poor compression**: Small files miss compression opportunities that span row groups.\n",
    "\n",
    "### How small files accumulate\n",
    "\n",
    "* **Frequent small appends**: Each streaming micro-batch creates one or more small files\n",
    "* **Many small partitions**: Fine-grained partitioning multiplied by many writers\n",
    "* **Delete files**: Position delete files accumulate with frequent row-level deletes\n",
    "\n",
    "### The solution: Compaction\n",
    "\n",
    "**Compaction** (also called **file rewriting** or **optimize**) merges small files into larger ones. Iceberg supports compaction through Apache Spark's `OPTIMIZE` command and other engines. PyIceberg does not yet implement `rewrite_data_files()`, so we'll cover compaction hands-on in the Spark module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production_considerations",
   "metadata": {},
   "source": [
    "## Production considerations\n",
    "\n",
    "### Catalog choice for S3\n",
    "\n",
    "In production, you wouldn't use SQLite as the catalog. The catalog must be:\n",
    "* **Highly available**: Tables can't be accessed if catalog is down\n",
    "* **Strongly consistent**: Concurrent writes must be serializable\n",
    "* **Scalable**: Handle thousands of tables and millions of operations\n",
    "\n",
    "Common production choices:\n",
    "\n",
    "* **AWS Glue Data Catalog**: Native AWS service, works well with S3, integrates with Athena, EMR, Glue ETL\n",
    "* **Lakekeeper (REST Catalog)**: Lightweight Rust implementation, runs as a service, works with any object store\n",
    "* **Apache Polaris (REST Catalog)**: Java-based, cooperatively developed by Snowflake/Dremio\n",
    "* **Project Nessie**: Git-like catalog with branching and multi-table transactions\n",
    "\n",
    "### File size recommendations\n",
    "\n",
    "| Data volume | Target file size | Row group size |\n",
    "|-------------|-----------------|----------------|\n",
    "| Small table (< 1 GB) | 128 MB | 8 MB |\n",
    "| Medium table (1-100 GB) | 256-512 MB | 16-32 MB |\n",
    "| Large table (> 100 GB) | 512 MB - 1 GB | 32-64 MB |\n",
    "\n",
    "### Snapshot expiration and orphan file deletion\n",
    "\n",
    "After writes and deletes, old files accumulate in S3:\n",
    "\n",
    "1. `expire_snapshots()` - removes old snapshot metadata (PyIceberg supports this)\n",
    "2. Delete orphan files - removes unreferenced data files left behind by expired snapshots or failed commits\n",
    "\n",
    "PyIceberg does not yet implement orphan file deletion. Apache Spark's `RemoveOrphanFiles` is the standard approach for this in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snapshot_cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Demonstrate snapshot expiration (removes old snapshot metadata)\n",
    "print(\"Snapshots before expiration:\")\n",
    "for snap in events_table.snapshots():\n",
    "    ts = datetime.fromtimestamp(snap.timestamp_ms / 1000)\n",
    "    print(f\"  {snap.snapshot_id} @ {ts.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Expire all snapshots older than 2 seconds (for demonstration)\n",
    "events_table.maintenance.expire_snapshots().older_than(\n",
    "    datetime.now() - timedelta(seconds=2)\n",
    ").commit()\n",
    "\n",
    "# Reload and check\n",
    "events_table = catalog.load_table('iot.events')\n",
    "print(f\"\\nSnapshots after expiration: {len(events_table.snapshots())}\")\n",
    "for snap in events_table.snapshots():\n",
    "    ts = datetime.fromtimestamp(snap.timestamp_ms / 1000)\n",
    "    print(f\"  {snap.snapshot_id} @ {ts.strftime('%H:%M:%S')} (current)\")\n",
    "\n",
    "print()\n",
    "print(\"Note: Metadata files for expired snapshots are removed from S3.\")\n",
    "print(\"Data files remain until orphan file cleanup is run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_s3_state",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final state of the S3 bucket\n",
    "all_objects = list_s3_objects('warehouse/')\n",
    "\n",
    "print(f\"Final S3 state: {len(all_objects)} objects\\n\")\n",
    "\n",
    "categories = {\n",
    "    'Metadata JSON': [o for o in all_objects if '.metadata.json' in o['Key']],\n",
    "    'Manifest list (AVRO)': [o for o in all_objects if 'snap-' in o['Key'] and '.avro' in o['Key']],\n",
    "    'Manifest (AVRO)': [o for o in all_objects if '-m' in o['Key'] and '.avro' in o['Key']],\n",
    "    'Data files (Parquet)': [o for o in all_objects if '.parquet' in o['Key']],\n",
    "}\n",
    "\n",
    "for category, objects in categories.items():\n",
    "    total = sum(o['Size'] for o in objects)\n",
    "    print(f\"  {category}: {len(objects)} file(s), {total / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "s3_sim.stop()\n",
    "\n",
    "# Also clean up local catalog DB\n",
    "shutil.rmtree(catalog_dir, ignore_errors=True)\n",
    "\n",
    "print(\"âœ… S3 simulator stopped and local files cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "review",
   "metadata": {},
   "source": [
    "## Review questions\n",
    "\n",
    "**Why does Iceberg avoid S3 LIST operations?**\n",
    "   - What does LIST return? Why is it expensive?\n",
    "   - How do manifests replace LIST calls?\n",
    "\n",
    "**What is the request sequence for a simple `SELECT COUNT(*) FROM events` on S3?**\n",
    "   - How many S3 GETs are needed before reading any Parquet data?\n",
    "   - What would change if there were 100 manifest files?\n",
    "\n",
    "**How does object-store-backed Iceberg differ from local-file-backed Iceberg?**\n",
    "   - Does the PyIceberg API change?\n",
    "   - What changes in terms of performance characteristics?\n",
    "\n",
    "**What is the difference between snapshot expiration and compaction?**\n",
    "   - Which affects data files? Which affects metadata?\n",
    "   - Can you run them independently?\n",
    "\n",
    "**Why might you keep SQLite as the catalog even when using S3 as the warehouse?**\n",
    "   - What does the catalog store vs. what goes in S3?\n",
    "   - What are the limitations of SQLite in production?\n",
    "\n",
    "**What happens to old data files after snapshot expiration?**\n",
    "   - Are they deleted immediately?\n",
    "   - What would happen to time-travel queries against expired snapshots?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Trace a query's S3 requests\n",
    "\n",
    "1. Restart the S3 simulator and create a fresh Iceberg table with 3 snapshots\n",
    "2. Run `daft.read_iceberg(table).filter(daft.col('type') == 'c8y_Event').collect()`\n",
    "3. Note down every S3 request in order (from the simulator log)\n",
    "4. Classify each request: catalog / metadata JSON / manifest list / manifest / data file\n",
    "5. How many requests are metadata vs. data?\n",
    "\n",
    "### Measure the small files impact\n",
    "\n",
    "1. Create two versions of the same table:\n",
    "   * Version A: 50 writes of 1,000 records each (50 data files)\n",
    "   * Version B: 1 write of 50,000 records (1 data file)\n",
    "2. Run the same query on both\n",
    "3. Count how many S3 requests are made for each version\n",
    "4. Calculate the estimated latency difference (assuming 50ms per request)\n",
    "\n",
    "### Compare catalog backends conceptually\n",
    "\n",
    "Research and compare the following Iceberg catalogs for a hypothetical IoT production deployment:\n",
    "\n",
    "* **AWS Glue** vs. **Lakekeeper** vs. **Project Nessie**\n",
    "\n",
    "For each, evaluate:\n",
    "1. How are concurrent writes handled?\n",
    "2. What infrastructure is required?\n",
    "3. Does it support multiple engines (Daft, Spark, Trino)?\n",
    "4. What are the cost implications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Object stores are the natural home for Iceberg data lakes:\n",
    "\n",
    "* **Designed for S3**: Iceberg's immutable files and append-only metadata work perfectly with object store semantics\n",
    "* **No directory listings**: Manifests eliminate expensive S3 LIST operations\n",
    "* **Minimal requests**: Metadata chain precisely identifies which data to read\n",
    "* **Catalog is separate**: Catalog provides atomic commits; warehouse just stores files\n",
    "* **Small files accumulate**: Frequent appends and deletes create many small files over time\n",
    "\n",
    "### Request pattern for a query\n",
    "\n",
    "```\n",
    "Catalog (local) â†’ Metadata JSON (1 GET)\n",
    "                â†’ Manifest list (1 GET)\n",
    "                â†’ Manifest files (n GETs, n = manifests that pass partition pruning)\n",
    "                â†’ Data files (m GETs, m = files that pass statistics pruning)\n",
    "                   â†’ Byte-range reads (per column chunk per row group)\n",
    "```\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "1. **Object stores = no POSIX**: No appends, no atomic multi-file ops - Iceberg handles this\n",
    "2. **Latency matters**: Each S3 request costs ~50-200ms; minimize request count\n",
    "3. **Small files are expensive**: Aim for 128MB-1GB per Parquet file\n",
    "4. **Compaction is essential but engine-dependent**: PyIceberg doesn't support it yet; use Spark\n",
    "5. **Choose the right catalog**: SQLite for dev, REST Catalog for production\n",
    "6. **Metadata overhead is tiny**: Typically < 1% of data size\n",
    "\n",
    "### What's next?\n",
    "\n",
    "You've now covered the complete Iceberg feature set with PyIceberg and Daft:\n",
    "\n",
    "* **Getting started**: Creating and querying tables\n",
    "* **Metadata deep dive**: How files link together\n",
    "* **Time travel**: Querying historical snapshots\n",
    "* **Schema evolution**: Changing schemas without rewrites\n",
    "* **Transactions**: Optimistic concurrency control\n",
    "* **Partitioning**: Efficient data organization for large scale\n",
    "* **Object stores**: Production deployment on S3\n",
    "\n",
    "The next module covers the **medallion architecture**: how to organize data lakes with bronze, silver, and gold layers using Iceberg and Parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
